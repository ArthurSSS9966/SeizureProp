{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           balanced_accuracy_score, roc_auc_score, roc_curve, auc,\n",
    "                           confusion_matrix, classification_report)\n",
    "from scipy import stats\n",
    "from scipy.fftpack import fft\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MultiPatientClassifier:\n",
    "    \"\"\"\n",
    "    Channel-Level标准化的Grey/White Matter分类系统\n",
    "    \n",
    "    核心思想：\n",
    "    1. 每个channel作为一个分类单位\n",
    "    2. 对每个channel的时间窗口特征进行channel-specific标准化\n",
    "    3. 使用平均概率来分类每个channel\n",
    "    4. 验证时计算每个patient的channel-level accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_folder, output_folder='results'):\n",
    "        self.processed_folder = processed_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.patients_data = {}\n",
    "        self.channel_features_data = {}  # 改为channel-centric存储\n",
    "        self.normalized_channel_features_data = {}\n",
    "        \n",
    "        self.classifiers = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "            'SVM': SVC(probability=True, random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'MLP': MLPClassifier(max_iter=1000, random_state=42, hidden_layer_sizes=(50,)),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'LDA': LDA(),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        print(\"Initializing Channel-Level Normalized Classifier\")\n",
    "        print(f\"Processed folder: {processed_folder}\")\n",
    "        print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    def load_all_patients(self):\n",
    "        \"\"\"Load all patients from the processed folder.\"\"\"\n",
    "        pkl_files = glob.glob(os.path.join(self.processed_folder, \"P*_processed.pkl\"))\n",
    "        print(f\"\\nFound {len(pkl_files)} patient files\")\n",
    "        for pkl_file in pkl_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                pid = data['patient_id']\n",
    "                self.patients_data[pid] = data\n",
    "                duration = data['processing_summary']['total_duration_seconds'] / 60\n",
    "                print(f\"✓ {pid}: {len(data['recordings'])} recordings, {duration:.1f} minutes\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load {os.path.basename(pkl_file)}: {e}\")\n",
    "        print(f\"Successfully loaded {len(self.patients_data)} patients\")\n",
    "        return bool(self.patients_data)\n",
    "    \n",
    "    def extract_electrode_classification(self, matter_data):\n",
    "        \"\"\"从matter数据提取电极分类\"\"\"\n",
    "        \n",
    "        matter_columns = ['MatterType', 'matter', 'Matter', 'mattertype', 'tissue_type', 'type']\n",
    "        matter_col = None\n",
    "        \n",
    "        for col in matter_columns:\n",
    "            if col in matter_data.columns:\n",
    "                matter_col = col\n",
    "                break\n",
    "        \n",
    "        if matter_col is None:\n",
    "            raise ValueError(f\"Matter type column not found. Available: {matter_data.columns.tolist()}\")\n",
    "        \n",
    "        matter_values = matter_data[matter_col].astype(str).str.lower()\n",
    "        \n",
    "        grey_mask = matter_values.isin(['G', 'g', 'Grey', 'grey', 'Gray', 'gray'])\n",
    "        white_mask = matter_values.isin(['W', 'w', 'White', 'white'])\n",
    "        \n",
    "        if np.sum(grey_mask) == 0 or np.sum(white_mask) == 0:\n",
    "            print(f\"    G/W format not found, trying other methods...\")\n",
    "            matter_values_lower = matter_values.str.lower()\n",
    "            \n",
    "            if np.sum(grey_mask) == 0:\n",
    "                grey_patterns = ['grey', 'gray', 'cortex', 'cortical']\n",
    "                grey_mask = matter_values_lower.str.contains('|'.join(grey_patterns), na=False, case=False)\n",
    "            \n",
    "            if np.sum(white_mask) == 0:\n",
    "                white_patterns = ['white']\n",
    "                white_mask = matter_values_lower.str.contains('|'.join(white_patterns), na=False, case=False)\n",
    "        \n",
    "        grey_indices = matter_data.index[grey_mask].tolist()\n",
    "        white_indices = matter_data.index[white_mask].tolist()\n",
    "        \n",
    "        classification_info = {\n",
    "            'matter_column': matter_col,\n",
    "            'total_electrodes': len(matter_data),\n",
    "            'grey_electrodes': len(grey_indices),\n",
    "            'white_electrodes': len(white_indices),\n",
    "            'grey_indices': grey_indices,\n",
    "            'white_indices': white_indices,\n",
    "            'matter_distribution': matter_data[matter_col].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        return grey_indices, white_indices, classification_info\n",
    "    \n",
    "    def extract_signal_features(self, signal, fs):\n",
    "        \"\"\"提取单个信号的特征\"\"\"\n",
    "        \n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 时域特征\n",
    "        features['std'] = np.std(signal)\n",
    "        features['mad'] = np.median(np.abs(signal - np.median(signal)))\n",
    "        features['range'] = np.ptp(signal)\n",
    "        features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        \n",
    "        # 信号复杂度\n",
    "        features['area'] = np.sum(np.abs(signal))\n",
    "        \n",
    "        # 频域特征\n",
    "        try:\n",
    "            n_fft = len(signal)\n",
    "            windowed_signal = signal * np.hamming(n_fft)\n",
    "            fft_vals = fft(windowed_signal)\n",
    "            fft_mag = np.abs(fft_vals[:n_fft//2])\n",
    "            freqs = np.fft.fftfreq(n_fft, 1/fs)[:n_fft//2]\n",
    "            \n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 100),\n",
    "                'high_gamma': (100, min(200, fs/2))\n",
    "            }\n",
    "            \n",
    "            total_power = np.sum(fft_mag**2)\n",
    "            for band_name, (low, high) in bands.items():\n",
    "                band_mask = (freqs >= low) & (freqs <= high)\n",
    "                if np.any(band_mask):\n",
    "                    band_power = np.sum(fft_mag[band_mask]**2)\n",
    "                    features[f'power_{band_name}'] = band_power\n",
    "                    features[f'rel_power_{band_name}'] = band_power / total_power if total_power > 0 else 0\n",
    "                else:\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                    features[f'rel_power_{band_name}'] = 0\n",
    "            \n",
    "            features['total_power'] = total_power\n",
    "                \n",
    "        except Exception as e:\n",
    "            bands = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high_gamma']\n",
    "            for band in bands:\n",
    "                features[f'power_{band}'] = 0\n",
    "                features[f'rel_power_{band}'] = 0\n",
    "            features['total_power'] = 0\n",
    "            features['peak_frequency'] = 0\n",
    "            features['spectral_centroid'] = 0\n",
    "            features['spectral_entropy'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_channel_centric_features(self, use_windowing=True, \n",
    "                                       window_size_ms=500, step_size_ms=250, \n",
    "                                       max_windows_per_channel=200):\n",
    "        \"\"\"\n",
    "        提取以channel为中心的特征\n",
    "        \n",
    "        核心思想：\n",
    "        - 每个channel作为一个分类单位\n",
    "        - 每个channel有多个时间窗口的特征\n",
    "        - 存储结构：{channel_id: {features: [], label: 0/1, patient_id: str}}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n🔄 Extracting Channel-Centric Features...\")\n",
    "        \n",
    "        channel_id = 0  # 全局channel ID\n",
    "        \n",
    "        for patient_id, patient_data in self.patients_data.items():\n",
    "            print(f\"\\n处理 {patient_id}...\")\n",
    "            \n",
    "            matter_data = patient_data['matter_data']\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            # 提取电极分类\n",
    "            try:\n",
    "                grey_indices, white_indices, classification_info = self.extract_electrode_classification(matter_data)\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Electrode classification failed: {e}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Electrode Classification: {classification_info['grey_electrodes']} grey, {classification_info['white_electrodes']} white\")\n",
    "            \n",
    "            if len(grey_indices) == 0 or len(white_indices) == 0:\n",
    "                print(f\"  ✗ Missing Grey or White Matter Electrode\")\n",
    "                continue\n",
    "            \n",
    "            # 合并所有recordings的数据\n",
    "            all_grey_data = []\n",
    "            all_white_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                neural_data = recording['neural_data_processed']\n",
    "                grey_data = neural_data[:, grey_indices]\n",
    "                white_data = neural_data[:, white_indices]\n",
    "                all_grey_data.append(grey_data)\n",
    "                all_white_data.append(white_data)\n",
    "            \n",
    "            combined_grey = np.vstack(all_grey_data) if all_grey_data else np.array([])\n",
    "            combined_white = np.vstack(all_white_data) if all_white_data else np.array([])\n",
    "            \n",
    "            print(f\"  Merging...: Grey {combined_grey.shape}, White {combined_white.shape}\")\n",
    "            \n",
    "            # 处理Grey Matter Channels\n",
    "            for ch_idx, electrode_idx in enumerate(grey_indices):\n",
    "                channel_data = combined_grey[:, ch_idx]\n",
    "                \n",
    "                # 为这个channel提取多个时间窗口的特征\n",
    "                channel_features = self._extract_features_for_single_channel(\n",
    "                    channel_data, fs, use_windowing, window_size_ms, step_size_ms, max_windows_per_channel\n",
    "                )\n",
    "                \n",
    "                if len(channel_features) > 0:\n",
    "                    self.channel_features_data[channel_id] = {\n",
    "                        'features': channel_features,  # List of feature dicts from different windows\n",
    "                        'label': 1,  # Grey matter\n",
    "                        'patient_id': patient_id,\n",
    "                        'electrode_idx': electrode_idx,\n",
    "                        'channel_idx': ch_idx,\n",
    "                        'matter_type': 'grey'\n",
    "                    }\n",
    "                    channel_id += 1\n",
    "            \n",
    "            # 处理White Matter Channels\n",
    "            for ch_idx, electrode_idx in enumerate(white_indices):\n",
    "                channel_data = combined_white[:, ch_idx]\n",
    "                \n",
    "                channel_features = self._extract_features_for_single_channel(\n",
    "                    channel_data, fs, use_windowing, window_size_ms, step_size_ms, max_windows_per_channel\n",
    "                )\n",
    "                \n",
    "                if len(channel_features) > 0:\n",
    "                    self.channel_features_data[channel_id] = {\n",
    "                        'features': channel_features,\n",
    "                        'label': 0,  # White matter\n",
    "                        'patient_id': patient_id,\n",
    "                        'electrode_idx': electrode_idx,\n",
    "                        'channel_idx': ch_idx,\n",
    "                        'matter_type': 'white'\n",
    "                    }\n",
    "                    channel_id += 1\n",
    "        \n",
    "        print(f\"\\n✅ Extracted features for {len(self.channel_features_data)} channels\")\n",
    "        \n",
    "        # 统计信息\n",
    "        grey_channels = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 1)\n",
    "        white_channels = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 0)\n",
    "        \n",
    "        print(f\"   Grey matter channels: {grey_channels}\")\n",
    "        print(f\"   White matter channels: {white_channels}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _extract_features_for_single_channel(self, channel_data, fs, use_windowing, \n",
    "                                           window_size_ms, step_size_ms, max_windows_per_channel):\n",
    "        \"\"\"为单个channel提取特征\"\"\"\n",
    "        \n",
    "        channel_features = []\n",
    "        \n",
    "        if use_windowing:\n",
    "            # 时间窗口方法\n",
    "            window_samples = int(window_size_ms * fs / 1000)\n",
    "            step_samples = int(step_size_ms * fs / 1000)\n",
    "            n_time = len(channel_data)\n",
    "            \n",
    "            window_count = 0\n",
    "            for start in range(0, n_time - window_samples + 1, step_samples):\n",
    "                if window_count >= max_windows_per_channel:\n",
    "                    break\n",
    "                    \n",
    "                end = start + window_samples\n",
    "                window_data = channel_data[start:end]\n",
    "                \n",
    "                features = self.extract_signal_features(window_data, fs)\n",
    "                if features is not None:\n",
    "                    channel_features.append(features)\n",
    "                    window_count += 1\n",
    "        else:\n",
    "            # 整个信号作为一个特征\n",
    "            features = self.extract_signal_features(channel_data, fs)\n",
    "            if features is not None:\n",
    "                channel_features.append(features)\n",
    "        \n",
    "        return channel_features\n",
    "    \n",
    "    def apply_channel_specific_normalization(self, normalization_method='robust'):\n",
    "        \"\"\"\n",
    "        对每个channel的特征进行channel-specific标准化\n",
    "        \n",
    "        核心思想：\n",
    "        1. 每个channel有多个时间窗口的特征\n",
    "        2. 在每个channel内部，对这些时间窗口的特征进行标准化\n",
    "        3. 这样消除了channel内部的系统性差异，保留了grey/white matter的差异\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n🔄 Applying Channel-Specific Normalization ({normalization_method})...\")\n",
    "        \n",
    "        if not self.channel_features_data:\n",
    "            print(\"  ❌ No channel features found. Run extract_channel_centric_features() first.\")\n",
    "            return False\n",
    "        \n",
    "        # 获取特征名称（从第一个channel的第一个窗口获取）\n",
    "        first_channel = list(self.channel_features_data.values())[0]\n",
    "        if len(first_channel['features']) == 0:\n",
    "            print(\"  ❌ No features found in channels.\")\n",
    "            return False\n",
    "        \n",
    "        feature_names = list(first_channel['features'][0].keys())\n",
    "        print(f\"  Normalizing {len(feature_names)} features across {len(self.channel_features_data)} channels\")\n",
    "        \n",
    "        normalization_stats = {}\n",
    "        \n",
    "        # 为每个channel进行标准化\n",
    "        for channel_id, channel_data in self.channel_features_data.items():\n",
    "            patient_id = channel_data['patient_id']\n",
    "            matter_type = channel_data['matter_type']\n",
    "            \n",
    "            # 将这个channel的所有窗口特征转换为DataFrame\n",
    "            features_list = channel_data['features']\n",
    "            if len(features_list) == 0:\n",
    "                continue\n",
    "                \n",
    "            features_df = pd.DataFrame(features_list)\n",
    "            \n",
    "            # 对这个channel的特征进行标准化\n",
    "            normalized_features_df = self._normalize_features(features_df, normalization_method)\n",
    "            \n",
    "            # 计算标准化统计信息\n",
    "            before_stats = self._get_feature_stats(features_df)\n",
    "            after_stats = self._get_feature_stats(normalized_features_df)\n",
    "            \n",
    "            # 转换回list of dicts\n",
    "            normalized_features_list = normalized_features_df.to_dict('records')\n",
    "            \n",
    "            # 存储标准化后的数据\n",
    "            normalized_channel_data = channel_data.copy()\n",
    "            normalized_channel_data['features'] = normalized_features_list\n",
    "            normalized_channel_data['normalization_method'] = normalization_method\n",
    "            normalized_channel_data['normalization_stats'] = {\n",
    "                'before': before_stats,\n",
    "                'after': after_stats\n",
    "            }\n",
    "            \n",
    "            self.normalized_channel_features_data[channel_id] = normalized_channel_data\n",
    "            \n",
    "            # 收集统计信息\n",
    "            key = f\"{patient_id}_{matter_type}\"\n",
    "            if key not in normalization_stats:\n",
    "                normalization_stats[key] = {\n",
    "                    'channels': 0,\n",
    "                    'before_std_mean': 0,\n",
    "                    'after_std_mean': 0\n",
    "                }\n",
    "            \n",
    "            normalization_stats[key]['channels'] += 1\n",
    "            normalization_stats[key]['before_std_mean'] += before_stats['std_mean']\n",
    "            normalization_stats[key]['after_std_mean'] += after_stats['std_mean']\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"\\n📊 Normalization Statistics by Patient and Matter Type:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Patient_Matter':<20} {'Channels':<10} {'Before_StdMean':<15} {'After_StdMean':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for key, stats in normalization_stats.items():\n",
    "            n_channels = stats['channels']\n",
    "            before_mean = stats['before_std_mean'] / n_channels\n",
    "            after_mean = stats['after_std_mean'] / n_channels\n",
    "            print(f\"{key:<20} {n_channels:<10} {before_mean:<15.3f} {after_mean:<15.3f}\")\n",
    "        \n",
    "        print(f\"\\n✅ Channel-specific normalization completed!\")\n",
    "        print(f\"   Normalized {len(self.normalized_channel_features_data)} channels\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _normalize_features(self, features_df, method='robust', outlier_clip=True, iqr_multiplier=1.5):\n",
    "        \"\"\"\n",
    "        应用特定的标准化方法，在标准化前使用IQR方法过滤outliers\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_df : pd.DataFrame\n",
    "            特征数据框\n",
    "        method : str\n",
    "            标准化方法 ('robust', 'standard', 'minmax', 'quantile')\n",
    "        outlier_clip : bool\n",
    "            是否在标准化前clip outliers\n",
    "        iqr_multiplier : float\n",
    "            IQR倍数，用于定义outlier阈值 (1.5为标准值，3.0为极端值)\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized_df = features_df.copy()\n",
    "        outlier_stats = {}  # 记录outlier统计信息\n",
    "        \n",
    "        for column in features_df.columns:\n",
    "            values = features_df[column].values\n",
    "            original_values = values.copy()\n",
    "            \n",
    "            # Step 1: 使用IQR方法clip outliers\n",
    "            if outlier_clip:\n",
    "                q25 = np.percentile(values, 25)\n",
    "                q75 = np.percentile(values, 75)\n",
    "                iqr = q75 - q25\n",
    "                \n",
    "                if iqr > 0:  # 避免除零错误\n",
    "                    lower_bound = q25 - iqr_multiplier * iqr\n",
    "                    upper_bound = q75 + iqr_multiplier * iqr\n",
    "                    \n",
    "                    # 记录outlier统计\n",
    "                    n_outliers = np.sum((values < lower_bound) | (values > upper_bound))\n",
    "                    outlier_stats[column] = {\n",
    "                        'n_outliers': n_outliers,\n",
    "                        'outlier_ratio': n_outliers / len(values),\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound,\n",
    "                        'original_range': [np.min(values), np.max(values)]\n",
    "                    }\n",
    "                    \n",
    "                    # Clip outliers\n",
    "                    values = np.clip(values, lower_bound, upper_bound)\n",
    "                else:\n",
    "                    # 如果IQR为0，说明值都相同，不需要clip\n",
    "                    outlier_stats[column] = {\n",
    "                        'n_outliers': 0,\n",
    "                        'outlier_ratio': 0.0,\n",
    "                        'lower_bound': values[0],\n",
    "                        'upper_bound': values[0],\n",
    "                        'original_range': [np.min(values), np.max(values)]\n",
    "                    }\n",
    "            \n",
    "            # Step 2: 应用标准化\n",
    "            if method == 'standard':\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                if std_val > 0:\n",
    "                    normalized_df[column] = (values - mean_val) / std_val\n",
    "                else:\n",
    "                    normalized_df[column] = values - mean_val\n",
    "                    \n",
    "            elif method == 'robust':\n",
    "                median_val = np.median(values)\n",
    "                mad_val = np.median(np.abs(values - median_val))\n",
    "                if mad_val > 0:\n",
    "                    normalized_df[column] = (values - median_val) / (1.4826 * mad_val)\n",
    "                else:\n",
    "                    normalized_df[column] = values - median_val\n",
    "                    \n",
    "            elif method == 'minmax':\n",
    "                min_val = np.min(values)\n",
    "                max_val = np.max(values)\n",
    "                if max_val > min_val:\n",
    "                    normalized_df[column] = (values - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    normalized_df[column] = np.zeros_like(values)\n",
    "                    \n",
    "            elif method == 'quantile':\n",
    "                q25 = np.percentile(values, 25)\n",
    "                q75 = np.percentile(values, 75)\n",
    "                iqr = q75 - q25\n",
    "                median_val = np.median(values)\n",
    "                if iqr > 0:\n",
    "                    normalized_df[column] = (values - median_val) / iqr\n",
    "                else:\n",
    "                    normalized_df[column] = values - median_val\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "        \n",
    "        # 可选：存储outlier统计信息以便后续分析\n",
    "        if outlier_clip and hasattr(self, '_outlier_stats'):\n",
    "            if not hasattr(self, '_outlier_stats'):\n",
    "                self._outlier_stats = {}\n",
    "            self._outlier_stats[f'normalize_{len(self._outlier_stats)}'] = outlier_stats\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    def _get_feature_stats(self, features_df):\n",
    "        \"\"\"获取特征的统计信息\"\"\"\n",
    "        means = features_df.mean()\n",
    "        stds = features_df.std()\n",
    "        \n",
    "        return {\n",
    "            'mean_min': means.min(),\n",
    "            'mean_max': means.max(),\n",
    "            'std_min': stds.min(),\n",
    "            'std_max': stds.max(),\n",
    "            'mean_mean': means.mean(),\n",
    "            'std_mean': stds.mean()\n",
    "        }\n",
    "    \n",
    "    def prepare_dataset_for_channel_classification(self, use_normalized=True):\n",
    "        \"\"\"\n",
    "        准备用于channel-level分类的数据集\n",
    "        \n",
    "        核心修改：\n",
    "        1. 每个channel的多个时间窗口样本会被分别训练\n",
    "        2. 在验证时，对同一个channel的所有样本的预测概率取平均\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        samples_data : list\n",
    "            每个sample的信息，包含：\n",
    "            - features: 特征向量\n",
    "            - label: 标签\n",
    "            - channel_id: 所属channel ID\n",
    "            - patient_id: 所属patient ID\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n📊 Preparing Dataset for Channel Classification...\")\n",
    "        \n",
    "        # 选择数据源\n",
    "        if use_normalized and self.normalized_channel_features_data:\n",
    "            data_source = self.normalized_channel_features_data\n",
    "        else:\n",
    "            data_source = self.channel_features_data\n",
    "        \n",
    "        if not data_source:\n",
    "            print(\"  ❌ No channel features available.\")\n",
    "            return None\n",
    "        \n",
    "        samples_data = []\n",
    "        \n",
    "        for channel_id, channel_data in data_source.items():\n",
    "            # 每个channel的每个时间窗口都作为一个独立的训练样本\n",
    "            for window_idx, window_features in enumerate(channel_data['features']):\n",
    "                sample_info = {\n",
    "                    'features': np.array(list(window_features.values())),\n",
    "                    'label': channel_data['label'],\n",
    "                    'channel_id': channel_id,\n",
    "                    'patient_id': channel_data['patient_id'],\n",
    "                    'matter_type': channel_data['matter_type'],\n",
    "                    'window_idx': window_idx\n",
    "                }\n",
    "                samples_data.append(sample_info)\n",
    "        \n",
    "        print(f\"   Total samples: {len(samples_data)}\")\n",
    "        print(f\"   From channels: {len(data_source)} channels\")\n",
    "        \n",
    "        # 统计信息\n",
    "        grey_samples = sum(1 for s in samples_data if s['label'] == 1)\n",
    "        white_samples = sum(1 for s in samples_data if s['label'] == 0)\n",
    "        \n",
    "        print(f\"   Grey matter samples: {grey_samples}\")\n",
    "        print(f\"   White matter samples: {white_samples}\")\n",
    "        \n",
    "        return samples_data\n",
    "    \n",
    "    def leave_one_patient_out_validation_channel_level(self, use_normalized=True):\n",
    "        \"\"\"\n",
    "        Channel-level的Leave-one-patient-out交叉验证\n",
    "        \n",
    "        核心修改：\n",
    "        1. 训练时使用所有的时间窗口样本\n",
    "        2. 验证时计算每个patient的每个channel的平均预测概率\n",
    "        3. 基于平均概率进行channel-level分类\n",
    "        4. 计算每个patient的channel-level accuracy\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n🔄 Channel-Level Leave-One-Patient-Out Validation\")\n",
    "        \n",
    "        # 准备数据\n",
    "        samples_data = self.prepare_dataset_for_channel_classification(use_normalized)\n",
    "        \n",
    "        if samples_data is None:\n",
    "            print(\"❌ Failed to prepare dataset\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 获取患者列表\n",
    "        patients = list(set([sample['patient_id'] for sample in samples_data]))\n",
    "        n_patients = len(patients)\n",
    "        \n",
    "        if n_patients < 3:\n",
    "            raise ValueError(f\"Need at least 3 patients, {n_patients} patients found\")\n",
    "        \n",
    "        print(f\"   {n_patients} patients, {len(samples_data)} samples total\")\n",
    "        \n",
    "        # 存储结果\n",
    "        cv_results = {name: [] for name in self.classifiers.keys()}\n",
    "        all_predictions = {name: {'y_true': [], 'y_pred': [], 'y_proba': [], 'test_patients': [], 'channel_ids': []} \n",
    "                          for name in self.classifiers.keys()}\n",
    "        \n",
    "        # Leave-one-patient-out循环\n",
    "        for fold, test_patient in enumerate(patients):\n",
    "            train_patients = [p for p in patients if p != test_patient]\n",
    "            \n",
    "            print(f\"\\nFold {fold+1}/{n_patients}: 测试 {test_patient}\")\n",
    "            \n",
    "            # 分离训练和测试样本\n",
    "            train_samples = [s for s in samples_data if s['patient_id'] in train_patients]\n",
    "            test_samples = [s for s in samples_data if s['patient_id'] == test_patient]\n",
    "            \n",
    "            # 准备训练数据 (sample-level)\n",
    "            X_train = np.array([s['features'] for s in train_samples])\n",
    "            y_train = np.array([s['label'] for s in train_samples])\n",
    "            \n",
    "            # 准备测试数据 (sample-level)  \n",
    "            X_test = np.array([s['features'] for s in test_samples])\n",
    "            y_test = np.array([s['label'] for s in test_samples])\n",
    "            \n",
    "            # 获取测试集的channel信息\n",
    "            test_channel_ids = [s['channel_id'] for s in test_samples]\n",
    "            test_channel_labels = [s['label'] for s in test_samples]\n",
    "            \n",
    "            print(f\"   Training samples: {len(X_train)} ({np.sum(y_train)} grey, {np.sum(y_train==0)} white)\")\n",
    "            print(f\"   Testing samples: {len(X_test)} ({np.sum(y_test)} grey, {np.sum(y_test==0)} white)\")\n",
    "            \n",
    "            # 跨患者标准化（如果使用原始特征）\n",
    "            if not use_normalized:\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # 训练和评估每个分类器\n",
    "            for clf_name, clf in self.classifiers.items():\n",
    "                try:\n",
    "                    # 训练分类器 (sample-level)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    \n",
    "                    # 预测测试集 (sample-level)\n",
    "                    y_pred_samples = clf.predict(X_test)\n",
    "                    \n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        y_proba_samples = clf.predict_proba(X_test)[:, 1]\n",
    "                    else:\n",
    "                        y_proba_samples = np.zeros_like(y_pred_samples, dtype=float)\n",
    "                    \n",
    "                    # 将sample-level预测聚合为channel-level预测\n",
    "                    channel_predictions = self._aggregate_predictions_to_channel_level(\n",
    "                        test_samples, y_pred_samples, y_proba_samples\n",
    "                    )\n",
    "                    \n",
    "                    # 提取channel-level的真实标签和预测\n",
    "                    channel_y_true = [pred['true_label'] for pred in channel_predictions]\n",
    "                    channel_y_pred = [pred['pred_label'] for pred in channel_predictions]\n",
    "                    channel_y_proba = [pred['avg_proba'] for pred in channel_predictions]\n",
    "                    channel_ids = [pred['channel_id'] for pred in channel_predictions]\n",
    "                    \n",
    "                    # 计算channel-level指标\n",
    "                    fold_results = {\n",
    "                        'fold': fold,\n",
    "                        'test_patient': test_patient,\n",
    "                        'accuracy': accuracy_score(channel_y_true, channel_y_pred),\n",
    "                        'f1_score': f1_score(channel_y_true, channel_y_pred, zero_division=0),\n",
    "                        'precision': precision_score(channel_y_true, channel_y_pred, zero_division=0),\n",
    "                        'recall': recall_score(channel_y_true, channel_y_pred, zero_division=0),\n",
    "                        'balanced_accuracy': balanced_accuracy_score(channel_y_true, channel_y_pred),\n",
    "                        'n_test_channels': len(channel_predictions),\n",
    "                        'n_test_samples': len(y_test)\n",
    "                    }\n",
    "                    \n",
    "                    if len(np.unique(channel_y_true)) > 1:\n",
    "                        fold_results['roc_auc'] = roc_auc_score(channel_y_true, channel_y_proba)\n",
    "                    else:\n",
    "                        fold_results['roc_auc'] = 0.5\n",
    "                    \n",
    "                    cv_results[clf_name].append(fold_results)\n",
    "                    \n",
    "                    # 存储channel-level预测结果\n",
    "                    all_predictions[clf_name]['y_true'].extend(channel_y_true)\n",
    "                    all_predictions[clf_name]['y_pred'].extend(channel_y_pred)\n",
    "                    all_predictions[clf_name]['y_proba'].extend(channel_y_proba)\n",
    "                    all_predictions[clf_name]['test_patients'].extend([test_patient] * len(channel_predictions))\n",
    "                    all_predictions[clf_name]['channel_ids'].extend(channel_ids)\n",
    "                    \n",
    "                    print(f\"    {clf_name}: Channels - F1={fold_results['f1_score']:.3f}, \"\n",
    "                          f\"Acc={fold_results['accuracy']:.3f}, AUC={fold_results['roc_auc']:.3f} \"\n",
    "                          f\"({fold_results['n_test_channels']} channels)\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"    {clf_name}: Error - {e}\")\n",
    "        \n",
    "        return cv_results, all_predictions, samples_data\n",
    "    \n",
    "    def _aggregate_predictions_to_channel_level(self, test_samples, y_pred_samples, y_proba_samples):\n",
    "        \"\"\"\n",
    "        将sample-level的预测聚合为channel-level的预测\n",
    "        \n",
    "        对每个channel的所有样本（时间窗口）的预测概率取平均，然后基于平均概率进行分类\n",
    "        \"\"\"\n",
    "        \n",
    "        # 按channel_id分组\n",
    "        channel_groups = {}\n",
    "        for i, sample in enumerate(test_samples):\n",
    "            channel_id = sample['channel_id']\n",
    "            if channel_id not in channel_groups:\n",
    "                channel_groups[channel_id] = {\n",
    "                    'sample_indices': [],\n",
    "                    'true_label': sample['label'],  # 同一个channel的所有样本应该有相同的标签\n",
    "                    'patient_id': sample['patient_id']\n",
    "                }\n",
    "            channel_groups[channel_id]['sample_indices'].append(i)\n",
    "        \n",
    "        # 为每个channel计算平均预测概率\n",
    "        channel_predictions = []\n",
    "        for channel_id, group_info in channel_groups.items():\n",
    "            sample_indices = group_info['sample_indices']\n",
    "            \n",
    "            # 获取这个channel的所有样本的预测概率\n",
    "            channel_probas = y_proba_samples[sample_indices]\n",
    "            \n",
    "            # 计算平均概率\n",
    "            avg_proba = np.mean(channel_probas)\n",
    "            \n",
    "            # 基于平均概率进行分类 (threshold = 0.5)\n",
    "            pred_label = 1 if avg_proba > 0.5 else 0\n",
    "            \n",
    "            channel_predictions.append({\n",
    "                'channel_id': channel_id,\n",
    "                'true_label': group_info['true_label'],\n",
    "                'pred_label': pred_label,\n",
    "                'avg_proba': avg_proba,\n",
    "                'patient_id': group_info['patient_id'],\n",
    "                'n_samples': len(sample_indices)\n",
    "            })\n",
    "        \n",
    "        return channel_predictions\n",
    "    \n",
    "    def analyze_channel_level_results(self, cv_results, all_predictions):\n",
    "        \"\"\"分析channel-level交叉验证结果\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Channel-Level Analysis Results...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        final_results = {}\n",
    "        \n",
    "        for clf_name in self.classifiers.keys():\n",
    "            if len(cv_results[clf_name]) > 0:\n",
    "                # CV指标统计\n",
    "                cv_metrics = {}\n",
    "                for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'roc_auc']:\n",
    "                    values = [fold[metric] for fold in cv_results[clf_name]]\n",
    "                    cv_metrics[f'{metric}_mean'] = np.mean(values)\n",
    "                    cv_metrics[f'{metric}_std'] = np.std(values)\n",
    "                \n",
    "                # 整体预测指标\n",
    "                y_true_all = np.array(all_predictions[clf_name]['y_true'])\n",
    "                y_pred_all = np.array(all_predictions[clf_name]['y_pred'])\n",
    "                y_proba_all = np.array(all_predictions[clf_name]['y_proba'])\n",
    "                \n",
    "                overall_metrics = {\n",
    "                    'overall_accuracy': accuracy_score(y_true_all, y_pred_all),\n",
    "                    'overall_f1': f1_score(y_true_all, y_pred_all),\n",
    "                    'overall_precision': precision_score(y_true_all, y_pred_all),\n",
    "                    'overall_recall': recall_score(y_true_all, y_pred_all),\n",
    "                    'overall_balanced_acc': balanced_accuracy_score(y_true_all, y_pred_all)\n",
    "                }\n",
    "                \n",
    "                if len(np.unique(y_true_all)) > 1:\n",
    "                    overall_metrics['overall_roc_auc'] = roc_auc_score(y_true_all, y_proba_all)\n",
    "                else:\n",
    "                    overall_metrics['overall_roc_auc'] = 0.5\n",
    "                \n",
    "                # 合并结果\n",
    "                final_results[clf_name] = {\n",
    "                    **cv_metrics,\n",
    "                    **overall_metrics,\n",
    "                    'cv_folds': cv_results[clf_name],\n",
    "                    'predictions': all_predictions[clf_name],\n",
    "                    'confusion_matrix': confusion_matrix(y_true_all, y_pred_all)\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{clf_name}:\")\n",
    "                print(f\"  CV F1: {cv_metrics['f1_score_mean']:.3f} ± {cv_metrics['f1_score_std']:.3f}\")\n",
    "                print(f\"  Overall F1: {overall_metrics['overall_f1']:.3f}\")\n",
    "                print(f\"  Overall Balanced Acc: {overall_metrics['overall_balanced_acc']:.3f}\")\n",
    "                print(f\"  Overall ROC AUC: {overall_metrics['overall_roc_auc']:.3f}\")\n",
    "        \n",
    "        # 找到最佳分类器\n",
    "        if final_results:\n",
    "            best_classifier = max(final_results.items(), key=lambda x: x[1]['overall_f1'])\n",
    "            best_name, best_metrics = best_classifier\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"🏆 Best Channel-Level Classifier: {best_name}\")\n",
    "            print(f\"   Overall F1 Score: {best_metrics['overall_f1']:.3f}\")\n",
    "            print(f\"   CV F1: {best_metrics['f1_score_mean']:.3f} ± {best_metrics['f1_score_std']:.3f}\")\n",
    "            print(f\"   Overall Balanced Acc: {best_metrics['overall_balanced_acc']:.3f}\")\n",
    "            print(f\"   Overall ROC AUC: {best_metrics['overall_roc_auc']:.3f}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        else:\n",
    "            best_name, best_metrics = None, None\n",
    "        \n",
    "        return final_results, best_name, best_metrics\n",
    "    \n",
    "    def analyze_patient_level_performance(self, cv_results, all_predictions):\n",
    "        \"\"\"\n",
    "        分析每个patient的channel-level性能\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n📊 Patient-Level Channel Classification Performance\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 为每个分类器分析patient-level性能\n",
    "        for clf_name in self.classifiers.keys():\n",
    "            if len(cv_results[clf_name]) == 0:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{clf_name}:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Patient':<12} {'Channels':<10} {'Accuracy':<10} {'F1':<8} {'Precision':<10} {'Recall':<8}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            total_channels = 0\n",
    "            total_correct = 0\n",
    "            patient_f1_scores = []\n",
    "            \n",
    "            for fold_result in cv_results[clf_name]:\n",
    "                test_patient = fold_result['test_patient']\n",
    "                n_channels = fold_result['n_test_channels']\n",
    "                accuracy = fold_result['accuracy']\n",
    "                f1 = fold_result['f1_score']\n",
    "                precision = fold_result['precision']\n",
    "                recall = fold_result['recall']\n",
    "                \n",
    "                print(f\"{test_patient:<12} {n_channels:<10} {accuracy:<10.3f} {f1:<8.3f} {precision:<10.3f} {recall:<8.3f}\")\n",
    "                \n",
    "                total_channels += n_channels\n",
    "                total_correct += int(accuracy * n_channels)\n",
    "                patient_f1_scores.append(f1)\n",
    "            \n",
    "            # 计算总体统计\n",
    "            overall_accuracy = total_correct / total_channels if total_channels > 0 else 0\n",
    "            mean_patient_f1 = np.mean(patient_f1_scores) if patient_f1_scores else 0\n",
    "            std_patient_f1 = np.std(patient_f1_scores) if patient_f1_scores else 0\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Total':<12} {total_channels:<10} {overall_accuracy:<10.3f} {mean_patient_f1:<8.3f}\")\n",
    "            print(f\"Patient F1 Std: {std_patient_f1:.3f}\")\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def create_channel_level_visualization(self, use_normalized=True, cv_results=None, all_predictions=None):\n",
    "        \"\"\"创建channel-level的可视化\"\"\"\n",
    "        \n",
    "        print(f\"\\n📊 Creating Channel-Level Visualizations...\")\n",
    "        \n",
    "        # 准备数据\n",
    "        samples_data = self.prepare_dataset_for_channel_classification(use_normalized)\n",
    "        \n",
    "        if samples_data is None:\n",
    "            print(\"❌ No data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # 创建患者信息映射\n",
    "        unique_patients = list(set([s['patient_id'] for s in samples_data]))\n",
    "        patient_colors = {}\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_patients)))\n",
    "        \n",
    "        for i, patient in enumerate(unique_patients):\n",
    "            patient_colors[patient] = colors[i]\n",
    "        \n",
    "        # 1. 特征分布对比（标准化前后）\n",
    "        if use_normalized and self.channel_features_data:\n",
    "            self._create_normalization_effect_plot()\n",
    "        \n",
    "        # 2. 患者间channel分布\n",
    "        self._create_patient_channel_distribution_plot(samples_data, patient_colors)\n",
    "        \n",
    "        # 3. 特征重要性分析\n",
    "        self._create_feature_importance_plot(samples_data)\n",
    "        \n",
    "        # 4. 如果有CV结果，创建性能可视化\n",
    "        if cv_results and all_predictions:\n",
    "            self._create_performance_visualization(cv_results, all_predictions)\n",
    "        \n",
    "        print(f\"  💾 Visualizations saved to: {self.output_folder}\")\n",
    "    \n",
    "    def _create_normalization_effect_plot(self):\n",
    "        \"\"\"创建标准化效果对比图\"\"\"\n",
    "        \n",
    "        print(\"  📈 Creating normalization effect plots...\")\n",
    "        \n",
    "        # 选择几个代表性的channels进行对比\n",
    "        sample_channels = list(self.channel_features_data.keys())[:6]  # 前6个channel\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, channel_id in enumerate(sample_channels):\n",
    "            if i >= 6:\n",
    "                break\n",
    "                \n",
    "            # 原始数据\n",
    "            orig_features = pd.DataFrame(self.channel_features_data[channel_id]['features'])\n",
    "            \n",
    "            # 标准化数据\n",
    "            if channel_id in self.normalized_channel_features_data:\n",
    "                norm_features = pd.DataFrame(self.normalized_channel_features_data[channel_id]['features'])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # 选择一个代表性特征进行可视化\n",
    "            feature_name = 'rel_power_gamma'  # 或者选择其他特征\n",
    "            if feature_name in orig_features.columns:\n",
    "                \n",
    "                ax = axes[i]\n",
    "                \n",
    "                # 绘制原始数据分布\n",
    "                ax.hist(orig_features[feature_name], alpha=0.6, label='Original', bins=20, color='blue')\n",
    "                \n",
    "                # 绘制标准化数据分布\n",
    "                ax.hist(norm_features[feature_name], alpha=0.6, label='Normalized', bins=20, color='red')\n",
    "                \n",
    "                # 设置标签和标题\n",
    "                patient_id = self.channel_features_data[channel_id]['patient_id']\n",
    "                matter_type = self.channel_features_data[channel_id]['matter_type']\n",
    "                \n",
    "                ax.set_title(f'{patient_id}_{matter_type}\\n{feature_name}', fontsize=10)\n",
    "                ax.set_xlabel('Feature Value')\n",
    "                ax.set_ylabel('Frequency')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 隐藏多余的子图\n",
    "        for i in range(len(sample_channels), 6):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Channel-Level Normalization Effect', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'channel_normalization_effect.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_patient_channel_distribution_plot(self, samples_data, patient_colors):\n",
    "        \"\"\"创建患者间channel分布图\"\"\"\n",
    "        \n",
    "        print(\"  📊 Creating patient channel distribution plot...\")\n",
    "        \n",
    "        # 统计每个患者的channel数量\n",
    "        patient_stats = {}\n",
    "        channel_counts = {}  # 统计unique channels\n",
    "        \n",
    "        for sample in samples_data:\n",
    "            patient_id = sample['patient_id']\n",
    "            matter_type = sample['matter_type']\n",
    "            channel_id = sample['channel_id']\n",
    "            \n",
    "            if patient_id not in patient_stats:\n",
    "                patient_stats[patient_id] = {'grey': set(), 'white': set()}\n",
    "            \n",
    "            patient_stats[patient_id][matter_type].add(channel_id)\n",
    "            \n",
    "        # 转换为计数\n",
    "        for patient_id in patient_stats:\n",
    "            patient_stats[patient_id]['grey'] = len(patient_stats[patient_id]['grey'])\n",
    "            patient_stats[patient_id]['white'] = len(patient_stats[patient_id]['white'])\n",
    "        \n",
    "        # 创建图表\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 1. 每个患者的channel分布\n",
    "        patients = list(patient_stats.keys())\n",
    "        grey_counts = [patient_stats[p]['grey'] for p in patients]\n",
    "        white_counts = [patient_stats[p]['white'] for p in patients]\n",
    "        \n",
    "        x = np.arange(len(patients))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, grey_counts, width, label='Grey Matter', alpha=0.8, color='red')\n",
    "        ax1.bar(x + width/2, white_counts, width, label='White Matter', alpha=0.8, color='blue')\n",
    "        \n",
    "        ax1.set_xlabel('Patient ID')\n",
    "        ax1.set_ylabel('Number of Channels')\n",
    "        ax1.set_title('Channel Distribution by Patient')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(patients, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 总体分布饼图\n",
    "        total_grey = sum(grey_counts)\n",
    "        total_white = sum(white_counts)\n",
    "        \n",
    "        ax2.pie([total_grey, total_white], \n",
    "               labels=['Grey Matter', 'White Matter'],\n",
    "               colors=['red', 'blue'],\n",
    "               autopct='%1.1f%%')\n",
    "        ax2.set_title(f'Overall Channel Distribution\\n(Total: {total_grey + total_white} channels)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'patient_channel_distribution.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_patient_feature_visualization(self, samples_data, selected_features=None, max_patients_per_plot=8):\n",
    "        \"\"\"\n",
    "        为每个feature创建单独的可视化图，每个图包含所有患者的该feature分布\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        samples_data : list\n",
    "            样本数据列表\n",
    "        selected_features : list, optional\n",
    "            要可视化的特征列表，如果为None则使用所有特征\n",
    "        max_patients_per_plot : int\n",
    "            每个图最多显示的患者数量\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"  📊 Creating per-feature patient visualization...\")\n",
    "        \n",
    "        # 获取特征名称\n",
    "        if not samples_data:\n",
    "            print(\"    ❌ No samples data available\")\n",
    "            return\n",
    "            \n",
    "        # 从第一个sample获取特征名称\n",
    "        first_channel = list(self.channel_features_data.values())[0]\n",
    "        all_feature_names = list(first_channel['features'][0].keys())\n",
    "        \n",
    "        if selected_features is None:\n",
    "            # 选择一些代表性特征进行可视化\n",
    "            selected_features = [\n",
    "                'std', 'rms', 'area',\n",
    "                'rel_power_delta', 'rel_power_theta', 'rel_power_alpha', \n",
    "                'rel_power_beta', 'rel_power_gamma', 'rel_power_high_gamma',\n",
    "                'range', 'mad'\n",
    "            ]\n",
    "            # 只保留实际存在的特征\n",
    "            selected_features = [f for f in selected_features if f in all_feature_names]\n",
    "        \n",
    "        # 组织数据：按patient和matter_type分组\n",
    "        patient_feature_data = {}\n",
    "        feature_names = list(samples_data[0]['features'].keys()) if samples_data else []\n",
    "        \n",
    "        for sample in samples_data:\n",
    "            patient_id = sample['patient_id']\n",
    "            matter_type = sample['matter_type']\n",
    "            \n",
    "            if patient_id not in patient_feature_data:\n",
    "                patient_feature_data[patient_id] = {'grey': [], 'white': []}\n",
    "            \n",
    "            # 将features转换为dict格式（如果是array）\n",
    "            if isinstance(sample['features'], np.ndarray):\n",
    "                feature_dict = {name: sample['features'][i] for i, name in enumerate(feature_names)}\n",
    "            else:\n",
    "                feature_dict = sample['features']\n",
    "            \n",
    "            patient_feature_data[patient_id][matter_type].append(feature_dict)\n",
    "        \n",
    "        # 获取患者列表并排序\n",
    "        patients = sorted(list(patient_feature_data.keys()))\n",
    "        \n",
    "        # 如果患者太多，分批处理\n",
    "        patient_batches = [patients[i:i+max_patients_per_plot] \n",
    "                          for i in range(0, len(patients), max_patients_per_plot)]\n",
    "        \n",
    "        # 为每个选定的特征创建可视化\n",
    "        for feature_name in selected_features:\n",
    "            print(f\"    Creating visualization for feature: {feature_name}\")\n",
    "            \n",
    "            for batch_idx, patient_batch in enumerate(patient_batches):\n",
    "                # 计算subplot布局\n",
    "                n_patients = len(patient_batch)\n",
    "                n_cols = min(4, n_patients)  # 最多4列\n",
    "                n_rows = (n_patients + n_cols - 1) // n_cols  # 向上取整\n",
    "                \n",
    "                # 创建figure\n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "                \n",
    "                # 确保axes是2D数组\n",
    "                if n_rows == 1 and n_cols == 1:\n",
    "                    axes = np.array([[axes]])\n",
    "                elif n_rows == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                elif n_cols == 1:\n",
    "                    axes = axes.reshape(-1, 1)\n",
    "                \n",
    "                # 为每个患者创建subplot\n",
    "                for i, patient_id in enumerate(patient_batch):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    ax = axes[row, col]\n",
    "                    \n",
    "                    # 提取该患者该特征的数据\n",
    "                    grey_values = []\n",
    "                    white_values = []\n",
    "                    \n",
    "                    for sample_dict in patient_feature_data[patient_id]['grey']:\n",
    "                        if feature_name in sample_dict:\n",
    "                            grey_values.append(sample_dict[feature_name])\n",
    "                    \n",
    "                    for sample_dict in patient_feature_data[patient_id]['white']:\n",
    "                        if feature_name in sample_dict:\n",
    "                            white_values.append(sample_dict[feature_name])\n",
    "                    \n",
    "                    # 绘制分布\n",
    "                    if grey_values:\n",
    "                        ax.hist(grey_values, bins=20, alpha=0.6, label='Grey', color='red', density=True)\n",
    "                    if white_values:\n",
    "                        ax.hist(white_values, bins=20, alpha=0.6, label='White', color='blue', density=True)\n",
    "                    \n",
    "                    # 设置标题和标签\n",
    "                    ax.set_title(f'{patient_id}\\n(G:{len(grey_values)}, W:{len(white_values)})', fontsize=10)\n",
    "                    ax.set_xlabel(f'{feature_name}')\n",
    "                    ax.set_ylabel('Density')\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # 设置合适的x轴范围\n",
    "                    all_values = grey_values + white_values\n",
    "                    if all_values:\n",
    "                        ax.set_xlim(np.percentile(all_values, [1, 99]))\n",
    "                \n",
    "                # 隐藏多余的subplots\n",
    "                for i in range(len(patient_batch), n_rows * n_cols):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    axes[row, col].set_visible(False)\n",
    "                \n",
    "                # 设置整体标题\n",
    "                batch_suffix = f\"_batch{batch_idx+1}\" if len(patient_batches) > 1 else \"\"\n",
    "                fig.suptitle(f'Feature Distribution: {feature_name}{batch_suffix}', fontsize=16, y=0.98)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.93)  # 为suptitle留出空间\n",
    "                \n",
    "                # 保存图片\n",
    "                safe_feature_name = feature_name.replace('/', '_').replace('\\\\', '_')\n",
    "                filename = f'feature_{safe_feature_name}{batch_suffix}.png'\n",
    "                plt.savefig(os.path.join(self.output_folder, filename), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        print(f\"    ✅ Created visualizations for {len(selected_features)} features\")\n",
    "        return True\n",
    "    \n",
    "    def _create_feature_importance_plot(self, samples_data):\n",
    "        \"\"\"创建特征重要性图\"\"\"\n",
    "        \n",
    "        print(\"  🎯 Creating feature importance plot...\")\n",
    "        \n",
    "        # 准备数据\n",
    "        X = np.array([s['features'] for s in samples_data])\n",
    "        y = np.array([s['label'] for s in samples_data])\n",
    "        \n",
    "        # 使用随机森林计算特征重要性\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # 获取特征重要性\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        # 获取特征名称\n",
    "        first_channel = list(self.channel_features_data.values())[0]\n",
    "        feature_names = list(first_channel['features'][0].keys())\n",
    "        \n",
    "        # 排序\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # 绘制前20个最重要的特征\n",
    "        n_features = min(20, len(importances))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(n_features), importances[indices[:n_features]], alpha=0.8)\n",
    "        plt.yticks(range(n_features), [feature_names[i] for i in indices[:n_features]])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'feature_importance.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_performance_visualization(self, cv_results, all_predictions):\n",
    "        \"\"\"创建性能可视化图表\"\"\"\n",
    "        \n",
    "        print(\"  📈 Creating performance visualization...\")\n",
    "        \n",
    "        # 1. 分类器性能对比\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 准备数据\n",
    "        classifiers = []\n",
    "        f1_means = []\n",
    "        f1_stds = []\n",
    "        accuracies = []\n",
    "        aucs = []\n",
    "        \n",
    "        for clf_name in self.classifiers.keys():\n",
    "            if len(cv_results[clf_name]) > 0:\n",
    "                classifiers.append(clf_name)\n",
    "                \n",
    "                f1_values = [fold['f1_score'] for fold in cv_results[clf_name]]\n",
    "                f1_means.append(np.mean(f1_values))\n",
    "                f1_stds.append(np.std(f1_values))\n",
    "                \n",
    "                acc_values = [fold['accuracy'] for fold in cv_results[clf_name]]\n",
    "                accuracies.append(np.mean(acc_values))\n",
    "                \n",
    "                auc_values = [fold['roc_auc'] for fold in cv_results[clf_name]]\n",
    "                aucs.append(np.mean(auc_values))\n",
    "        \n",
    "        # F1 Score with error bars\n",
    "        x_pos = np.arange(len(classifiers))\n",
    "        ax1.bar(x_pos, f1_means, yerr=f1_stds, capsize=5, alpha=0.8)\n",
    "        ax1.set_xlabel('Classifiers')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('F1 Score by Classifier (with std)')\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(classifiers, rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        ax2.bar(x_pos, accuracies, alpha=0.8, color='orange')\n",
    "        ax2.set_xlabel('Classifiers')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy by Classifier')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(classifiers, rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC AUC comparison\n",
    "        ax3.bar(x_pos, aucs, alpha=0.8, color='green')\n",
    "        ax3.set_xlabel('Classifiers')\n",
    "        ax3.set_ylabel('ROC AUC')\n",
    "        ax3.set_title('ROC AUC by Classifier')\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(classifiers, rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Patient-wise performance for best classifier\n",
    "        if f1_means:\n",
    "            best_clf_idx = np.argmax(f1_means)\n",
    "            best_clf_name = classifiers[best_clf_idx]\n",
    "            \n",
    "            patient_f1s = [fold['f1_score'] for fold in cv_results[best_clf_name]]\n",
    "            patient_names = [fold['test_patient'] for fold in cv_results[best_clf_name]]\n",
    "            \n",
    "            ax4.bar(range(len(patient_f1s)), patient_f1s, alpha=0.8, color='purple')\n",
    "            ax4.set_xlabel('Patients')\n",
    "            ax4.set_ylabel('F1 Score')\n",
    "            ax4.set_title(f'Patient-wise F1 Score ({best_clf_name})')\n",
    "            ax4.set_xticks(range(len(patient_names)))\n",
    "            ax4.set_xticklabels(patient_names, rotation=45)\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'performance_comparison.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def run_complete_channel_level_analysis(self, \n",
    "                                          use_windowing=True,\n",
    "                                          window_size_ms=500,\n",
    "                                          step_size_ms=250,\n",
    "                                          max_windows_per_channel=200,\n",
    "                                          normalization_method='robust'):\n",
    "        \"\"\"\n",
    "        运行完整的channel-level分析\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"🧠 Channel-Level Grey/White Matter Classification Analysis\")\n",
    "        print(f\"=\" * 80)\n",
    "        \n",
    "        # 1. 加载患者数据\n",
    "        if not self.load_all_patients():\n",
    "            print(\"❌ Loading patient data failed\")\n",
    "            return None\n",
    "        \n",
    "        # 2. 提取channel-centric特征\n",
    "        if not self.extract_channel_centric_features(\n",
    "            use_windowing=use_windowing,\n",
    "            window_size_ms=window_size_ms,\n",
    "            step_size_ms=step_size_ms,\n",
    "            max_windows_per_channel=max_windows_per_channel\n",
    "        ):\n",
    "            print(\"❌ Channel feature extraction failed\")\n",
    "            return None\n",
    "        \n",
    "        # 3. 应用channel-specific标准化\n",
    "        if not self.apply_channel_specific_normalization(normalization_method):\n",
    "            print(\"❌ Channel-specific normalization failed\")\n",
    "            return None\n",
    "        \n",
    "        # 4. 主要分析：channel-level分类\n",
    "        cv_results, all_predictions, samples_data = self.leave_one_patient_out_validation_channel_level(\n",
    "            use_normalized=True\n",
    "        )\n",
    "        \n",
    "        final_results, best_name, best_metrics = self.analyze_channel_level_results(cv_results, all_predictions)\n",
    "        \n",
    "        # 5. 分析patient-level性能\n",
    "        self.analyze_patient_level_performance(cv_results, all_predictions)\n",
    "        \n",
    "        # 6. 创建可视化\n",
    "        self.create_channel_level_visualization(use_normalized=True, cv_results=cv_results, all_predictions=all_predictions)\n",
    "        \n",
    "        # 7. 保存结果\n",
    "        self._save_channel_level_results(final_results, best_name, normalization_method, cv_results)\n",
    "        \n",
    "        # 8. 汇总输出\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Channel-Level Analysis Complete\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        n_channels = len(self.channel_features_data)\n",
    "        n_grey = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 1)\n",
    "        n_white = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 0)\n",
    "        n_patients = len(set([ch['patient_id'] for ch in self.channel_features_data.values()]))\n",
    "        \n",
    "        print(f\"Number of Patients: {n_patients}\")\n",
    "        print(f\"Total Channels: {n_channels}\")\n",
    "        print(f\"Grey Matter Channels: {n_grey}\")\n",
    "        print(f\"White Matter Channels: {n_white}\")\n",
    "        print(f\"Normalization Method: {normalization_method}\")\n",
    "        \n",
    "        if best_name and best_metrics:\n",
    "            print(f\"\\n🏆 Best Channel-Level Classifier: {best_name}\")\n",
    "            print(f\"   Overall F1 Score: {best_metrics['overall_f1']:.3f}\")\n",
    "            print(f\"   CV F1: {best_metrics['f1_score_mean']:.3f} ± {best_metrics['f1_score_std']:.3f}\")\n",
    "            print(f\"   Overall Balanced Acc: {best_metrics['overall_balanced_acc']:.3f}\")\n",
    "            print(f\"   Overall ROC AUC: {best_metrics['overall_roc_auc']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n📁 Results saved to: {self.output_folder}\")\n",
    "        \n",
    "        return {\n",
    "            'final_results': final_results,\n",
    "            'best_classifier': best_name,\n",
    "            'best_metrics': best_metrics,\n",
    "            'samples_data': samples_data,\n",
    "            'cv_results': cv_results,\n",
    "            'all_predictions': all_predictions,\n",
    "            'n_channels': n_channels,\n",
    "            'n_patients': n_patients\n",
    "        }\n",
    "    \n",
    "    def _save_channel_level_results(self, final_results, best_name, normalization_method, cv_results):\n",
    "        \"\"\"保存channel-level结果\"\"\"\n",
    "        \n",
    "        # 保存分类结果\n",
    "        if final_results:\n",
    "            summary_data = []\n",
    "            for clf_name, metrics in final_results.items():\n",
    "                summary_data.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'CV_F1_Mean': metrics['f1_score_mean'],\n",
    "                    'CV_F1_Std': metrics['f1_score_std'],\n",
    "                    'Overall_F1': metrics['overall_f1'],\n",
    "                    'Overall_Balanced_Acc': metrics['overall_balanced_acc'],\n",
    "                    'Overall_ROC_AUC': metrics['overall_roc_auc'],\n",
    "                    'Normalization_Method': normalization_method,\n",
    "                    'Analysis_Level': 'channel'\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_csv(\n",
    "                os.path.join(self.output_folder, f'channel_level_classification_summary_{normalization_method}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # 保存patient-level详细结果\n",
    "        if cv_results:\n",
    "            patient_results = []\n",
    "            for clf_name, folds in cv_results.items():\n",
    "                for fold in folds:\n",
    "                    patient_results.append({\n",
    "                        'Classifier': clf_name,\n",
    "                        'Patient': fold['test_patient'],\n",
    "                        'Fold': fold['fold'],\n",
    "                        'N_Channels': fold['n_test_channels'],\n",
    "                        'N_Samples': fold['n_test_samples'],\n",
    "                        'Accuracy': fold['accuracy'],\n",
    "                        'F1_Score': fold['f1_score'],\n",
    "                        'Precision': fold['precision'],\n",
    "                        'Recall': fold['recall'],\n",
    "                        'Balanced_Accuracy': fold['balanced_accuracy'],\n",
    "                        'ROC_AUC': fold['roc_auc']\n",
    "                    })\n",
    "            \n",
    "            patient_df = pd.DataFrame(patient_results)\n",
    "            patient_df.to_csv(\n",
    "                os.path.join(self.output_folder, f'patient_level_channel_results_{normalization_method}.csv'),\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        print(f\"  💾 Channel-level results saved\")\n",
    "\n",
    "\n",
    "def compare_windowing_strategies(processed_folder, output_base='comparison_results'):\n",
    "    \"\"\"比较不同窗口策略的性能\"\"\"\n",
    "    \n",
    "    strategies = [\n",
    "        {'use_windowing': True, 'window_size_ms': 10000, 'step_size_ms': 5000, 'name': 'window_1000ms'},\n",
    "        {'use_windowing': True, 'window_size_ms': 5000, 'step_size_ms': 2500, 'name': 'window_500ms'},\n",
    "        {'use_windowing': True, 'window_size_ms': 2500, 'step_size_ms': 1250, 'name': 'window_250ms'}\n",
    "    ]\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        strategy_name = strategy.pop('name')\n",
    "        output_folder = f\"{output_base}_{strategy_name}\"\n",
    "        \n",
    "        print(f\"\\nStrategy Testing: {strategy_name}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        classifier = MultiPatientClassifier(processed_folder, output_folder)\n",
    "        results = classifier.run_complete_analysis(**strategy)\n",
    "        \n",
    "        if results:\n",
    "            comparison_results[strategy_name] = {\n",
    "                'best_classifier': results['best_classifier'],\n",
    "                'best_f1': results['best_metrics']['overall_f1'],\n",
    "                'best_balanced_acc': results['best_metrics']['overall_balanced_acc'],\n",
    "                'n_features': results['data_summary']['n_features'],\n",
    "                'total_samples': results['data_summary']['total_samples']\n",
    "            }\n",
    "    \n",
    "    # 创建对比报告\n",
    "    if comparison_results:\n",
    "        comparison_df = pd.DataFrame(comparison_results).T\n",
    "        comparison_df.to_csv(os.path.join(output_base, 'strategy_comparison.csv'))\n",
    "        \n",
    "        print(f\"\\n策略对比结果:\")\n",
    "        print(comparison_df.to_string())\n",
    "        \n",
    "        # 找到最佳策略\n",
    "        best_strategy = comparison_df['best_f1'].idxmax()\n",
    "        print(f\"\\n🏆 最佳策略: {best_strategy}\")\n",
    "        print(f\"   F1分数: {comparison_df.loc[best_strategy, 'best_f1']:.3f}\")\n",
    "    \n",
    "    return comparison_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6965b16e6e1782a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 设置路径\n",
    "processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_test\"\n",
    "output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_alt\"\n",
    "\n",
    "print(\"多Patient Grey/White Matter分类系统\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 创建分类器实例\n",
    "classifier = MultiPatientClassifier(\n",
    "    processed_folder=processed_folder,\n",
    "    output_folder=output_folder\n",
    ")\n",
    "\n",
    "# 运行完整分析\n",
    "results = classifier.run_complete_channel_level_analysis(\n",
    "    use_windowing=True,\n",
    "    window_size_ms=10000,\n",
    "    step_size_ms=5000,\n",
    "    max_windows_per_channel=200,\n",
    "    normalization_method='robust'\n",
    ")\n",
    "\n",
    "classifier._create_patient_feature_visualization(samples_data=results['samples_data'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a120d07483055b14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\"\n",
    "output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results\"\n",
    "\n",
    "results_2 = compare_windowing_strategies(processed_folder, output_base=r'D:\\BlcRepo\\LabCode\\SeizureProp\\result\\gw_comparison_results')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aac45254a5c1905a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PatientFeatureVisualizer:\n",
    "    \"\"\"\n",
    "    可视化每个患者的特征分布，用于分析为什么grey/white matter分类性能不好\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_folder, output_folder='feature_visualization'):\n",
    "        \"\"\"\n",
    "        初始化可视化器\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        processed_folder : str\n",
    "            包含预处理pkl文件的文件夹路径\n",
    "        output_folder : str\n",
    "            可视化结果输出文件夹\n",
    "        \"\"\"\n",
    "        self.processed_folder = processed_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.patients_data = {}\n",
    "        self.combined_features = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "        # 创建输出文件夹\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # 设置颜色调色板\n",
    "        self.patient_colors = plt.cm.Set3(np.linspace(0, 1, 20))  # 支持最多20个患者\n",
    "        \n",
    "        print(f\"Initializing Patient Feature Visualizer\")\n",
    "        print(f\"Processed folder: {processed_folder}\")\n",
    "        print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    def load_and_extract_features(self):\n",
    "        \"\"\"加载所有患者数据并提取特征\"\"\"\n",
    "        \n",
    "        pkl_files = glob.glob(os.path.join(self.processed_folder, \"P*_processed.pkl\"))\n",
    "        print(f\"\\nFound {len(pkl_files)} patient files\")\n",
    "        \n",
    "        all_patient_features = []\n",
    "        \n",
    "        for pkl_file in pkl_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                pid = data['patient_id']\n",
    "                self.patients_data[pid] = data\n",
    "                \n",
    "                # 提取特征（重用之前的代码逻辑）\n",
    "                patient_features = self._extract_patient_features(pid, data)\n",
    "                \n",
    "                if patient_features is not None:\n",
    "                    all_patient_features.append(patient_features)\n",
    "                    print(f\"✓ {pid}: {patient_features['n_samples']} samples\")\n",
    "                else:\n",
    "                    print(f\"✗ {pid}: Feature extraction failed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load {os.path.basename(pkl_file)}: {e}\")\n",
    "        \n",
    "        # 合并所有患者的特征\n",
    "        if all_patient_features:\n",
    "            self._combine_all_features(all_patient_features)\n",
    "            print(f\"\\nSuccessfully processed {len(all_patient_features)} patients\")\n",
    "            print(f\"Total samples: {len(self.combined_features)}\")\n",
    "            print(f\"Total features: {len(self.feature_names)}\")\n",
    "        else:\n",
    "            print(\"No valid patient features extracted\")\n",
    "        \n",
    "        return len(all_patient_features) > 0\n",
    "    \n",
    "    def _extract_patient_features(self, patient_id, patient_data):\n",
    "        \"\"\"为单个患者提取特征（从MultiPatientClassifier适配）\"\"\"\n",
    "        \n",
    "        matter_data = patient_data['matter_data']\n",
    "        recordings = patient_data['recordings']\n",
    "        \n",
    "        # 提取电极分类\n",
    "        try:\n",
    "            grey_indices, white_indices, classification_info = self._extract_electrode_classification(matter_data)\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {patient_id} electrode classification failed: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if len(grey_indices) == 0 or len(white_indices) == 0:\n",
    "            print(f\"  ✗ {patient_id} missing grey or white matter electrodes\")\n",
    "            return None\n",
    "        \n",
    "        # 合并所有recordings的数据\n",
    "        all_grey_data = []\n",
    "        all_white_data = []\n",
    "        fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "        \n",
    "        for recording in recordings:\n",
    "            neural_data = recording['neural_data_processed']\n",
    "            grey_data = neural_data[:, grey_indices]\n",
    "            white_data = neural_data[:, white_indices]\n",
    "            all_grey_data.append(grey_data)\n",
    "            all_white_data.append(white_data)\n",
    "        \n",
    "        combined_grey = np.vstack(all_grey_data) if all_grey_data else np.array([])\n",
    "        combined_white = np.vstack(all_white_data) if all_white_data else np.array([])\n",
    "        \n",
    "        # 创建窗口样本\n",
    "        all_samples = []\n",
    "        \n",
    "        # Grey matter samples\n",
    "        grey_samples = self._create_windowed_samples(\n",
    "            combined_grey, grey_indices, 1, patient_id, fs=fs\n",
    "        )\n",
    "        \n",
    "        # White matter samples  \n",
    "        white_samples = self._create_windowed_samples(\n",
    "            combined_white, white_indices, 0, patient_id, fs=fs\n",
    "        )\n",
    "        \n",
    "        all_samples = grey_samples + white_samples\n",
    "        \n",
    "        if len(all_samples) == 0:\n",
    "            return None\n",
    "        \n",
    "        # 转换为DataFrame\n",
    "        features_df = pd.DataFrame(all_samples)\n",
    "        \n",
    "        # 分离特征和标签\n",
    "        meta_columns = ['patient_id', 'electrode_idx', 'channel_idx', 'window_start', 'label']\n",
    "        feature_columns = [col for col in features_df.columns if col not in meta_columns]\n",
    "        \n",
    "        # 数据清理\n",
    "        features_df[feature_columns] = features_df[feature_columns].fillna(0)\n",
    "        features_df[feature_columns] = features_df[feature_columns].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'features': features_df[feature_columns],\n",
    "            'labels': features_df['label'].values,\n",
    "            'meta': features_df[meta_columns],\n",
    "            'n_samples': len(features_df),\n",
    "            'n_grey_samples': np.sum(features_df['label'] == 1),\n",
    "            'n_white_samples': np.sum(features_df['label'] == 0)\n",
    "        }\n",
    "    \n",
    "    def _extract_electrode_classification(self, matter_data):\n",
    "        \"\"\"提取电极分类（从MultiPatientClassifier适配）\"\"\"\n",
    "        \n",
    "        matter_columns = ['MatterType', 'matter', 'Matter', 'mattertype', 'tissue_type', 'type']\n",
    "        matter_col = None\n",
    "        \n",
    "        for col in matter_columns:\n",
    "            if col in matter_data.columns:\n",
    "                matter_col = col\n",
    "                break\n",
    "        \n",
    "        if matter_col is None:\n",
    "            raise ValueError(f\"Matter type column not found. Available: {matter_data.columns.tolist()}\")\n",
    "        \n",
    "        matter_values = matter_data[matter_col].astype(str).str.lower()\n",
    "        \n",
    "        grey_mask = matter_values.isin(['G', 'g', 'Grey', 'grey', 'Gray', 'gray'])\n",
    "        white_mask = matter_values.isin(['W', 'w', 'White', 'white'])\n",
    "        \n",
    "        # 如果G/W格式没找到，尝试包含匹配\n",
    "        if np.sum(grey_mask) == 0 or np.sum(white_mask) == 0:\n",
    "            matter_values_lower = matter_values.str.lower()\n",
    "            \n",
    "            if np.sum(grey_mask) == 0:\n",
    "                grey_patterns = ['grey', 'gray', 'cortex', 'cortical']\n",
    "                grey_mask = matter_values_lower.str.contains('|'.join(grey_patterns), na=False, case=False)\n",
    "            \n",
    "            if np.sum(white_mask) == 0:\n",
    "                white_patterns = ['white']\n",
    "                white_mask = matter_values_lower.str.contains('|'.join(white_patterns), na=False, case=False)\n",
    "        \n",
    "        grey_indices = matter_data.index[grey_mask].tolist()\n",
    "        white_indices = matter_data.index[white_mask].tolist()\n",
    "        \n",
    "        classification_info = {\n",
    "            'matter_column': matter_col,\n",
    "            'total_electrodes': len(matter_data),\n",
    "            'grey_electrodes': len(grey_indices),\n",
    "            'white_electrodes': len(white_indices),\n",
    "            'grey_indices': grey_indices,\n",
    "            'white_indices': white_indices\n",
    "        }\n",
    "        \n",
    "        return grey_indices, white_indices, classification_info\n",
    "    \n",
    "    def _create_windowed_samples(self, data, electrode_indices, label, patient_id, \n",
    "                                window_size_ms=50000, step_size_ms=25000, max_windows_per_channel=200, fs=512):\n",
    "        \"\"\"创建时间窗口样本\"\"\"\n",
    "        \n",
    "        samples = []\n",
    "        window_samples = int(window_size_ms * fs / 1000)\n",
    "        step_samples = int(step_size_ms * fs / 1000)\n",
    "        \n",
    "        n_time, n_channels = data.shape\n",
    "        \n",
    "        for ch_idx, electrode_idx in enumerate(electrode_indices):\n",
    "            channel_data = data[:, ch_idx]\n",
    "            channel_windows = []\n",
    "            \n",
    "            for start in range(0, n_time - window_samples + 1, step_samples):\n",
    "                end = start + window_samples\n",
    "                window_data = channel_data[start:end]\n",
    "                \n",
    "                features = self._extract_signal_features(window_data, fs)\n",
    "                if features is not None:\n",
    "                    features['patient_id'] = patient_id\n",
    "                    features['electrode_idx'] = electrode_idx\n",
    "                    features['channel_idx'] = ch_idx\n",
    "                    features['window_start'] = start\n",
    "                    features['label'] = label\n",
    "                    \n",
    "                    channel_windows.append(features)\n",
    "                \n",
    "                if len(channel_windows) >= max_windows_per_channel:\n",
    "                    break\n",
    "            \n",
    "            # 随机采样限制窗口数\n",
    "            if len(channel_windows) > max_windows_per_channel:\n",
    "                indices = np.random.choice(len(channel_windows), max_windows_per_channel, replace=False)\n",
    "                channel_windows = [channel_windows[i] for i in sorted(indices)]\n",
    "            \n",
    "            samples.extend(channel_windows)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _extract_signal_features(self, signal, fs):\n",
    "        \"\"\"提取单个信号的特征（从MultiPatientClassifier适配）\"\"\"\n",
    "        \n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 时域特征\n",
    "        features['mean'] = np.mean(signal)\n",
    "        features['std'] = np.std(signal)\n",
    "        features['var'] = np.var(signal)\n",
    "        features['median'] = np.median(signal)\n",
    "        features['mad'] = np.median(np.abs(signal - np.median(signal)))\n",
    "        features['range'] = np.ptp(signal)\n",
    "        features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        \n",
    "        # 统计矩\n",
    "        try:\n",
    "            features['skewness'] = stats.skew(signal)\n",
    "            features['kurtosis'] = stats.kurtosis(signal)\n",
    "        except:\n",
    "            features['skewness'] = 0\n",
    "            features['kurtosis'] = 0\n",
    "        \n",
    "        # 信号复杂度\n",
    "        features['zero_crossings'] = np.sum(np.diff(np.signbit(signal)))\n",
    "        features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "        features['area'] = np.sum(np.abs(signal))\n",
    "        features['energy'] = np.sum(signal**2)\n",
    "        \n",
    "        # 频域特征（简化版本，避免错误）\n",
    "        try:\n",
    "            from scipy.fftpack import fft\n",
    "            n_fft = len(signal)\n",
    "            windowed_signal = signal * np.hamming(n_fft)\n",
    "            fft_vals = fft(windowed_signal)\n",
    "            fft_mag = np.abs(fft_vals[:n_fft//2])\n",
    "            freqs = np.fft.fftfreq(n_fft, 1/fs)[:n_fft//2]\n",
    "            \n",
    "            # 频带功率\n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 100),\n",
    "                'high_gamma': (100, min(200, fs/2))\n",
    "            }\n",
    "            \n",
    "            total_power = np.sum(fft_mag**2)\n",
    "            for band_name, (low, high) in bands.items():\n",
    "                band_mask = (freqs >= low) & (freqs <= high)\n",
    "                if np.any(band_mask):\n",
    "                    band_power = np.sum(fft_mag[band_mask]**2)\n",
    "                    features[f'power_{band_name}'] = band_power\n",
    "                    features[f'rel_power_{band_name}'] = band_power / total_power if total_power > 0 else 0\n",
    "                else:\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                    features[f'rel_power_{band_name}'] = 0\n",
    "            \n",
    "            features['total_power'] = total_power\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 如果频域分析失败，设置默认值\n",
    "            bands = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high_gamma']\n",
    "            for band in bands:\n",
    "                features[f'power_{band}'] = 0\n",
    "                features[f'rel_power_{band}'] = 0\n",
    "            features['total_power'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _combine_all_features(self, all_patient_features):\n",
    "        \"\"\"合并所有患者的特征\"\"\"\n",
    "        \n",
    "        # 找到共同的特征列\n",
    "        all_feature_columns = []\n",
    "        for patient_data in all_patient_features:\n",
    "            all_feature_columns.append(set(patient_data['features'].columns))\n",
    "        \n",
    "        common_features = set.intersection(*all_feature_columns)\n",
    "        self.feature_names = list(common_features)\n",
    "        \n",
    "        # 合并数据\n",
    "        combined_data = []\n",
    "        for patient_data in all_patient_features:\n",
    "            patient_id = patient_data['patient_id']\n",
    "            features = patient_data['features'][self.feature_names]\n",
    "            labels = patient_data['labels']\n",
    "            \n",
    "            # 添加患者ID到每一行\n",
    "            for i in range(len(features)):\n",
    "                row = {\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': labels[i],\n",
    "                    'matter_type': 'Grey' if labels[i] == 1 else 'White'\n",
    "                }\n",
    "                # 添加特征值\n",
    "                for feature_name in self.feature_names:\n",
    "                    row[feature_name] = features.iloc[i][feature_name]\n",
    "                \n",
    "                combined_data.append(row)\n",
    "        \n",
    "        self.combined_features = pd.DataFrame(combined_data)\n",
    "        \n",
    "        # 数据清理\n",
    "        feature_cols = [col for col in self.combined_features.columns \n",
    "                       if col not in ['patient_id', 'label', 'matter_type']]\n",
    "        self.combined_features[feature_cols] = self.combined_features[feature_cols].fillna(0)\n",
    "        self.combined_features[feature_cols] = self.combined_features[feature_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    def create_individual_feature_plots(self, n_features_per_page=9):\n",
    "        \"\"\"为每个特征创建患者分布的散点图\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating individual feature distribution plots...\")\n",
    "        \n",
    "        # 获取患者列表和颜色映射\n",
    "        patients = self.combined_features['patient_id'].unique()\n",
    "        patient_color_map = {patient: self.patient_colors[i % len(self.patient_colors)] \n",
    "                           for i, patient in enumerate(patients)}\n",
    "        \n",
    "        # 分页处理特征\n",
    "        n_pages = (len(self.feature_names) + n_features_per_page - 1) // n_features_per_page\n",
    "        \n",
    "        for page in range(n_pages):\n",
    "            start_idx = page * n_features_per_page\n",
    "            end_idx = min(start_idx + n_features_per_page, len(self.feature_names))\n",
    "            page_features = self.feature_names[start_idx:end_idx]\n",
    "            \n",
    "            # 计算子图布局\n",
    "            n_features_in_page = len(page_features)\n",
    "            n_cols = 3\n",
    "            n_rows = (n_features_in_page + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "            if n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            if n_features_in_page == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            \n",
    "            fig.suptitle(f'Feature Distribution by Patient - Page {page+1}/{n_pages}', \n",
    "                        fontsize=16, y=0.98)\n",
    "            \n",
    "            for i, feature_name in enumerate(page_features):\n",
    "                row = i // n_cols\n",
    "                col = i % n_cols\n",
    "                ax = axes[row, col]\n",
    "                \n",
    "                # 为每个患者创建散点图\n",
    "                for j, patient in enumerate(patients):\n",
    "                    patient_data = self.combined_features[self.combined_features['patient_id'] == patient]\n",
    "                    \n",
    "                    # Grey matter points\n",
    "                    grey_data = patient_data[patient_data['matter_type'] == 'Grey']\n",
    "                    if len(grey_data) > 0:\n",
    "                        y_grey = np.random.normal(1, 0.1, len(grey_data))  # 添加垂直抖动\n",
    "                        ax.scatter(grey_data[feature_name], y_grey, \n",
    "                                 c=[patient_color_map[patient]], alpha=0.6, s=20,\n",
    "                                 label=f'{patient}_Grey' if j == 0 else \"\")\n",
    "                    \n",
    "                    # White matter points\n",
    "                    white_data = patient_data[patient_data['matter_type'] == 'White']\n",
    "                    if len(white_data) > 0:\n",
    "                        y_white = np.random.normal(0, 0.1, len(white_data))  # 添加垂直抖动\n",
    "                        ax.scatter(white_data[feature_name], y_white, \n",
    "                                 c=[patient_color_map[patient]], alpha=0.6, s=20, \n",
    "                                 marker='s', label=f'{patient}_White' if j == 0 else \"\")\n",
    "                \n",
    "                ax.set_xlabel(feature_name)\n",
    "                ax.set_ylabel('Matter Type')\n",
    "                ax.set_yticks([0, 1])\n",
    "                ax.set_yticklabels(['White', 'Grey'])\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_title(f'{feature_name}', fontsize=10)\n",
    "                \n",
    "                # 添加箱线图层覆盖显示分布\n",
    "                grey_values = self.combined_features[self.combined_features['matter_type'] == 'Grey'][feature_name]\n",
    "                white_values = self.combined_features[self.combined_features['matter_type'] == 'White'][feature_name]\n",
    "                \n",
    "                if len(grey_values) > 0 and len(white_values) > 0:\n",
    "                    # 计算简单的分布统计\n",
    "                    grey_median = np.median(grey_values)\n",
    "                    white_median = np.median(white_values)\n",
    "                    \n",
    "                    ax.axvline(grey_median, ymax=1.25, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "                    ax.axvline(white_median, ymax=0.25, color='blue', linestyle='--', alpha=0.7, linewidth=2)\n",
    "            \n",
    "            # 隐藏空的子图\n",
    "            for i in range(n_features_in_page, n_rows * n_cols):\n",
    "                row = i // n_cols\n",
    "                col = i % n_cols\n",
    "                axes[row, col].set_visible(False)\n",
    "            \n",
    "            # 添加患者颜色图例\n",
    "            handles = []\n",
    "            labels = []\n",
    "            for patient in patients:\n",
    "                handles.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                        markerfacecolor=patient_color_map[patient], markersize=8))\n",
    "                labels.append(patient)\n",
    "            \n",
    "            fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.02), \n",
    "                      ncol=min(6, len(patients)), fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(bottom=0.15, top=0.92)\n",
    "            plt.savefig(os.path.join(self.output_folder, f'feature_distributions_page_{page+1}.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  ✓ Page {page+1}/{n_pages} saved\")\n",
    "    \n",
    "    def create_feature_summary_statistics(self):\n",
    "        \"\"\"创建特征统计汇总表\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating feature summary statistics...\")\n",
    "        \n",
    "        summary_stats = []\n",
    "        \n",
    "        for feature_name in self.feature_names:\n",
    "            # 整体统计\n",
    "            overall_mean = self.combined_features[feature_name].mean()\n",
    "            overall_std = self.combined_features[feature_name].std()\n",
    "            \n",
    "            # 按matter type分组统计\n",
    "            grey_data = self.combined_features[self.combined_features['matter_type'] == 'Grey'][feature_name]\n",
    "            white_data = self.combined_features[self.combined_features['matter_type'] == 'White'][feature_name]\n",
    "            \n",
    "            grey_mean = grey_data.mean()\n",
    "            grey_std = grey_data.std()\n",
    "            white_mean = white_data.mean()\n",
    "            white_std = white_data.std()\n",
    "            \n",
    "            # 统计检验\n",
    "            try:\n",
    "                t_stat, p_value = stats.ttest_ind(grey_data, white_data)\n",
    "            except:\n",
    "                t_stat, p_value = 0, 1\n",
    "            \n",
    "            # 效应量 (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(grey_data) - 1) * grey_std**2 + \n",
    "                                 (len(white_data) - 1) * white_std**2) / \n",
    "                                (len(grey_data) + len(white_data) - 2))\n",
    "            cohens_d = (grey_mean - white_mean) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            # 按患者分组的变异系数\n",
    "            patient_means = []\n",
    "            for patient in self.combined_features['patient_id'].unique():\n",
    "                patient_data = self.combined_features[self.combined_features['patient_id'] == patient][feature_name]\n",
    "                if len(patient_data) > 0:\n",
    "                    patient_means.append(patient_data.mean())\n",
    "            \n",
    "            between_patient_cv = np.std(patient_means) / np.mean(patient_means) if len(patient_means) > 0 and np.mean(patient_means) != 0 else 0\n",
    "            \n",
    "            summary_stats.append({\n",
    "                'feature_name': feature_name,\n",
    "                'overall_mean': overall_mean,\n",
    "                'overall_std': overall_std,\n",
    "                'grey_mean': grey_mean,\n",
    "                'grey_std': grey_std,\n",
    "                'white_mean': white_mean,\n",
    "                'white_std': white_std,\n",
    "                'mean_difference': grey_mean - white_mean,\n",
    "                'cohens_d': cohens_d,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'between_patient_cv': between_patient_cv\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df = summary_df.sort_values('p_value')  # 按p值排序\n",
    "        \n",
    "        # 保存统计结果\n",
    "        summary_df.to_csv(os.path.join(self.output_folder, 'feature_statistics.csv'), index=False)\n",
    "        \n",
    "        # 创建统计可视化\n",
    "        self._plot_feature_statistics(summary_df)\n",
    "        \n",
    "        print(f\"  ✓ Feature statistics saved\")\n",
    "        return summary_df\n",
    "    \n",
    "    def _plot_feature_statistics(self, summary_df):\n",
    "        \"\"\"可视化特征统计结果\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Effect size (Cohen's d) 分布\n",
    "        axes[0, 0].hist(summary_df['cohens_d'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[0, 0].set_xlabel(\"Cohen's d\")\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Effect Size Distribution')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. P-value 分布\n",
    "        axes[0, 1].hist(summary_df['p_value'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(0.05, color='red', linestyle='--', alpha=0.7, label='p=0.05')\n",
    "        axes[0, 1].set_xlabel('P-value')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('P-value Distribution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. 患者间变异系数\n",
    "        axes[1, 0].hist(summary_df['between_patient_cv'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Between-Patient CV')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Between-Patient Variability')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Effect size vs P-value\n",
    "        significant = summary_df['significant']\n",
    "        axes[1, 1].scatter(summary_df.loc[~significant, 'cohens_d'], \n",
    "                          summary_df.loc[~significant, 'p_value'], \n",
    "                          alpha=0.6, label='Non-significant', color='gray')\n",
    "        axes[1, 1].scatter(summary_df.loc[significant, 'cohens_d'], \n",
    "                          summary_df.loc[significant, 'p_value'], \n",
    "                          alpha=0.8, label='Significant', color='red')\n",
    "        axes[1, 1].axhline(0.05, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1, 1].axvline(0, color='gray', linestyle='-', alpha=0.5)\n",
    "        axes[1, 1].set_xlabel(\"Cohen's d\")\n",
    "        axes[1, 1].set_ylabel('P-value')\n",
    "        axes[1, 1].set_title('Effect Size vs Statistical Significance')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'feature_statistics_plots.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def create_patient_comparison_plots(self):\n",
    "        \"\"\"创建患者间比较图\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating patient comparison plots...\")\n",
    "        \n",
    "        # 计算每个患者的特征均值\n",
    "        patient_means = []\n",
    "        patients = self.combined_features['patient_id'].unique()\n",
    "        \n",
    "        for patient in patients:\n",
    "            patient_data = self.combined_features[self.combined_features['patient_id'] == patient]\n",
    "            \n",
    "            # 分别计算grey和white matter的均值\n",
    "            grey_means = patient_data[patient_data['matter_type'] == 'Grey'][self.feature_names].mean()\n",
    "            white_means = patient_data[patient_data['matter_type'] == 'White'][self.feature_names].mean()\n",
    "            \n",
    "            patient_means.append({\n",
    "                'patient_id': patient,\n",
    "                'matter_type': 'Grey',\n",
    "                **grey_means.to_dict()\n",
    "            })\n",
    "            \n",
    "            patient_means.append({\n",
    "                'patient_id': patient,\n",
    "                'matter_type': 'White', \n",
    "                **white_means.to_dict()\n",
    "            })\n",
    "        \n",
    "        patient_means_df = pd.DataFrame(patient_means)\n",
    "        \n",
    "        # 1. 患者相似性热图\n",
    "        self._create_patient_similarity_heatmap(patient_means_df)\n",
    "        \n",
    "        # 2. PCA分析\n",
    "        self._create_pca_analysis()\n",
    "        \n",
    "        # 3. 最具区分性的特征\n",
    "        self._create_discriminative_features_plot()\n",
    "        \n",
    "        print(f\"  ✓ Patient comparison plots saved\")\n",
    "    \n",
    "    def _create_patient_similarity_heatmap(self, patient_means_df):\n",
    "        \"\"\"创建患者相似性热图\"\"\"\n",
    "        \n",
    "        # 只使用grey matter数据进行患者相似性分析\n",
    "        grey_means = patient_means_df[patient_means_df['matter_type'] == 'Grey']\n",
    "        \n",
    "        # 计算患者间的相关性\n",
    "        patients = grey_means['patient_id'].values\n",
    "        feature_matrix = grey_means[self.feature_names].values\n",
    "        \n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "        \n",
    "        # 计算相关性矩阵\n",
    "        correlation_matrix = np.corrcoef(feature_matrix_scaled)\n",
    "        \n",
    "        # 创建热图\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   xticklabels=patients, yticklabels=patients, fmt='.2f')\n",
    "        plt.title('Patient Similarity (Grey Matter Features)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'patient_similarity_heatmap.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_pca_analysis(self):\n",
    "        \"\"\"创建PCA分析图\"\"\"\n",
    "        \n",
    "        # 准备数据\n",
    "        feature_data = self.combined_features[self.feature_names]\n",
    "        labels = self.combined_features['matter_type']\n",
    "        patients = self.combined_features['patient_id']\n",
    "        \n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        feature_data_scaled = scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # PCA\n",
    "        pca = PCA(n_components=10)\n",
    "        pca_result = pca.fit_transform(feature_data_scaled)\n",
    "        \n",
    "        # 创建图表\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 1. 按matter type着色\n",
    "        unique_patients = patients.unique()\n",
    "        patient_color_map = {patient: self.patient_colors[i % len(self.patient_colors)] \n",
    "                           for i, patient in enumerate(unique_patients)}\n",
    "        \n",
    "        for matter_type in ['Grey', 'White']:\n",
    "            mask = labels == matter_type\n",
    "            marker = 'o' if matter_type == 'Grey' else 's'\n",
    "            axes[0].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                          alpha=0.6, label=matter_type, s=30, marker=marker)\n",
    "        \n",
    "        axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        axes[0].set_title('PCA: Grey vs White Matter')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 按患者着色\n",
    "        for patient in unique_patients:\n",
    "            mask = patients == patient\n",
    "            if np.any(mask):\n",
    "                axes[1].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                              c=[patient_color_map[patient]], alpha=0.6, label=patient, s=30)\n",
    "        \n",
    "        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        axes[1].set_title('PCA: By Patient')\n",
    "        axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'pca_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 保存主成分信息\n",
    "        component_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'PC1': pca.components_[0],\n",
    "            'PC2': pca.components_[1]\n",
    "        })\n",
    "        component_df = component_df.reindex(component_df['PC1'].abs().sort_values(ascending=False).index)\n",
    "        component_df.to_csv(os.path.join(self.output_folder, 'pca_components.csv'), index=False)\n",
    "    \n",
    "    def _create_discriminative_features_plot(self):\n",
    "        \"\"\"创建最具区分性特征的可视化\"\"\"\n",
    "        \n",
    "        # 读取之前计算的特征统计\n",
    "        stats_file = os.path.join(self.output_folder, 'feature_statistics.csv')\n",
    "        if os.path.exists(stats_file):\n",
    "            stats_df = pd.read_csv(stats_file)\n",
    "        else:\n",
    "            stats_df = self.create_feature_summary_statistics()\n",
    "        \n",
    "        # 选择最具区分性的特征（按Cohen's d排序）\n",
    "        top_features = stats_df.nlargest(12, 'cohens_d')['feature_name'].tolist()\n",
    "        \n",
    "        # 创建箱线图\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, feature in enumerate(top_features):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # 准备数据\n",
    "            grey_data = self.combined_features[self.combined_features['matter_type'] == 'Grey'][feature]\n",
    "            white_data = self.combined_features[self.combined_features['matter_type'] == 'White'][feature]\n",
    "            \n",
    "            # 箱线图\n",
    "            box_data = [white_data, grey_data]\n",
    "            bp = ax.boxplot(box_data, labels=['White', 'Grey'], patch_artist=True)\n",
    "            bp['boxes'][0].set_facecolor('lightblue')\n",
    "            bp['boxes'][1].set_facecolor('lightcoral')\n",
    "            \n",
    "            # 添加统计信息\n",
    "            stat_info = stats_df[stats_df['feature_name'] == feature].iloc[0]\n",
    "            ax.set_title(f'{feature}\\nCohen\\'s d: {stat_info[\"cohens_d\"]:.3f}\\np: {stat_info[\"p_value\"]:.3e}',\n",
    "                        fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 隐藏空的子图\n",
    "        for i in range(len(top_features), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Most Discriminative Features (Top 12 by Cohen\\'s d)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'discriminative_features.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def create_patient_specific_analysis(self):\n",
    "        \"\"\"创建患者特异性分析\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating patient-specific analysis...\")\n",
    "        \n",
    "        patients = self.combined_features['patient_id'].unique()\n",
    "        \n",
    "        # 计算每个患者的特征分布特性\n",
    "        patient_analysis = []\n",
    "        \n",
    "        for patient in patients:\n",
    "            patient_data = self.combined_features[self.combined_features['patient_id'] == patient]\n",
    "            \n",
    "            grey_data = patient_data[patient_data['matter_type'] == 'Grey'][self.feature_names]\n",
    "            white_data = patient_data[patient_data['matter_type'] == 'White'][self.feature_names]\n",
    "            \n",
    "            # 计算分离性指标\n",
    "            separability_scores = []\n",
    "            significant_features = 0\n",
    "            \n",
    "            for feature in self.feature_names:\n",
    "                if len(grey_data) > 5 and len(white_data) > 5:\n",
    "                    try:\n",
    "                        # t检验\n",
    "                        t_stat, p_val = stats.ttest_ind(grey_data[feature], white_data[feature])\n",
    "                        \n",
    "                        # Cohen's d\n",
    "                        pooled_std = np.sqrt(((len(grey_data) - 1) * grey_data[feature].std()**2 + \n",
    "                                            (len(white_data) - 1) * white_data[feature].std()**2) / \n",
    "                                           (len(grey_data) + len(white_data) - 2))\n",
    "                        cohens_d = abs((grey_data[feature].mean() - white_data[feature].mean()) / pooled_std) if pooled_std > 0 else 0\n",
    "                        \n",
    "                        separability_scores.append(cohens_d)\n",
    "                        if p_val < 0.05:\n",
    "                            significant_features += 1\n",
    "                    except:\n",
    "                        separability_scores.append(0)\n",
    "                else:\n",
    "                    separability_scores.append(0)\n",
    "            \n",
    "            patient_analysis.append({\n",
    "                'patient_id': patient,\n",
    "                'n_grey_samples': len(grey_data),\n",
    "                'n_white_samples': len(white_data),\n",
    "                'mean_separability': np.mean(separability_scores),\n",
    "                'max_separability': np.max(separability_scores),\n",
    "                'significant_features': significant_features,\n",
    "                'separability_std': np.std(separability_scores)\n",
    "            })\n",
    "        \n",
    "        patient_analysis_df = pd.DataFrame(patient_analysis)\n",
    "        patient_analysis_df.to_csv(os.path.join(self.output_folder, 'patient_analysis.csv'), index=False)\n",
    "        \n",
    "        # 可视化患者分析结果\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # 1. 平均分离性\n",
    "        axes[0, 0].bar(patient_analysis_df['patient_id'], patient_analysis_df['mean_separability'])\n",
    "        axes[0, 0].set_xlabel('Patient ID')\n",
    "        axes[0, 0].set_ylabel('Mean Separability (Cohen\\'s d)')\n",
    "        axes[0, 0].set_title('Average Feature Separability by Patient')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. 显著特征数量\n",
    "        axes[0, 1].bar(patient_analysis_df['patient_id'], patient_analysis_df['significant_features'])\n",
    "        axes[0, 1].set_xlabel('Patient ID')\n",
    "        axes[0, 1].set_ylabel('Number of Significant Features')\n",
    "        axes[0, 1].set_title('Significant Features by Patient (p<0.05)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. 样本数量\n",
    "        x = np.arange(len(patient_analysis_df))\n",
    "        width = 0.35\n",
    "        axes[1, 0].bar(x - width/2, patient_analysis_df['n_grey_samples'], width, label='Grey', alpha=0.8)\n",
    "        axes[1, 0].bar(x + width/2, patient_analysis_df['n_white_samples'], width, label='White', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('Patient ID')\n",
    "        axes[1, 0].set_ylabel('Sample Count')\n",
    "        axes[1, 0].set_title('Sample Distribution by Patient')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(patient_analysis_df['patient_id'], rotation=45)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. 分离性vs样本数量\n",
    "        total_samples = patient_analysis_df['n_grey_samples'] + patient_analysis_df['n_white_samples']\n",
    "        axes[1, 1].scatter(total_samples, patient_analysis_df['mean_separability'], s=60, alpha=0.7)\n",
    "        for i, patient in enumerate(patient_analysis_df['patient_id']):\n",
    "            axes[1, 1].annotate(patient, (total_samples.iloc[i], patient_analysis_df['mean_separability'].iloc[i]),\n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        axes[1, 1].set_xlabel('Total Samples')\n",
    "        axes[1, 1].set_ylabel('Mean Separability')\n",
    "        axes[1, 1].set_title('Separability vs Sample Size')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'patient_specific_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  ✓ Patient-specific analysis saved\")\n",
    "        return patient_analysis_df\n",
    "    \n",
    "    def run_complete_visualization(self):\n",
    "        \"\"\"运行完整的可视化分析\"\"\"\n",
    "        \n",
    "        print(f\"Starting Complete Patient Feature Visualization\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        # 1. 加载和提取特征\n",
    "        if not self.load_and_extract_features():\n",
    "            print(\"❌ Failed to load and extract features\")\n",
    "            return None\n",
    "        \n",
    "        # 2. 创建个别特征分布图\n",
    "        self.create_individual_feature_plots()\n",
    "        \n",
    "        # 3. 创建特征统计汇总\n",
    "        stats_df = self.create_feature_summary_statistics()\n",
    "        \n",
    "        # 4. 创建患者比较图\n",
    "        self.create_patient_comparison_plots()\n",
    "        \n",
    "        # 5. 创建患者特异性分析\n",
    "        patient_analysis = self.create_patient_specific_analysis()\n",
    "        \n",
    "        # 6. 创建汇总报告\n",
    "        self._create_summary_report(stats_df, patient_analysis)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Visualization Analysis Complete\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"📁 Results saved to: {self.output_folder}\")\n",
    "        print(f\"   - feature_distributions_page_*.png: Individual feature distributions\")\n",
    "        print(f\"   - feature_statistics.csv: Feature statistics summary\")\n",
    "        print(f\"   - feature_statistics_plots.png: Statistical analysis plots\")\n",
    "        print(f\"   - patient_similarity_heatmap.png: Patient similarity analysis\")\n",
    "        print(f\"   - pca_analysis.png: PCA analysis\")\n",
    "        print(f\"   - discriminative_features.png: Most discriminative features\")\n",
    "        print(f\"   - patient_specific_analysis.png: Patient-specific analysis\")\n",
    "        print(f\"   - summary_report.txt: Text summary report\")\n",
    "        \n",
    "        return {\n",
    "            'feature_stats': stats_df,\n",
    "            'patient_analysis': patient_analysis,\n",
    "            'n_patients': len(self.combined_features['patient_id'].unique()),\n",
    "            'n_features': len(self.feature_names),\n",
    "            'total_samples': len(self.combined_features)\n",
    "        }\n",
    "    \n",
    "    def _create_summary_report(self, stats_df, patient_analysis):\n",
    "        \"\"\"创建文本汇总报告\"\"\"\n",
    "        \n",
    "        report_path = os.path.join(self.output_folder, 'summary_report.txt')\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"PATIENT FEATURE VISUALIZATION ANALYSIS REPORT\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            \n",
    "            # 基本信息\n",
    "            f.write(\"BASIC INFORMATION:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Number of patients: {len(self.combined_features['patient_id'].unique())}\\n\")\n",
    "            f.write(f\"Number of features: {len(self.feature_names)}\\n\")\n",
    "            f.write(f\"Total samples: {len(self.combined_features)}\\n\")\n",
    "            \n",
    "            grey_samples = len(self.combined_features[self.combined_features['matter_type'] == 'Grey'])\n",
    "            white_samples = len(self.combined_features[self.combined_features['matter_type'] == 'White'])\n",
    "            f.write(f\"Grey matter samples: {grey_samples}\\n\")\n",
    "            f.write(f\"White matter samples: {white_samples}\\n\\n\")\n",
    "            \n",
    "            # 特征分析\n",
    "            f.write(\"FEATURE ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            significant_features = len(stats_df[stats_df['significant'] == True])\n",
    "            f.write(f\"Statistically significant features (p<0.05): {significant_features}/{len(stats_df)}\\n\")\n",
    "            \n",
    "            high_effect_features = len(stats_df[stats_df['cohens_d'].abs() > 0.5])\n",
    "            f.write(f\"Features with large effect size (|Cohen's d| > 0.5): {high_effect_features}/{len(stats_df)}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nTop 10 most discriminative features (by Cohen's d):\\n\")\n",
    "            top_10 = stats_df.nlargest(10, 'cohens_d')\n",
    "            for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "                f.write(f\"  {i:2d}. {row['feature_name']:20s} (d={row['cohens_d']:6.3f}, p={row['p_value']:8.3e})\\n\")\n",
    "            \n",
    "            # 患者分析\n",
    "            f.write(f\"\\nPATIENT ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Average separability across patients: {patient_analysis['mean_separability'].mean():.3f} ± {patient_analysis['mean_separability'].std():.3f}\\n\")\n",
    "            f.write(f\"Patient with highest separability: {patient_analysis.loc[patient_analysis['mean_separability'].idxmax(), 'patient_id']} ({patient_analysis['mean_separability'].max():.3f})\\n\")\n",
    "            f.write(f\"Patient with lowest separability: {patient_analysis.loc[patient_analysis['mean_separability'].idxmin(), 'patient_id']} ({patient_analysis['mean_separability'].min():.3f})\\n\")\n",
    "            \n",
    "            f.write(f\"\\nAverage significant features per patient: {patient_analysis['significant_features'].mean():.1f} ± {patient_analysis['significant_features'].std():.1f}\\n\")\n",
    "            \n",
    "            # 问题诊断\n",
    "            f.write(f\"\\nPOTENTIAL ISSUES IDENTIFIED:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            if significant_features < len(stats_df) * 0.1:\n",
    "                f.write(\"⚠️  Very few features show statistical significance between grey/white matter\\n\")\n",
    "            \n",
    "            if patient_analysis['mean_separability'].std() > patient_analysis['mean_separability'].mean() * 0.5:\n",
    "                f.write(\"⚠️  High variability in separability across patients\\n\")\n",
    "            \n",
    "            low_sep_patients = len(patient_analysis[patient_analysis['mean_separability'] < 0.2])\n",
    "            if low_sep_patients > len(patient_analysis) * 0.3:\n",
    "                f.write(f\"⚠️  {low_sep_patients} patients show very low feature separability (<0.2)\\n\")\n",
    "            \n",
    "            high_cv_features = len(stats_df[stats_df['between_patient_cv'] > 1.0])\n",
    "            if high_cv_features > len(stats_df) * 0.2:\n",
    "                f.write(f\"⚠️  {high_cv_features} features show high between-patient variability (CV>1.0)\\n\")\n",
    "            \n",
    "            f.write(f\"\\nRECOMMENDations:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(\"1. Consider patient-specific normalization or feature selection\\n\")\n",
    "            f.write(\"2. Investigate data preprocessing consistency across patients\\n\")\n",
    "            f.write(\"3. Consider hierarchical models that account for patient variability\\n\")\n",
    "            f.write(\"4. Focus on the most discriminative features for model training\\n\")\n",
    "            f.write(\"5. Consider removing patients with very low separability\\n\")\n",
    "\n",
    "# 使用示例函数\n",
    "def analyze_patient_features(processed_folder, output_folder='feature_analysis_results'):\n",
    "    \"\"\"\n",
    "    便捷的分析函数\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_folder : str\n",
    "        包含患者数据的文件夹路径\n",
    "    output_folder : str\n",
    "        输出文件夹路径\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: 分析结果字典\n",
    "    \"\"\"\n",
    "    \n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    results = visualizer.run_complete_visualization()\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec7bae6632f405b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 患者特征可视化分析 - 使用示例\n",
    "# 方法1: 使用便捷函数\n",
    "def quick_analysis():\n",
    "    \"\"\"快速分析示例\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\feature_visualization\"\n",
    "    \n",
    "    print(\"🔍 Starting Patient Feature Visualization Analysis...\")\n",
    "    \n",
    "    # 运行完整分析\n",
    "    results = analyze_patient_features(processed_folder, output_folder)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n✅ Analysis completed successfully!\")\n",
    "        print(f\"📊 Key findings:\")\n",
    "        print(f\"   - {results['n_patients']} patients analyzed\")\n",
    "        print(f\"   - {results['n_features']} features extracted\")\n",
    "        print(f\"   - {results['total_samples']} total samples\")\n",
    "        \n",
    "        # 显示一些关键统计\n",
    "        stats_df = results['feature_stats']\n",
    "        significant_count = len(stats_df[stats_df['significant'] == True])\n",
    "        print(f\"   - {significant_count}/{len(stats_df)} features are statistically significant\")\n",
    "        \n",
    "        # 显示最有区分性的特征\n",
    "        top_features = stats_df.nlargest(5, 'cohens_d')['feature_name'].tolist()\n",
    "        print(f\"   - Top discriminative features: {', '.join(top_features[:3])}...\")\n",
    "    else:\n",
    "        print(\"❌ Analysis failed\")\n",
    "\n",
    "# 方法2: 详细的步骤控制\n",
    "def detailed_analysis():\n",
    "    \"\"\"详细分析示例，可以控制每个步骤\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\detailed_feature_analysis\"\n",
    "    \n",
    "    # 创建可视化器\n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    \n",
    "    # 步骤1: 加载数据和提取特征\n",
    "    print(\"📂 Loading patient data and extracting features...\")\n",
    "    if not visualizer.load_and_extract_features():\n",
    "        print(\"❌ Failed to load data\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ Loaded {len(visualizer.combined_features['patient_id'].unique())} patients\")\n",
    "    print(f\"📊 Total samples: {len(visualizer.combined_features)}\")\n",
    "    print(f\"🔢 Features: {len(visualizer.feature_names)}\")\n",
    "    \n",
    "    # 步骤2: 创建个别特征分布图\n",
    "    print(\"\\n📈 Creating individual feature distribution plots...\")\n",
    "    visualizer.create_individual_feature_plots(n_features_per_page=12)  # 每页12个特征\n",
    "    \n",
    "    # 步骤3: 统计分析\n",
    "    print(\"\\n📊 Creating feature statistics...\")\n",
    "    stats_df = visualizer.create_feature_summary_statistics()\n",
    "    \n",
    "    # 显示一些关键发现\n",
    "    significant_features = stats_df[stats_df['significant'] == True]\n",
    "    print(f\"   🎯 {len(significant_features)} features are statistically significant\")\n",
    "    \n",
    "    if len(significant_features) > 0:\n",
    "        best_feature = significant_features.loc[significant_features['cohens_d'].abs().idxmax()]\n",
    "        print(f\"   🏆 Best discriminative feature: {best_feature['feature_name']}\")\n",
    "        print(f\"      - Cohen's d: {best_feature['cohens_d']:.3f}\")\n",
    "        print(f\"      - p-value: {best_feature['p_value']:.3e}\")\n",
    "    \n",
    "    # 步骤4: 患者比较分析\n",
    "    print(\"\\n👥 Creating patient comparison analysis...\")\n",
    "    visualizer.create_patient_comparison_plots()\n",
    "    \n",
    "    # 步骤5: 患者特异性分析\n",
    "    print(\"\\n🔍 Creating patient-specific analysis...\")\n",
    "    patient_analysis = visualizer.create_patient_specific_analysis()\n",
    "    \n",
    "    # 显示患者相关发现\n",
    "    best_patient = patient_analysis.loc[patient_analysis['mean_separability'].idxmax()]\n",
    "    worst_patient = patient_analysis.loc[patient_analysis['mean_separability'].idxmin()]\n",
    "    \n",
    "    print(f\"   📈 Best separability: {best_patient['patient_id']} (d={best_patient['mean_separability']:.3f})\")\n",
    "    print(f\"   📉 Worst separability: {worst_patient['patient_id']} (d={worst_patient['mean_separability']:.3f})\")\n",
    "    \n",
    "    # 识别潜在问题\n",
    "    low_sep_patients = patient_analysis[patient_analysis['mean_separability'] < 0.2]\n",
    "    if len(low_sep_patients) > 0:\n",
    "        print(f\"   ⚠️  {len(low_sep_patients)} patients have very low separability:\")\n",
    "        for _, patient in low_sep_patients.iterrows():\n",
    "            print(f\"      - {patient['patient_id']}: {patient['mean_separability']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n📁 All results saved to: {output_folder}\")\n",
    "    \n",
    "    return {\n",
    "        'visualizer': visualizer,\n",
    "        'stats': stats_df,\n",
    "        'patient_analysis': patient_analysis\n",
    "    }\n",
    "\n",
    "# 方法3: 针对性分析特定特征\n",
    "def analyze_specific_features(feature_list=None):\n",
    "    \"\"\"分析特定特征的示例\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\specific_features\"\n",
    "    \n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    \n",
    "    # 加载数据\n",
    "    if not visualizer.load_and_extract_features():\n",
    "        return\n",
    "    \n",
    "    # 如果没有指定特征，使用最具区分性的特征\n",
    "    if feature_list is None:\n",
    "        stats_df = visualizer.create_feature_summary_statistics()\n",
    "        feature_list = stats_df.nlargest(8, 'cohens_d')['feature_name'].tolist()\n",
    "    \n",
    "    print(f\"🎯 Analyzing specific features: {feature_list}\")\n",
    "    \n",
    "    # 创建针对性可视化\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    patients = visualizer.combined_features['patient_id'].unique()\n",
    "    patient_colors = plt.cm.Set3(np.linspace(0, 1, len(patients)))\n",
    "    patient_color_map = {patient: patient_colors[i] for i, patient in enumerate(patients)}\n",
    "    \n",
    "    for i, feature in enumerate(feature_list[:8]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # 为每个患者绘制散点\n",
    "        for j, patient in enumerate(patients):\n",
    "            patient_data = visualizer.combined_features[visualizer.combined_features['patient_id'] == patient]\n",
    "            \n",
    "            grey_data = patient_data[patient_data['matter_type'] == 'Grey']\n",
    "            white_data = patient_data[patient_data['matter_type'] == 'White']\n",
    "            \n",
    "            if len(grey_data) > 0:\n",
    "                y_grey = np.random.normal(1, 0.05, len(grey_data))\n",
    "                ax.scatter(grey_data[feature], y_grey, c=[patient_color_map[patient]], \n",
    "                          alpha=0.6, s=15, marker='o')\n",
    "            \n",
    "            if len(white_data) > 0:\n",
    "                y_white = np.random.normal(0, 0.05, len(white_data))\n",
    "                ax.scatter(white_data[feature], y_white, c=[patient_color_map[patient]], \n",
    "                          alpha=0.6, s=15, marker='s')\n",
    "        \n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Matter Type')\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_yticklabels(['White', 'Grey'])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_title(feature, fontsize=10)\n",
    "    \n",
    "    # 隐藏空的子图\n",
    "    for i in range(len(feature_list), 8):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # 添加图例\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for patient in patients:\n",
    "        handles.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                markerfacecolor=patient_color_map[patient], markersize=8))\n",
    "        labels.append(patient)\n",
    "    \n",
    "    plt.figlegend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.02), \n",
    "                 ncol=min(6, len(patients)), fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Patient-Specific Feature Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig(os.path.join(output_folder, 'specific_features_analysis.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def troubleshoot_classification_issues():\n",
    "    \"\"\"诊断分类问题的专门分析\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\troubleshooting\"\n",
    "    \n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    \n",
    "    if not visualizer.load_and_extract_features():\n",
    "        return\n",
    "    \n",
    "    print(\"🔧 TROUBLESHOOTING CLASSIFICATION ISSUES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. 检查数据平衡性\n",
    "    patients = visualizer.combined_features['patient_id'].unique()\n",
    "    print(f\"\\n1. DATA BALANCE CHECK:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    total_grey = 0\n",
    "    total_white = 0\n",
    "    imbalanced_patients = []\n",
    "    \n",
    "    for patient in patients:\n",
    "        patient_data = visualizer.combined_features[visualizer.combined_features['patient_id'] == patient]\n",
    "        n_grey = len(patient_data[patient_data['matter_type'] == 'Grey'])\n",
    "        n_white = len(patient_data[patient_data['matter_type'] == 'White'])\n",
    "        \n",
    "        total_grey += n_grey\n",
    "        total_white += n_white\n",
    "        \n",
    "        ratio = min(n_grey, n_white) / max(n_grey, n_white) if max(n_grey, n_white) > 0 else 0\n",
    "        \n",
    "        print(f\"   {patient}: Grey={n_grey:4d}, White={n_white:4d}, Ratio={ratio:.2f}\")\n",
    "        \n",
    "        if ratio < 0.3:  # 严重不平衡\n",
    "            imbalanced_patients.append(patient)\n",
    "    \n",
    "    print(f\"\\n   Overall: Grey={total_grey}, White={total_white}\")\n",
    "    overall_ratio = min(total_grey, total_white) / max(total_grey, total_white)\n",
    "    print(f\"   Overall Ratio: {overall_ratio:.3f}\")\n",
    "    \n",
    "    if imbalanced_patients:\n",
    "        print(f\"   ⚠️  Severely imbalanced patients: {imbalanced_patients}\")\n",
    "    \n",
    "    # 2. 检查特征质量\n",
    "    print(f\"\\n2. FEATURE QUALITY CHECK:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    stats_df = visualizer.create_feature_summary_statistics()\n",
    "    \n",
    "    # 无区分性的特征\n",
    "    weak_features = stats_df[stats_df['cohens_d'].abs() < 0.1]\n",
    "    print(f\"   Weak features (|Cohen's d| < 0.1): {len(weak_features)}/{len(stats_df)}\")\n",
    "    \n",
    "    # 高变异性特征\n",
    "    high_var_features = stats_df[stats_df['between_patient_cv'] > 1.5]\n",
    "    print(f\"   High variability features (CV > 1.5): {len(high_var_features)}/{len(stats_df)}\")\n",
    "    \n",
    "    # 显著性特征\n",
    "    significant_features = stats_df[stats_df['significant'] == True]\n",
    "    print(f\"   Statistically significant features: {len(significant_features)}/{len(stats_df)}\")\n",
    "    \n",
    "    if len(significant_features) < len(stats_df) * 0.1:\n",
    "        print(\"   ⚠️  Very few significant features - possible data quality issue!\")\n",
    "    \n",
    "    # 3. 患者异质性检查\n",
    "    print(f\"\\n3. PATIENT HETEROGENEITY CHECK:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    patient_analysis = visualizer.create_patient_specific_analysis()\n",
    "    \n",
    "    low_sep_patients = patient_analysis[patient_analysis['mean_separability'] < 0.2]\n",
    "    print(f\"   Patients with low separability (<0.2): {len(low_sep_patients)}/{len(patient_analysis)}\")\n",
    "    \n",
    "    if len(low_sep_patients) > 0:\n",
    "        print(\"   Low separability patients:\")\n",
    "        for _, patient in low_sep_patients.iterrows():\n",
    "            print(f\"      {patient['patient_id']}: {patient['mean_separability']:.3f}\")\n",
    "    \n",
    "    # 计算患者间相似性\n",
    "    feature_matrix = []\n",
    "    patient_list = []\n",
    "    \n",
    "    for patient in patients:\n",
    "        patient_data = visualizer.combined_features[visualizer.combined_features['patient_id'] == patient]\n",
    "        grey_data = patient_data[patient_data['matter_type'] == 'Grey'][visualizer.feature_names]\n",
    "        \n",
    "        if len(grey_data) > 0:\n",
    "            patient_features = grey_data.mean().values\n",
    "            feature_matrix.append(patient_features)\n",
    "            patient_list.append(patient)\n",
    "    \n",
    "    if len(feature_matrix) > 1:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        \n",
    "        # 标准化并计算距离\n",
    "        scaler = StandardScaler()\n",
    "        feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "        distances = pdist(feature_matrix_scaled, metric='euclidean')\n",
    "        distance_matrix = squareform(distances)\n",
    "        \n",
    "        avg_distance = np.mean(distances)\n",
    "        print(f\"   Average inter-patient distance: {avg_distance:.3f}\")\n",
    "        \n",
    "        if avg_distance > 5.0:\n",
    "            print(\"   ⚠️  High inter-patient variability detected!\")\n",
    "    \n",
    "    # 4. 建议生成\n",
    "    print(f\"\\n4. RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if overall_ratio < 0.7:\n",
    "        recommendations.append(\"Use balanced sampling or class weights in training\")\n",
    "    \n",
    "    if len(imbalanced_patients) > len(patients) * 0.3:\n",
    "        recommendations.append(\"Consider removing severely imbalanced patients\")\n",
    "    \n",
    "    if len(weak_features) > len(stats_df) * 0.5:\n",
    "        recommendations.append(\"Perform feature selection to remove weak features\")\n",
    "    \n",
    "    if len(high_var_features) > len(stats_df) * 0.3:\n",
    "        recommendations.append(\"Consider patient-specific normalization\")\n",
    "    \n",
    "    if len(low_sep_patients) > len(patients) * 0.3:\n",
    "        recommendations.append(\"Investigate data preprocessing consistency\")\n",
    "        recommendations.append(\"Consider hierarchical/mixed-effects models\")\n",
    "    \n",
    "    if len(significant_features) < len(stats_df) * 0.1:\n",
    "        recommendations.append(\"Review feature extraction methodology\")\n",
    "        recommendations.append(\"Consider different time windows or signal processing\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        print(\"   ✅ No major issues detected in the data\")\n",
    "    \n",
    "    # 5. 生成改进建议的具体代码\n",
    "    print(f\"\\n5. IMPLEMENTATION SUGGESTIONS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    print(\"   # 特征选择示例:\")\n",
    "    print(\"   from sklearn.feature_selection import SelectKBest, f_classif\")\n",
    "    print(\"   selector = SelectKBest(f_classif, k=20)\")\n",
    "    print(\"   X_selected = selector.fit_transform(X, y)\")\n",
    "    \n",
    "    print(\"\\n   # 类别平衡示例:\")\n",
    "    print(\"   from sklearn.utils.class_weight import compute_class_weight\")\n",
    "    print(\"   class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\")\n",
    "    \n",
    "    print(\"\\n   # 患者标准化示例:\")\n",
    "    print(\"   from sklearn.preprocessing import StandardScaler\")\n",
    "    print(\"   for patient in patients:\")\n",
    "    print(\"       scaler = StandardScaler()\")\n",
    "    print(\"       patient_data = scaler.fit_transform(patient_data)\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae60bef374614b48",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "detailed_analysis()\n",
    "troubleshoot_classification_issues()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "45999cf6eb4cc82f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "from scipy.integrate import trapezoid\n",
    "class ChannelClassifier:\n",
    "    \"\"\"\n",
    "    Channel-Level Grey/White Matter分类器\n",
    "    \n",
    "    核心特点：\n",
    "    1. 每个channel作为独立样本（无moving window）\n",
    "    2. Patient-level标准化\n",
    "    3. Training-only outlier处理\n",
    "    4. 精简特征集\n",
    "    5. 双重验证（Channel-level + Patient-level）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_folder, output_folder='results'):\n",
    "        self.processed_folder = processed_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.patients_data = {}\n",
    "        self.channels_data = {}  # {channel_id: {features, label, patient_id, ...}}\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifiers = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            'SVM': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "            'MLP': MLPClassifier(max_iter=1000, random_state=42, hidden_layer_sizes=(50,)),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'LDA': LDA(),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"🧠 Channel Classifier initialized\")\n",
    "        print(f\"   Data: {processed_folder}\")\n",
    "        print(f\"   Output: {output_folder}\")\n",
    "    \n",
    "    def load_patients(self):\n",
    "        \"\"\"加载所有患者数据\"\"\"\n",
    "        pkl_files = glob.glob(os.path.join(self.processed_folder, \"P*_processed.pkl\"))\n",
    "        print(f\"\\n📂 Loading {len(pkl_files)} patients...\")\n",
    "        \n",
    "        for pkl_file in pkl_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                pid = data['patient_id']\n",
    "                self.patients_data[pid] = data\n",
    "                duration = data['processing_summary']['total_duration_seconds'] / 60\n",
    "                print(f\"  ✓ {pid}: {duration:.1f} min\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {os.path.basename(pkl_file)}: {e}\")\n",
    "        \n",
    "        print(f\"✅ Loaded {len(self.patients_data)} patients\")\n",
    "        return len(self.patients_data) > 0\n",
    "    \n",
    "    def _extract_electrode_types(self, matter_data):\n",
    "        \"\"\"提取电极类型\"\"\"\n",
    "        # 查找matter列\n",
    "        matter_cols = ['MatterType', 'matter', 'Matter', 'mattertype', 'tissue_type', 'type']\n",
    "        matter_col = None\n",
    "        for col in matter_cols:\n",
    "            if col in matter_data.columns:\n",
    "                matter_col = col\n",
    "                break\n",
    "        \n",
    "        if matter_col is None:\n",
    "            raise ValueError(f\"Matter column not found in {matter_data.columns.tolist()}\")\n",
    "        \n",
    "        # 提取grey/white索引\n",
    "        matter_values = matter_data[matter_col].astype(str).str.lower()\n",
    "        grey_mask = matter_values.isin(['g', 'grey', 'gray'])\n",
    "        white_mask = matter_values.isin(['w', 'white'])\n",
    "        \n",
    "        # 如果标准格式找不到，尝试模式匹配\n",
    "        if grey_mask.sum() == 0:\n",
    "            grey_mask = matter_values.str.contains('grey|gray|cortex', case=False, na=False)\n",
    "        if white_mask.sum() == 0:\n",
    "            white_mask = matter_values.str.contains('white', case=False, na=False)\n",
    "        \n",
    "        grey_indices = matter_data.index[grey_mask].tolist()\n",
    "        white_indices = matter_data.index[white_mask].tolist()\n",
    "        \n",
    "        return grey_indices, white_indices\n",
    "    \n",
    "    def _extract_features(self, signal, fs, avg_power_spectrum=None, freqs=None):\n",
    "        \"\"\"提取精简特征集\"\"\"\n",
    "        if len(signal) < 100 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        features['range'] = np.ptp(signal)\n",
    "        features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "        features['energy'] = np.sum(signal**2)\n",
    "        \n",
    "        # 频域特征：相对功率\n",
    "        try:\n",
    "            signal_freqs, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), scaling='density')\n",
    "            total_power = np.sum(psd)\n",
    "            \n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 100),\n",
    "                'high_gamma': (100, min(200, fs/2))\n",
    "            }\n",
    "            \n",
    "            # 计算每个频段的功率和相对偏移\n",
    "            for band_name, (low, high) in bands.items():\n",
    "                band_mask = (signal_freqs >= low) & (signal_freqs <= high)\n",
    "                if np.any(band_mask):\n",
    "                    band_power = np.sum(psd[band_mask])\n",
    "                    features[f'power_{band_name}'] = band_power\n",
    "                    \n",
    "                    # 计算该频段相对于患者平均功率谱的垂直偏移\n",
    "                    if avg_power_spectrum is not None and freqs is not None:\n",
    "                        avg_band_mask = (freqs >= low) & (freqs <= high)\n",
    "                        if np.any(avg_band_mask):\n",
    "                            # 在log scale下计算该频段的平均偏移\n",
    "                            signal_band_psd_log = np.log10(psd[band_mask] + 1e-12)\n",
    "                            avg_band_psd_log = np.log10(avg_power_spectrum[avg_band_mask] + 1e-12)\n",
    "                            \n",
    "                            # 如果频率点数不同，取平均值\n",
    "                            signal_avg = np.mean(signal_band_psd_log)\n",
    "                            avg_avg = np.mean(avg_band_psd_log)\n",
    "                            \n",
    "                            features[f'rel_power_{band_name}'] = signal_avg - avg_avg\n",
    "                        else:\n",
    "                            features[f'rel_power_{band_name}'] = 0\n",
    "                    else:\n",
    "                        features[f'rel_power_{band_name}'] = 0\n",
    "                else:\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                    features[f'rel_power_{band_name}'] = 0\n",
    "            \n",
    "            features['total_power'] = total_power\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in frequency analysis: {e}\")\n",
    "            bands = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high_gamma']\n",
    "            for band in bands:\n",
    "                features[f'power_{band}'] = 0\n",
    "                features[f'rel_power_{band}'] = 0\n",
    "            features['total_power'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"提取所有channels的特征\"\"\"\n",
    "        print(f\"\\n🔄 Extracting channel features...\")\n",
    "        \n",
    "        channel_id = 0\n",
    "        \n",
    "        for patient_id, patient_data in self.patients_data.items():\n",
    "            print(f\"  Processing {patient_id}...\")\n",
    "            \n",
    "            try:\n",
    "                matter_data = patient_data['matter_data']\n",
    "                recordings = patient_data['recordings']\n",
    "                \n",
    "                # 提取电极类型\n",
    "                grey_indices, white_indices = self._extract_electrode_types(matter_data)\n",
    "                \n",
    "                if len(grey_indices) == 0 or len(white_indices) == 0:\n",
    "                    print(f\"    ✗ Missing grey/white electrodes\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"    {len(grey_indices)} grey, {len(white_indices)} white electrodes\")\n",
    "                \n",
    "                # 合并所有recording数据\n",
    "                all_data = []\n",
    "                fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "                \n",
    "                for recording in recordings:\n",
    "                    data_segment = recording['neural_data_processed']\n",
    "                    if isinstance(data_segment, list):\n",
    "                        all_data.extend(data_segment)\n",
    "                    else:\n",
    "                        all_data.append(data_segment)\n",
    "    \n",
    "                if not all_data:\n",
    "                    print(f\"    ✗ No data found\")\n",
    "                    continue\n",
    "                    \n",
    "                combined_data = np.vstack(all_data)\n",
    "                duration_min = len(combined_data) / fs / 60\n",
    "                print(f\"    Combined: {combined_data.shape} ({duration_min:.1f} min)\")\n",
    "    \n",
    "                # 计算患者级别的平均功率谱 (1-150Hz)\n",
    "                print(f\"    Computing patient-level average power spectrum...\")\n",
    "                all_power_spectra = []\n",
    "                \n",
    "                for ch in range(combined_data.shape[1]):\n",
    "                    sig = combined_data[:, ch]\n",
    "                    if len(sig) > 0 and not np.all(sig == 0):\n",
    "                        try:\n",
    "                            freqs_ch, psd_ch = welch(sig, fs=fs, nperseg=min(1024, len(sig)), scaling='density')\n",
    "                            all_power_spectra.append(psd_ch)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Warning: Error computing PSD for channel {ch}: {e}\")\n",
    "     \n",
    "                if all_power_spectra:\n",
    "                    all_power_spectra = np.vstack(all_power_spectra)\n",
    "                    avg_power_spectrum = np.mean(all_power_spectra, axis=0)\n",
    "                    # 使用第一个成功计算的频率向量作为参考\n",
    "                    freqs, _ = welch(combined_data[:, 0], fs=fs, nperseg=min(1024, len(combined_data[:, 0])), scaling='density')\n",
    "                else:\n",
    "                    avg_power_spectrum = None\n",
    "                    freqs = None\n",
    "                    print(f\"    Warning: Could not compute reference power spectrum\")\n",
    "                \n",
    "                # 处理Grey Matter channels\n",
    "                for electrode_idx in grey_indices:\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        features = self._extract_features(signal, fs, avg_power_spectrum, freqs)\n",
    "                        \n",
    "                        if features is not None:\n",
    "                            self.channels_data[channel_id] = {\n",
    "                                'features': features,\n",
    "                                'label': 1,  # Grey = 1\n",
    "                                'patient_id': patient_id,\n",
    "                                'electrode_idx': electrode_idx,\n",
    "                                'matter_type': 'grey',\n",
    "                                'duration_sec': len(signal) / fs\n",
    "                            }\n",
    "                            channel_id += 1\n",
    "                \n",
    "                # 处理White Matter channels\n",
    "                for electrode_idx in white_indices:\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        features = self._extract_features(signal, fs, avg_power_spectrum, freqs)\n",
    "                        \n",
    "                        if features is not None:\n",
    "                            self.channels_data[channel_id] = {\n",
    "                                'features': features,\n",
    "                                'label': 0,  # White = 0\n",
    "                                'patient_id': patient_id,\n",
    "                                'electrode_idx': electrode_idx,\n",
    "                                'matter_type': 'white',\n",
    "                                'duration_sec': len(signal) / fs\n",
    "                            }\n",
    "                            channel_id += 1\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Error processing {patient_id}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        n_grey = sum(1 for ch in self.channels_data.values() if ch['label'] == 1)\n",
    "        n_white = sum(1 for ch in self.channels_data.values() if ch['label'] == 0)\n",
    "        \n",
    "        print(f\"\\n✅ Extracted {len(self.channels_data)} channels\")\n",
    "        print(f\"   Grey: {n_grey}, White: {n_white}\")\n",
    "        \n",
    "        if self.channels_data:\n",
    "            first_ch = list(self.channels_data.values())[0]\n",
    "            print(f\"   Features: {len(first_ch['features'])}\")\n",
    "            print(f\"   Feature names: {list(first_ch['features'].keys())}\")\n",
    "        \n",
    "        return len(self.channels_data) > 0\n",
    "    \n",
    "    def normalize_by_patient(self, method='robust'):\n",
    "        \"\"\"Patient-level标准化\"\"\"\n",
    "        print(f\"\\n🔄 Patient-level normalization ({method})...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            return False\n",
    "        \n",
    "        # 按patient分组\n",
    "        patient_groups = {}\n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            pid = ch_data['patient_id']\n",
    "            if pid not in patient_groups:\n",
    "                patient_groups[pid] = []\n",
    "            patient_groups[pid].append((ch_id, ch_data))\n",
    "        \n",
    "        # 获取特征名称\n",
    "        feature_names = list(list(self.channels_data.values())[0]['features'].keys())\n",
    "        print(f\"  Normalizing {len(feature_names)} features across {len(patient_groups)} patients\")\n",
    "        \n",
    "        # 为每个patient标准化\n",
    "        for pid, channels in patient_groups.items():\n",
    "            print(f\"    {pid}: {len(channels)} channels\")\n",
    "            \n",
    "            # 收集该patient的所有特征\n",
    "            features_matrix = []\n",
    "            for ch_id, ch_data in channels:\n",
    "                feature_vector = [ch_data['features'][fname] for fname in feature_names]\n",
    "                features_matrix.append(feature_vector)\n",
    "            \n",
    "            features_df = pd.DataFrame(features_matrix, columns=feature_names)\n",
    "            \n",
    "            # 标准化\n",
    "            normalized_df = self._normalize_dataframe(features_df, method)\n",
    "            \n",
    "            # 更新特征\n",
    "            for i, (ch_id, ch_data) in enumerate(channels):\n",
    "                normalized_features = normalized_df.iloc[i].to_dict()\n",
    "                self.channels_data[ch_id]['features'] = normalized_features\n",
    "                self.channels_data[ch_id]['normalized'] = True\n",
    "        \n",
    "        print(f\"✅ Patient-level normalization completed\")\n",
    "        return True\n",
    "    \n",
    "    def _normalize_dataframe(self, df, method):\n",
    "        \"\"\"标准化DataFrame\"\"\"\n",
    "        normalized_df = df.copy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            values = df[col].values\n",
    "            \n",
    "            if method == 'robust':\n",
    "                median_val = np.median(values)\n",
    "                mad_val = np.median(np.abs(values - median_val))\n",
    "                if mad_val > 0:\n",
    "                    normalized_df[col] = (values - median_val) / (1.4826 * mad_val)\n",
    "                else:\n",
    "                    normalized_df[col] = values - median_val\n",
    "            elif method == 'standard':\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                if std_val > 0:\n",
    "                    normalized_df[col] = (values - mean_val) / std_val\n",
    "                else:\n",
    "                    normalized_df[col] = values - mean_val\n",
    "            elif method == 'minmax':\n",
    "                min_val, max_val = np.min(values), np.max(values)\n",
    "                if max_val > min_val:\n",
    "                    normalized_df[col] = (values - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    normalized_df[col] = np.zeros_like(values)\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"准备训练数据\"\"\"\n",
    "        if not self.channels_data:\n",
    "            return None, None, None\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        info = []\n",
    "        \n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            features = ch_data['features']\n",
    "            feature_vector = np.array(list(features.values()))\n",
    "            \n",
    "            X.append(feature_vector)\n",
    "            y.append(ch_data['label'])\n",
    "            info.append({\n",
    "                'channel_id': ch_id,\n",
    "                'patient_id': ch_data['patient_id'],\n",
    "                'matter_type': ch_data['matter_type'],\n",
    "                'electrode_idx': ch_data['electrode_idx']\n",
    "            })\n",
    "        \n",
    "        return np.array(X), np.array(y), info\n",
    "    \n",
    "    def _clip_outliers_training_only(self, X_train, X_test, iqr_factor=1.5):\n",
    "        \"\"\"只基于训练集计算outlier边界\"\"\"\n",
    "        X_train_clipped = X_train.copy()\n",
    "        X_test_clipped = X_test.copy()\n",
    "        \n",
    "        for feature_idx in range(X_train.shape[1]):\n",
    "            train_vals = X_train[:, feature_idx]\n",
    "            \n",
    "            q25, q75 = np.percentile(train_vals, [25, 75])\n",
    "            iqr = q75 - q25\n",
    "            \n",
    "            if iqr > 0:\n",
    "                lower = q25 - iqr_factor * iqr\n",
    "                upper = q75 + iqr_factor * iqr\n",
    "                \n",
    "                X_train_clipped[:, feature_idx] = np.clip(train_vals, lower, upper)\n",
    "                X_test_clipped[:, feature_idx] = np.clip(X_test[:, feature_idx], lower, upper)\n",
    "        \n",
    "        return X_train_clipped, X_test_clipped\n",
    "    \n",
    "    def channel_level_validation(self, test_size=0.2, outlier_clip=True, iqr_factor=1.5):\n",
    "        \"\"\"Channel-level验证：随机分割\"\"\"\n",
    "        print(f\"\\n🔍 Channel-Level Validation (random split)\")\n",
    "        \n",
    "        X, y, info = self._prepare_data()\n",
    "        if X is None:\n",
    "            return None\n",
    "        \n",
    "        # 随机分割\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"  Train: {len(X_train)} ({np.sum(y_train)} grey)\")\n",
    "        print(f\"  Test: {len(X_test)} ({np.sum(y_test)} grey)\")\n",
    "        \n",
    "        # Outlier clipping\n",
    "        if outlier_clip:\n",
    "            X_train, X_test = self._clip_outliers_training_only(X_train, X_test, iqr_factor)\n",
    "            print(f\"  Applied outlier clipping (IQR × {iqr_factor})\")\n",
    "        \n",
    "        # 训练和评估\n",
    "        results = {}\n",
    "        for clf_name, clf in self.classifiers.items():\n",
    "            try:\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else np.zeros_like(y_pred)\n",
    "                \n",
    "                results[clf_name] = {\n",
    "                    'accuracy': accuracy_score(y_test, y_pred),\n",
    "                    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "                    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                    'balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "                    'roc_auc': roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                    'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                    'y_true': y_test,\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_proba': y_proba\n",
    "                }\n",
    "                \n",
    "                print(f\"  {clf_name}: F1={results[clf_name]['f1']:.3f}, Acc={results[clf_name]['accuracy']:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {clf_name}: Error - {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def patient_level_validation(self, outlier_clip=True, iqr_factor=1.5):\n",
    "        \"\"\"Patient-level验证：Leave-one-patient-out\"\"\"\n",
    "        print(f\"\\n🔍 Patient-Level Validation (LOPO)\")\n",
    "        \n",
    "        X, y, info = self._prepare_data()\n",
    "        if X is None:\n",
    "            return None\n",
    "        \n",
    "        patients = list(set([inf['patient_id'] for inf in info]))\n",
    "        print(f\"  {len(patients)} patients for LOPO\")\n",
    "        \n",
    "        if len(patients) < 3:\n",
    "            print(\"  ❌ Need at least 3 patients\")\n",
    "            return None\n",
    "        \n",
    "        # 存储结果\n",
    "        cv_results = {name: [] for name in self.classifiers.keys()}\n",
    "        all_predictions = {name: {'y_true': [], 'y_pred': [], 'y_proba': []} for name in self.classifiers.keys()}\n",
    "        \n",
    "        for fold, test_patient in enumerate(patients):\n",
    "            print(f\"\\n  Fold {fold+1}/{len(patients)}: Test {test_patient}\")\n",
    "            \n",
    "            # 分离数据\n",
    "            train_mask = np.array([inf['patient_id'] != test_patient for inf in info])\n",
    "            test_mask = ~train_mask\n",
    "            \n",
    "            X_train, y_train = X[train_mask], y[train_mask]\n",
    "            X_test, y_test = X[test_mask], y[test_mask]\n",
    "            \n",
    "            print(f\"    Train: {len(X_train)} ({np.sum(y_train)} grey)\")\n",
    "            print(f\"    Test: {len(X_test)} ({np.sum(y_test)} grey)\")\n",
    "            \n",
    "            # Outlier clipping\n",
    "            if outlier_clip:\n",
    "                X_train, X_test = self._clip_outliers_training_only(X_train, X_test, iqr_factor)\n",
    "            \n",
    "            # 训练评估\n",
    "            for clf_name, clf in self.classifiers.items():\n",
    "                try:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else np.zeros_like(y_pred)\n",
    "                    \n",
    "                    fold_result = {\n",
    "                        'fold': fold,\n",
    "                        'test_patient': test_patient,\n",
    "                        'accuracy': accuracy_score(y_test, y_pred),\n",
    "                        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "                        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                        'balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "                        'roc_auc': roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                        'n_test': len(y_test)\n",
    "                    }\n",
    "                    \n",
    "                    cv_results[clf_name].append(fold_result)\n",
    "                    all_predictions[clf_name]['y_true'].extend(y_test)\n",
    "                    all_predictions[clf_name]['y_pred'].extend(y_pred)\n",
    "                    all_predictions[clf_name]['y_proba'].extend(y_proba)\n",
    "                    \n",
    "                    print(f\"      {clf_name}: F1={fold_result['f1']:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      {clf_name}: Error - {e}\")\n",
    "        \n",
    "        return cv_results, all_predictions\n",
    "    \n",
    "    def analyze_results(self, channel_results, patient_results):\n",
    "        \"\"\"分析结果\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"📊 Results Analysis\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        analysis = {'channel': {}, 'patient': {}}\n",
    "        \n",
    "        # Channel-level结果\n",
    "        if channel_results:\n",
    "            print(f\"\\n🔍 Channel-Level Results:\")\n",
    "            best_channel_f1 = 0\n",
    "            best_channel_clf = None\n",
    "            \n",
    "            for clf_name, metrics in channel_results.items():\n",
    "                f1 = metrics['f1']\n",
    "                acc = metrics['accuracy']\n",
    "                auc = metrics['roc_auc']\n",
    "                print(f\"  {clf_name}: F1={f1:.3f}, Acc={acc:.3f}, AUC={auc:.3f}\")\n",
    "                \n",
    "                if f1 > best_channel_f1:\n",
    "                    best_channel_f1 = f1\n",
    "                    best_channel_clf = clf_name\n",
    "            \n",
    "            print(f\"  🏆 Best: {best_channel_clf} (F1={best_channel_f1:.3f})\")\n",
    "            \n",
    "            analysis['channel'] = {\n",
    "                'results': channel_results,\n",
    "                'best_clf': best_channel_clf,\n",
    "                'best_f1': best_channel_f1\n",
    "            }\n",
    "        \n",
    "        # Patient-level结果\n",
    "        if patient_results:\n",
    "            cv_results, all_preds = patient_results\n",
    "            print(f\"\\n🔍 Patient-Level Results:\")\n",
    "            \n",
    "            best_patient_f1 = 0\n",
    "            best_patient_clf = None\n",
    "            patient_summary = {}\n",
    "            \n",
    "            for clf_name in self.classifiers.keys():\n",
    "                if len(cv_results[clf_name]) > 0:\n",
    "                    # CV统计\n",
    "                    f1_values = [fold['f1'] for fold in cv_results[clf_name]]\n",
    "                    f1_mean = np.mean(f1_values)\n",
    "                    f1_std = np.std(f1_values)\n",
    "                    \n",
    "                    # 总体指标\n",
    "                    y_true = np.array(all_preds[clf_name]['y_true'])\n",
    "                    y_pred = np.array(all_preds[clf_name]['y_pred'])\n",
    "                    overall_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "                    overall_acc = accuracy_score(y_true, y_pred)\n",
    "                    \n",
    "                    patient_summary[clf_name] = {\n",
    "                        'cv_f1_mean': f1_mean,\n",
    "                        'cv_f1_std': f1_std,\n",
    "                        'overall_f1': overall_f1,\n",
    "                        'overall_acc': overall_acc\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {clf_name}: CV F1={f1_mean:.3f}±{f1_std:.3f}, Overall F1={overall_f1:.3f}\")\n",
    "                    \n",
    "                    if overall_f1 > best_patient_f1:\n",
    "                        best_patient_f1 = overall_f1\n",
    "                        best_patient_clf = clf_name\n",
    "            \n",
    "            print(f\"  🏆 Best: {best_patient_clf} (F1={best_patient_f1:.3f})\")\n",
    "            \n",
    "            analysis['patient'] = {\n",
    "                'results': patient_summary,\n",
    "                'best_clf': best_patient_clf,\n",
    "                'best_f1': best_patient_f1\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_visualizations(self, analysis_results):\n",
    "        \"\"\"创建可视化\"\"\"\n",
    "        print(f\"\\n📊 Creating visualizations...\")\n",
    "        \n",
    "        # 1. 数据集分布\n",
    "        self._plot_dataset_distribution()\n",
    "        \n",
    "        # 2. 验证对比\n",
    "        self._plot_validation_comparison(analysis_results)\n",
    "        \n",
    "        # 3. 特征重要性\n",
    "        self._plot_feature_importance()\n",
    "        \n",
    "        # 4. 患者特征分布 (新增)\n",
    "        self._plot_patient_feature_distributions()\n",
    "        \n",
    "        print(f\"  💾 Saved to {self.output_folder}\")\n",
    "    \n",
    "    def _plot_patient_feature_distributions(self, selected_features=None, max_patients_per_plot=12):\n",
    "        \"\"\"\n",
    "        为每个feature创建单独的可视化图，每个图包含所有患者的该feature分布\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        selected_features : list, optional\n",
    "            要可视化的特征列表，如果为None则使用所有特征\n",
    "        max_patients_per_plot : int\n",
    "            每个图最多显示的患者数量\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"  📊 Creating patient feature distributions...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            print(f\"    ❌ No channel data available\")\n",
    "            return\n",
    "        \n",
    "        # 获取特征名称\n",
    "        first_channel = list(self.channels_data.values())[0]\n",
    "        all_feature_names = list(first_channel['features'].keys())\n",
    "        \n",
    "        if selected_features is None:\n",
    "            # 选择一些代表性特征进行可视化\n",
    "            selected_features = [\n",
    "                'mean', 'std', 'rms', 'line_length', 'energy',\n",
    "                'rel_power_delta', 'rel_power_theta', 'rel_power_alpha', \n",
    "                'rel_power_beta', 'rel_power_gamma', 'rel_power_high_gamma',\n",
    "                'skewness', 'kurtosis'\n",
    "            ]\n",
    "            # 只保留实际存在的特征\n",
    "            selected_features = [f for f in selected_features if f in all_feature_names]\n",
    "        \n",
    "        # 组织数据：按patient和matter_type分组\n",
    "        patient_feature_data = {}\n",
    "        \n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            patient_id = ch_data['patient_id']\n",
    "            matter_type = ch_data['matter_type']\n",
    "            features = ch_data['features']\n",
    "            \n",
    "            if patient_id not in patient_feature_data:\n",
    "                patient_feature_data[patient_id] = {'grey': [], 'white': []}\n",
    "            \n",
    "            patient_feature_data[patient_id][matter_type].append(features)\n",
    "        \n",
    "        # 获取患者列表并排序\n",
    "        patients = sorted(list(patient_feature_data.keys()))\n",
    "        \n",
    "        # 如果患者太多，分批处理\n",
    "        patient_batches = [patients[i:i+max_patients_per_plot] \n",
    "                          for i in range(0, len(patients), max_patients_per_plot)]\n",
    "        \n",
    "        # 为每个选定的特征创建可视化\n",
    "        for feature_name in selected_features:\n",
    "            print(f\"    Creating visualization for feature: {feature_name}\")\n",
    "            \n",
    "            for batch_idx, patient_batch in enumerate(patient_batches):\n",
    "                # 计算subplot布局\n",
    "                n_patients = len(patient_batch)\n",
    "                n_cols = min(4, n_patients)  # 最多4列\n",
    "                n_rows = (n_patients + n_cols - 1) // n_cols  # 向上取整\n",
    "                \n",
    "                # 创建figure\n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "                \n",
    "                # 确保axes是2D数组\n",
    "                if n_rows == 1 and n_cols == 1:\n",
    "                    axes = np.array([[axes]])\n",
    "                elif n_rows == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                elif n_cols == 1:\n",
    "                    axes = axes.reshape(-1, 1)\n",
    "                \n",
    "                # 为每个患者创建subplot\n",
    "                for i, patient_id in enumerate(patient_batch):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    ax = axes[row, col]\n",
    "                    \n",
    "                    # 提取该患者该特征的数据\n",
    "                    grey_values = []\n",
    "                    white_values = []\n",
    "                    \n",
    "                    for ch_features in patient_feature_data[patient_id]['grey']:\n",
    "                        if feature_name in ch_features:\n",
    "                            grey_values.append(ch_features[feature_name])\n",
    "                    \n",
    "                    for ch_features in patient_feature_data[patient_id]['white']:\n",
    "                        if feature_name in ch_features:\n",
    "                            white_values.append(ch_features[feature_name])\n",
    "                    \n",
    "                    # 绘制分布\n",
    "                    if grey_values:\n",
    "                        ax.hist(grey_values, bins=15, alpha=0.6, label='Grey', color='red', density=True)\n",
    "                    if white_values:\n",
    "                        ax.hist(white_values, bins=15, alpha=0.6, label='White', color='blue', density=True)\n",
    "                    \n",
    "                    # 设置标题和标签\n",
    "                    ax.set_title(f'{patient_id}\\n(G:{len(grey_values)}, W:{len(white_values)})', fontsize=10)\n",
    "                    ax.set_xlabel(f'{feature_name}', fontsize=8)\n",
    "                    ax.set_ylabel('Density', fontsize=8)\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # 设置合适的x轴范围\n",
    "                    all_values = grey_values + white_values\n",
    "                    if all_values:\n",
    "                        # 使用1-99百分位数设置范围，避免极端值影响\n",
    "                        p01, p99 = np.percentile(all_values, [1, 99])\n",
    "                        if p99 > p01:  # 确保范围有效\n",
    "                            ax.set_xlim(p01, p99)\n",
    "                    \n",
    "                    # 美化刻度\n",
    "                    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "                \n",
    "                # 隐藏多余的subplots\n",
    "                for i in range(len(patient_batch), n_rows * n_cols):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    axes[row, col].set_visible(False)\n",
    "                \n",
    "                # 设置整体标题\n",
    "                batch_suffix = f\"_batch{batch_idx+1}\" if len(patient_batches) > 1 else \"\"\n",
    "                fig.suptitle(f'Feature Distribution: {feature_name}{batch_suffix}', fontsize=16, y=0.98)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.93)  # 为suptitle留出空间\n",
    "                \n",
    "                # 保存图片\n",
    "                safe_feature_name = feature_name.replace('/', '_').replace('\\\\', '_')\n",
    "                filename = f'feature_{safe_feature_name}{batch_suffix}.png'\n",
    "                plt.savefig(os.path.join(self.output_folder, filename), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        print(f\"    ✅ Created visualizations for {len(selected_features)} features\")\n",
    "    \n",
    "    def create_detailed_feature_visualization(self, features_to_plot=None):\n",
    "        \"\"\"\n",
    "        创建详细的特征可视化（独立方法，可单独调用）\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_to_plot : list, optional\n",
    "            指定要绘制的特征列表\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 Creating detailed feature visualizations...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            print(f\"❌ No channel data available\")\n",
    "            return\n",
    "        \n",
    "        # 如果没有指定特征，使用所有特征\n",
    "        if features_to_plot is None:\n",
    "            first_channel = list(self.channels_data.values())[0]\n",
    "            features_to_plot = list(first_channel['features'].keys())\n",
    "        \n",
    "        # 创建特征可视化子文件夹\n",
    "        feature_viz_folder = os.path.join(self.output_folder, 'feature_distributions')\n",
    "        os.makedirs(feature_viz_folder, exist_ok=True)\n",
    "        \n",
    "        # 临时改变输出文件夹\n",
    "        original_folder = self.output_folder\n",
    "        self.output_folder = feature_viz_folder\n",
    "        \n",
    "        try:\n",
    "            self._plot_patient_feature_distributions(\n",
    "                selected_features=features_to_plot,\n",
    "                max_patients_per_plot=12\n",
    "            )\n",
    "        finally:\n",
    "            # 恢复原始输出文件夹\n",
    "            self.output_folder = original_folder\n",
    "        \n",
    "        print(f\"✅ Feature visualizations saved to: {feature_viz_folder}\")\n",
    "    \n",
    "    def create_feature_summary_statistics(self):\n",
    "        \"\"\"创建特征统计摘要表\"\"\"\n",
    "        print(f\"\\n📊 Creating feature summary statistics...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            print(f\"❌ No channel data available\")\n",
    "            return\n",
    "        \n",
    "        # 按patient和matter type分组统计\n",
    "        summary_stats = []\n",
    "        \n",
    "        # 获取特征名称\n",
    "        first_channel = list(self.channels_data.values())[0]\n",
    "        feature_names = list(first_channel['features'].keys())\n",
    "        \n",
    "        # 按patient分组\n",
    "        patient_groups = {}\n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            patient_id = ch_data['patient_id']\n",
    "            matter_type = ch_data['matter_type']\n",
    "            \n",
    "            if patient_id not in patient_groups:\n",
    "                patient_groups[patient_id] = {'grey': [], 'white': []}\n",
    "            \n",
    "            patient_groups[patient_id][matter_type].append(ch_data['features'])\n",
    "        \n",
    "        # 为每个patient和matter type计算统计\n",
    "        for patient_id, matter_data in patient_groups.items():\n",
    "            for matter_type in ['grey', 'white']:\n",
    "                if matter_data[matter_type]:  # 如果有数据\n",
    "                    for feature_name in feature_names:\n",
    "                        # 收集该特征的所有值\n",
    "                        feature_values = [ch_features[feature_name] \n",
    "                                        for ch_features in matter_data[matter_type]\n",
    "                                        if feature_name in ch_features]\n",
    "                        \n",
    "                        if feature_values:\n",
    "                            summary_stats.append({\n",
    "                                'Patient': patient_id,\n",
    "                                'Matter_Type': matter_type,\n",
    "                                'Feature': feature_name,\n",
    "                                'N_Channels': len(feature_values),\n",
    "                                'Mean': np.mean(feature_values),\n",
    "                                'Std': np.std(feature_values),\n",
    "                                'Median': np.median(feature_values),\n",
    "                                'Min': np.min(feature_values),\n",
    "                                'Max': np.max(feature_values),\n",
    "                                'Q25': np.percentile(feature_values, 25),\n",
    "                                'Q75': np.percentile(feature_values, 75)\n",
    "                            })\n",
    "        \n",
    "        # 保存统计摘要\n",
    "        if summary_stats:\n",
    "            summary_df = pd.DataFrame(summary_stats)\n",
    "            summary_df.to_csv(\n",
    "                os.path.join(self.output_folder, 'feature_summary_statistics.csv'),\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            print(f\"  ✅ Feature summary statistics saved to feature_summary_statistics.csv\")\n",
    "        else:\n",
    "            print(f\"  ❌ No statistics to save\")\n",
    "            \n",
    "    def _plot_dataset_distribution(self):\n",
    "        \"\"\"数据集分布图\"\"\"\n",
    "        # 统计患者分布\n",
    "        patient_stats = {}\n",
    "        for ch_data in self.channels_data.values():\n",
    "            pid = ch_data['patient_id']\n",
    "            matter = ch_data['matter_type']\n",
    "            \n",
    "            if pid not in patient_stats:\n",
    "                patient_stats[pid] = {'grey': 0, 'white': 0}\n",
    "            patient_stats[pid][matter] += 1\n",
    "        \n",
    "        # 绘图\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # 患者分布条形图\n",
    "        patients = list(patient_stats.keys())\n",
    "        grey_counts = [patient_stats[p]['grey'] for p in patients]\n",
    "        white_counts = [patient_stats[p]['white'] for p in patients]\n",
    "        \n",
    "        x = np.arange(len(patients))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, grey_counts, width, label='Grey', color='red', alpha=0.7)\n",
    "        ax1.bar(x + width/2, white_counts, width, label='White', color='blue', alpha=0.7)\n",
    "        ax1.set_xlabel('Patients')\n",
    "        ax1.set_ylabel('Channel Count')\n",
    "        ax1.set_title('Channel Distribution by Patient')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(patients, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 总体饼图\n",
    "        total_grey = sum(grey_counts)\n",
    "        total_white = sum(white_counts)\n",
    "        \n",
    "        ax2.pie([total_grey, total_white], \n",
    "               labels=['Grey Matter', 'White Matter'],\n",
    "               colors=['red', 'blue'], \n",
    "               autopct='%1.1f%%')\n",
    "        ax2.set_title(f'Overall Distribution\\n({total_grey + total_white} channels)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'dataset_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_validation_comparison(self, analysis_results):\n",
    "        \"\"\"验证方法对比图\"\"\"\n",
    "        channel_res = analysis_results.get('channel', {}).get('results', {})\n",
    "        patient_res = analysis_results.get('patient', {}).get('results', {})\n",
    "        \n",
    "        if not channel_res or not patient_res:\n",
    "            return\n",
    "        \n",
    "        # 准备数据\n",
    "        classifiers = list(self.classifiers.keys())\n",
    "        channel_f1s = [channel_res.get(clf, {}).get('f1', 0) for clf in classifiers]\n",
    "        patient_f1s = [patient_res.get(clf, {}).get('overall_f1', 0) for clf in classifiers]\n",
    "        \n",
    "        # 绘图\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        x = np.arange(len(classifiers))\n",
    "        width = 0.35\n",
    "        \n",
    "        # F1对比\n",
    "        ax1.bar(x - width/2, channel_f1s, width, label='Channel-Level', alpha=0.7, color='green')\n",
    "        ax1.bar(x + width/2, patient_f1s, width, label='Patient-Level', alpha=0.7, color='orange')\n",
    "        ax1.set_xlabel('Classifiers')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('F1 Score Comparison')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(classifiers, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 差值图\n",
    "        f1_diff = np.array(channel_f1s) - np.array(patient_f1s)\n",
    "        colors = ['green' if d > 0 else 'red' for d in f1_diff]\n",
    "        \n",
    "        ax2.bar(x, f1_diff, alpha=0.7, color=colors)\n",
    "        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax2.set_xlabel('Classifiers')\n",
    "        ax2.set_ylabel('F1 Difference (Channel - Patient)')\n",
    "        ax2.set_title('F1 Score Difference')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(classifiers, rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'validation_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_feature_importance(self):\n",
    "        \"\"\"特征重要性图\"\"\"\n",
    "        X, y, _ = self._prepare_data()\n",
    "        if X is None:\n",
    "            return\n",
    "        \n",
    "        # 使用随机森林计算重要性\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        importances = rf.feature_importances_\n",
    "        feature_names = list(list(self.channels_data.values())[0]['features'].keys())\n",
    "        \n",
    "        # 排序\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # 绘图\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(range(len(importances)), importances[indices], alpha=0.7)\n",
    "        plt.yticks(range(len(importances)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Feature Importance (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def save_results(self, analysis_results, norm_method):\n",
    "        \"\"\"保存结果\"\"\"\n",
    "        print(f\"\\n💾 Saving results...\")\n",
    "        \n",
    "        # Channel-level结果\n",
    "        channel_res = analysis_results.get('channel', {}).get('results', {})\n",
    "        if channel_res:\n",
    "            channel_data = []\n",
    "            for clf_name, metrics in channel_res.items():\n",
    "                channel_data.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'Validation': 'Channel_Level',\n",
    "                    'F1': metrics['f1'],\n",
    "                    'Accuracy': metrics['accuracy'],\n",
    "                    'Balanced_Acc': metrics['balanced_acc'],\n",
    "                    'ROC_AUC': metrics['roc_auc'],\n",
    "                    'Precision': metrics['precision'],\n",
    "                    'Recall': metrics['recall']\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(channel_data).to_csv(\n",
    "                os.path.join(self.output_folder, f'channel_results_{norm_method}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # Patient-level结果\n",
    "        patient_res = analysis_results.get('patient', {}).get('results', {})\n",
    "        if patient_res:\n",
    "            patient_data = []\n",
    "            for clf_name, metrics in patient_res.items():\n",
    "                patient_data.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'Validation': 'Patient_Level',\n",
    "                    'CV_F1_Mean': metrics['cv_f1_mean'],\n",
    "                    'CV_F1_Std': metrics['cv_f1_std'],\n",
    "                    'Overall_F1': metrics['overall_f1'],\n",
    "                    'Overall_Acc': metrics['overall_acc']\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(patient_data).to_csv(\n",
    "                os.path.join(self.output_folder, f'patient_results_{norm_method}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # 总结对比\n",
    "        summary_data = []\n",
    "        for clf_name in self.classifiers.keys():\n",
    "            row = {'Classifier': clf_name}\n",
    "            \n",
    "            if channel_res and clf_name in channel_res:\n",
    "                row['Channel_F1'] = channel_res[clf_name]['f1']\n",
    "                row['Channel_Acc'] = channel_res[clf_name]['accuracy']\n",
    "            else:\n",
    "                row['Channel_F1'] = 0\n",
    "                row['Channel_Acc'] = 0\n",
    "            \n",
    "            if patient_res and clf_name in patient_res:\n",
    "                row['Patient_F1'] = patient_res[clf_name]['overall_f1']\n",
    "                row['Patient_CV_F1'] = patient_res[clf_name]['cv_f1_mean']\n",
    "                row['Patient_CV_Std'] = patient_res[clf_name]['cv_f1_std']\n",
    "            else:\n",
    "                row['Patient_F1'] = 0\n",
    "                row['Patient_CV_F1'] = 0\n",
    "                row['Patient_CV_Std'] = 0\n",
    "            \n",
    "            summary_data.append(row)\n",
    "        \n",
    "        pd.DataFrame(summary_data).to_csv(\n",
    "            os.path.join(self.output_folder, f'summary_{norm_method}.csv'), \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # 数据集信息\n",
    "        dataset_info = {\n",
    "            'total_channels': len(self.channels_data),\n",
    "            'grey_channels': sum(1 for ch in self.channels_data.values() if ch['label'] == 1),\n",
    "            'white_channels': sum(1 for ch in self.channels_data.values() if ch['label'] == 0),\n",
    "            'n_patients': len(set(ch['patient_id'] for ch in self.channels_data.values())),\n",
    "            'normalization': norm_method\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.output_folder, 'dataset_info.txt'), 'w') as f:\n",
    "            for key, value in dataset_info.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        print(f\"  ✅ Results saved to {self.output_folder}\")\n",
    "    \n",
    "    def run_analysis(self, normalization='robust', outlier_clip=True, iqr_factor=1.5, \n",
    "                   create_feature_viz=True, features_to_visualize=None):\n",
    "        \"\"\"运行完整分析\"\"\"\n",
    "        print(f\"🧠 Channel-Level Grey/White Matter Classification\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 1. 加载数据\n",
    "        if not self.load_patients():\n",
    "            print(\"❌ Failed to load patients\")\n",
    "            return None\n",
    "        \n",
    "        # 2. 提取特征\n",
    "        if not self.extract_all_features():\n",
    "            print(\"❌ Failed to extract features\")\n",
    "            return None\n",
    "        \n",
    "        # 3. 标准化\n",
    "        if not self.normalize_by_patient(normalization):\n",
    "            print(\"❌ Failed to normalize\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Channel-level验证\n",
    "        channel_results = self.channel_level_validation(\n",
    "            outlier_clip=outlier_clip, \n",
    "            iqr_factor=iqr_factor\n",
    "        )\n",
    "        \n",
    "        # 5. Patient-level验证\n",
    "        patient_results = self.patient_level_validation(\n",
    "            outlier_clip=outlier_clip, \n",
    "            iqr_factor=iqr_factor\n",
    "        )\n",
    "        \n",
    "        # 6. 分析结果\n",
    "        analysis_results = self.analyze_results(channel_results, patient_results)\n",
    "        \n",
    "        # 7. 可视化\n",
    "        self.create_visualizations(analysis_results)\n",
    "        \n",
    "        # 8. 详细特征可视化 (可选)\n",
    "        if create_feature_viz:\n",
    "            self.create_detailed_feature_visualization(features_to_visualize)\n",
    "            self.create_feature_summary_statistics()\n",
    "        \n",
    "        # 9. 保存结果\n",
    "        self.save_results(analysis_results, normalization)\n",
    "        \n",
    "        # 10. 总结\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✅ Analysis Complete\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        n_channels = len(self.channels_data)\n",
    "        n_grey = sum(1 for ch in self.channels_data.values() if ch['label'] == 1)\n",
    "        n_white = sum(1 for ch in self.channels_data.values() if ch['label'] == 0)\n",
    "        n_patients = len(set(ch['patient_id'] for ch in self.channels_data.values()))\n",
    "        \n",
    "        print(f\"📊 Dataset:\")\n",
    "        print(f\"   Patients: {n_patients}\")\n",
    "        print(f\"   Channels: {n_channels} ({n_grey} grey, {n_white} white)\")\n",
    "        print(f\"   Normalization: {normalization} (patient-level)\")\n",
    "        print(f\"   Outlier clipping: {'Yes' if outlier_clip else 'No'} (IQR × {iqr_factor})\")\n",
    "        print(f\"   Feature visualization: {'Yes' if create_feature_viz else 'No'}\")\n",
    "        \n",
    "        # 最佳结果\n",
    "        if analysis_results.get('channel', {}).get('best_clf'):\n",
    "            channel_best = analysis_results['channel']\n",
    "            print(f\"\\n🏆 Best Channel-Level: {channel_best['best_clf']}\")\n",
    "            print(f\"   F1 Score: {channel_best['best_f1']:.3f}\")\n",
    "        \n",
    "        if analysis_results.get('patient', {}).get('best_clf'):\n",
    "            patient_best = analysis_results['patient']\n",
    "            print(f\"\\n🏆 Best Patient-Level: {patient_best['best_clf']}\")\n",
    "            print(f\"   F1 Score: {patient_best['best_f1']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n📁 Results: {self.output_folder}\")\n",
    "        if create_feature_viz:\n",
    "            print(f\"📁 Feature visualizations: {self.output_folder}/feature_distributions/\")\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "def create_feature_visualizations_only():\n",
    "    \"\"\"只创建特征可视化的示例\"\"\"\n",
    "    classifier = ChannelClassifier(\n",
    "        processed_folder= r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_test\",\n",
    "        output_folder= r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\gwclassification_feature_viz_only_test\"\n",
    "    )\n",
    "    \n",
    "    # 加载数据和提取特征\n",
    "    classifier.load_patients()\n",
    "    classifier.extract_all_features()\n",
    "    classifier.normalize_by_patient('robust')\n",
    "    \n",
    "    # 只创建特征可视化\n",
    "    classifier.create_detailed_feature_visualization()\n",
    "    \n",
    "    # 创建统计摘要\n",
    "    classifier.create_feature_summary_statistics()\n",
    "    \n",
    "    print(\"✅ Feature visualizations created!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f06bf3b6db8cea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "create_feature_visualizations_only()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0f20f56a5fca73",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "classifier = ChannelClassifier(\n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\",\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\"\n",
    ")\n",
    "\n",
    "# 运行分析\n",
    "results = classifier.run_analysis(\n",
    "    normalization='standard',    # 'robust', 'standard', 'minmax'\n",
    "    outlier_clip=True,         # 是否进行outlier clipping\n",
    "    iqr_factor=1.5            # IQR倍数\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(\"\\n🎯 Quick Summary:\")\n",
    "    \n",
    "    # Channel-level结果\n",
    "    if 'channel' in results and results['channel'].get('best_clf'):\n",
    "        channel_best = results['channel']\n",
    "        print(f\"Channel-Level Best: {channel_best['best_clf']} (F1={channel_best['best_f1']:.3f})\")\n",
    "    \n",
    "    # Patient-level结果\n",
    "    if 'patient' in results and results['patient'].get('best_clf'):\n",
    "        patient_best = results['patient']\n",
    "        print(f\"Patient-Level Best: {patient_best['best_clf']} (F1={patient_best['best_f1']:.3f})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41b13b726bd5f41d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_psd_analysis_from_classifier(classifier, figsize=(20, 12)):\n",
    "    \"\"\"\n",
    "    从ChannelClassifier输出绘制PSD分析图\n",
    "    \n",
    "    Args:\n",
    "        classifier: ChannelClassifier对象，包含channels_data和patients_data\n",
    "        figsize: 图片大小\n",
    "    \"\"\"\n",
    "    \n",
    "    # 定义频段和颜色\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4, '#90EE90'),      # 浅绿色\n",
    "        'theta': (4, 8, '#FFA500'),        # 橙色  \n",
    "        'alpha': (8, 13, '#FFB6C1'),       # 浅粉色\n",
    "        'beta': (13, 30, '#87CEEB'),       # 天蓝色\n",
    "        'low_gamma': (30, 60, '#DDA0DD'),  # 梅花色\n",
    "        'high_gamma': (60, 100, '#F0E68C'), # 卡其色\n",
    "        'ripple': (100, 200, '#D3D3D3')    # 浅灰色\n",
    "    }\n",
    "    \n",
    "    # 从classifier获取数据\n",
    "    channels_data = classifier.channels_data\n",
    "    patients_data = classifier.patients_data\n",
    "    \n",
    "    # 获取唯一患者ID\n",
    "    patient_ids = list(set([ch['patient_id'] for ch in channels_data.values()]))\n",
    "    patient_ids = sorted(patient_ids)\n",
    "    \n",
    "    # 计算每个患者需要的子图数量\n",
    "    n_patients = len(patient_ids)\n",
    "    n_cols = min(3, n_patients)  # 最多3列\n",
    "    n_rows = (n_patients + n_cols - 1) // n_cols  # 向上取整\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_patients == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    plt.suptitle('Power Spectral Density Analysis by Patient', fontsize=16, y=0.95)\n",
    "    \n",
    "    for idx, patient_id in enumerate(patient_ids):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            ax = axes[col] if n_cols > 1 else axes[0]\n",
    "        else:\n",
    "            ax = axes[row, col]\n",
    "        \n",
    "        # 获取该患者的所有channels\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        if not patient_channels:\n",
    "            ax.set_title(f'{patient_id}\\nNo data available')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        # 分离灰质和白质channels\n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        # 获取该患者的原始数据\n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            # 合并所有recording数据\n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                # 计算每个channel的PSD\n",
    "                grey_psds = []\n",
    "                white_psds = []\n",
    "                freqs = None\n",
    "                \n",
    "                # 绘制个体channel的PSD（浅色）\n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            ax.plot(f, psd, color='blue', alpha=0.1, linewidth=0.5)\n",
    "                            grey_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "                \n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            ax.plot(f, psd, color='red', alpha=0.1, linewidth=0.5)\n",
    "                            white_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "                \n",
    "                # 计算并绘制平均PSD（粗线）\n",
    "                if grey_psds and freqs is not None:\n",
    "                    grey_psds = np.vstack(grey_psds)\n",
    "                    grey_mean_psd = np.mean(grey_psds, axis=0)\n",
    "                    ax.plot(freqs, grey_mean_psd, color='blue', linewidth=3, \n",
    "                           label=f'Grey Matter (n={len(grey_psds)})', alpha=0.8)\n",
    "                \n",
    "                if white_psds and freqs is not None:\n",
    "                    white_psds = np.vstack(white_psds)\n",
    "                    white_mean_psd = np.mean(white_psds, axis=0)\n",
    "                    ax.plot(freqs, white_mean_psd, color='red', linewidth=3, \n",
    "                           label=f'White Matter (n={len(white_psds)})', alpha=0.8)\n",
    "        \n",
    "        # 添加频段背景\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        for band_name, (low_freq, high_freq, color) in freq_bands.items():\n",
    "            if high_freq <= 200:  # 只显示200Hz以下的频段\n",
    "                ax.axvspan(low_freq, high_freq, alpha=0.2, color=color)\n",
    "                # 在顶部添加频段标签\n",
    "                mid_freq = (low_freq + high_freq) / 2\n",
    "                ax.text(mid_freq, y_max * 0.9, band_name, \n",
    "                       ha='center', va='center', fontsize=8, \n",
    "                       bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "        \n",
    "        # 设置坐标轴\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('Power Spectral Density (log scale)')\n",
    "        ax.set_xlim(0, 150)  # 限制到150Hz\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        # 统计信息\n",
    "        n_grey = len(grey_channels)\n",
    "        n_white = len(white_channels)\n",
    "        ax.set_title(f'{patient_id}\\n(G:{n_grey}, W:{n_white})')\n",
    "    \n",
    "    # 隐藏多余的子图\n",
    "    for idx in range(n_patients, n_rows * n_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        if n_rows == 1:\n",
    "            axes[col].axis('off')\n",
    "        else:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_combined_patient_psd_from_classifier(classifier, figsize=(15, 8)):\n",
    "    \"\"\"\n",
    "    从ChannelClassifier绘制所有患者合并的PSD对比图\n",
    "    \n",
    "    Args:\n",
    "        classifier: ChannelClassifier对象\n",
    "        figsize: 图片大小\n",
    "    \"\"\"\n",
    "    \n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4, '#90EE90'),\n",
    "        'theta': (4, 8, '#FFA500'),\n",
    "        'alpha': (8, 13, '#FFB6C1'),\n",
    "        'beta': (13, 30, '#87CEEB'),\n",
    "        'low_gamma': (30, 60, '#DDA0DD'),\n",
    "        'high_gamma': (60, 100, '#F0E68C'),\n",
    "        'ripple': (100, 200, '#D3D3D3')\n",
    "    }\n",
    "    \n",
    "    # 从classifier获取数据\n",
    "    channels_data = classifier.channels_data\n",
    "    patients_data = classifier.patients_data\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    all_grey_psds = []\n",
    "    all_white_psds = []\n",
    "    freqs = None\n",
    "    \n",
    "    # 获取唯一患者ID\n",
    "    patient_ids = list(set([ch['patient_id'] for ch in channels_data.values()]))\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        # 获取该患者的channels\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        # 获取原始数据\n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                # 计算灰质channels的PSD\n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_grey_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "                \n",
    "                # 计算白质channels的PSD\n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_white_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "    \n",
    "    # 计算并绘制总体平均PSD\n",
    "    if all_grey_psds and freqs is not None:\n",
    "        all_grey_psds = np.vstack(all_grey_psds)\n",
    "        grey_mean_psd = np.mean(all_grey_psds, axis=0)\n",
    "        grey_std_psd = np.std(all_grey_psds, axis=0)\n",
    "        \n",
    "        ax.plot(freqs, grey_mean_psd, color='blue', linewidth=3, \n",
    "               label=f'Grey Matter (n={len(all_grey_psds)})')\n",
    "        ax.fill_between(freqs, grey_mean_psd - grey_std_psd, \n",
    "                       grey_mean_psd + grey_std_psd, \n",
    "                       color='blue', alpha=0.2)\n",
    "    \n",
    "    if all_white_psds and freqs is not None:\n",
    "        all_white_psds = np.vstack(all_white_psds)\n",
    "        white_mean_psd = np.mean(all_white_psds, axis=0)\n",
    "        white_std_psd = np.std(all_white_psds, axis=0)\n",
    "        \n",
    "        ax.plot(freqs, white_mean_psd, color='red', linewidth=3, \n",
    "               label=f'White Matter (n={len(all_white_psds)})')\n",
    "        ax.fill_between(freqs, white_mean_psd - white_std_psd, \n",
    "                       white_mean_psd + white_std_psd, \n",
    "                       color='red', alpha=0.2)\n",
    "    \n",
    "    # 添加频段背景\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for band_name, (low_freq, high_freq, color) in freq_bands.items():\n",
    "        if high_freq <= 150:\n",
    "            ax.axvspan(low_freq, high_freq, alpha=0.2, color=color)\n",
    "            mid_freq = (low_freq + high_freq) / 2\n",
    "            ax.text(mid_freq, y_max * 0.9, band_name, \n",
    "                   ha='center', va='center', fontsize=10, \n",
    "                   bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Power Spectral Density (log scale)')\n",
    "    ax.set_xlim(0, 150)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_title('Average Power Spectral Density - All Patients Combined')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_psd_comparison_grid_from_classifier(classifier, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    从ChannelClassifier绘制完整的PSD对比网格\n",
    "    \n",
    "    Args:\n",
    "        classifier: ChannelClassifier对象\n",
    "        figsize: 图片大小\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从classifier获取数据\n",
    "    channels_data = classifier.channels_data\n",
    "    patients_data = classifier.patients_data\n",
    "    \n",
    "    # 获取唯一患者ID\n",
    "    patient_ids = list(set([ch['patient_id'] for ch in channels_data.values()]))\n",
    "    patient_ids = sorted(patient_ids)\n",
    "    n_patients = len(patient_ids)\n",
    "    \n",
    "    # 计算网格布局：患者图 + 1个总体图\n",
    "    n_total_plots = n_patients + 1\n",
    "    n_cols = min(3, n_total_plots)\n",
    "    n_rows = (n_total_plots + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 创建患者子图\n",
    "    for idx, patient_id in enumerate(patient_ids):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        \n",
    "        # 绘制单个患者的PSD（简化版本）\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                grey_psds = []\n",
    "                white_psds = []\n",
    "                \n",
    "                # 计算平均PSD\n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            grey_psds.append(psd)\n",
    "                \n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            white_psds.append(psd)\n",
    "                \n",
    "                # 绘制平均线\n",
    "                if grey_psds:\n",
    "                    grey_mean_psd = np.mean(np.vstack(grey_psds), axis=0)\n",
    "                    ax.plot(f, grey_mean_psd, color='blue', linewidth=2, \n",
    "                           label=f'Grey (n={len(grey_psds)})')\n",
    "                \n",
    "                if white_psds:\n",
    "                    white_mean_psd = np.mean(np.vstack(white_psds), axis=0)\n",
    "                    ax.plot(f, white_mean_psd, color='red', linewidth=2, \n",
    "                           label=f'White (n={len(white_psds)})')\n",
    "        \n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlim(0, 150)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_title(f'{patient_id}')\n",
    "        \n",
    "        if idx >= (n_rows - 1) * n_cols:  # 最后一行\n",
    "            ax.set_xlabel('Frequency (Hz)')\n",
    "        if idx % n_cols == 0:  # 第一列\n",
    "            ax.set_ylabel('PSD (log)')\n",
    "    \n",
    "    # 创建总体对比图\n",
    "    ax_combined = plt.subplot(n_rows, n_cols, n_patients + 1)\n",
    "    \n",
    "    # 总体PSD计算\n",
    "    all_grey_psds = []\n",
    "    all_white_psds = []\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_grey_psds.append(psd)\n",
    "                \n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_white_psds.append(psd)\n",
    "    \n",
    "    # 绘制总体平均\n",
    "    if all_grey_psds:\n",
    "        grey_mean_psd = np.mean(np.vstack(all_grey_psds), axis=0)\n",
    "        ax_combined.plot(f, grey_mean_psd, color='blue', linewidth=3, \n",
    "                        label=f'Grey Matter (n={len(all_grey_psds)})')\n",
    "    \n",
    "    if all_white_psds:\n",
    "        white_mean_psd = np.mean(np.vstack(all_white_psds), axis=0)\n",
    "        ax_combined.plot(f, white_mean_psd, color='red', linewidth=3, \n",
    "                        label=f'White Matter (n={len(all_white_psds)})')\n",
    "    \n",
    "    ax_combined.set_yscale('log')\n",
    "    ax_combined.set_xlim(0, 150)\n",
    "    ax_combined.grid(True, alpha=0.3)\n",
    "    ax_combined.legend()\n",
    "    ax_combined.set_title('All Patients Combined')\n",
    "    ax_combined.set_xlabel('Frequency (Hz)')\n",
    "    if (n_patients + 1 - 1) % n_cols == 0:\n",
    "        ax_combined.set_ylabel('PSD (log)')\n",
    "    \n",
    "    plt.suptitle('Power Spectral Density Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc4caf4339d278fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 方法1: 详细的个体患者PSD图\n",
    "fig1 = plot_psd_analysis_from_classifier(classifier, figsize=(20, 12))\n",
    "fig1.savefig(r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\\psd_analysis_individual_patients.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# 方法2: 所有患者合并的PSD对比图\n",
    "fig2 = plot_combined_patient_psd_from_classifier(classifier, figsize=(15, 8))\n",
    "fig2.savefig(r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\\psd_analysis_combined_patients.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# 方法3: 完整的网格布局（推荐）\n",
    "fig3 = plot_psd_comparison_grid_from_classifier(classifier, figsize=(20, 15))\n",
    "fig3.savefig(r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\\psd_analysis_comparison_grid.png\", dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c161e4b120179054",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def plot_classification_results_comprehensive(channel_results, patient_results, \n",
    "                                            save_path=None, figsize=(16, 12)):\n",
    "    \"\"\"\n",
    "    创建分类结果的综合可视化图表\n",
    "    \n",
    "    Args:\n",
    "        channel_results: Channel-level结果字典\n",
    "        patient_results: Patient-level结果（包含cv_results和all_predictions）\n",
    "        save_path: 保存路径（可选）\n",
    "        figsize: 图片尺寸\n",
    "    \n",
    "    Returns:\n",
    "        fig: matplotlib figure对象\n",
    "    \"\"\"\n",
    "    \n",
    "    # 解析patient_results\n",
    "    if isinstance(patient_results, tuple) and len(patient_results) == 2:\n",
    "        cv_results, all_predictions = patient_results\n",
    "    else:\n",
    "        cv_results = patient_results\n",
    "        all_predictions = None\n",
    "    \n",
    "    # 设置样式\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # 创建2x2子图布局\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 定义颜色\n",
    "    colors = {\n",
    "        'channel': '#3498db',  # 蓝色\n",
    "        'accuracy': '#f39c12',  # 橙色\n",
    "        'roc': '#2ecc71',      # 绿色\n",
    "        'patient': '#9b59b6'   # 紫色\n",
    "    }\n",
    "    \n",
    "    # 获取分类器名称\n",
    "    classifiers = list(channel_results.keys()) if channel_results else []\n",
    "    \n",
    "    # 1. F1 Score对比（左上）\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    plot_f1_comparison(ax1, channel_results, cv_results, all_predictions, classifiers, colors)\n",
    "    \n",
    "    # 2. Accuracy对比（右上）\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    plot_accuracy_comparison(ax2, channel_results, classifiers, colors)\n",
    "    \n",
    "    # 3. ROC AUC对比（左下）\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    plot_roc_comparison(ax3, channel_results, classifiers, colors)\n",
    "    \n",
    "    # 4. Patient-wise结果（右下）\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    plot_patient_wise_results(ax4, cv_results, colors)\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    \n",
    "    # 保存图片\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"📊 Results visualization saved to: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_f1_comparison(ax, channel_results, cv_results, all_predictions, classifiers, colors):\n",
    "    \"\"\"绘制F1 Score对比图\"\"\"\n",
    "    \n",
    "    # 提取Channel-level F1 scores\n",
    "    channel_f1s = []\n",
    "    channel_stds = []\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        if clf in channel_results:\n",
    "            channel_f1s.append(channel_results[clf]['f1'])\n",
    "            channel_stds.append(0)  # Channel-level没有std\n",
    "        else:\n",
    "            channel_f1s.append(0)\n",
    "            channel_stds.append(0)\n",
    "    \n",
    "    # 提取Patient-level F1 scores\n",
    "    patient_f1s = []\n",
    "    patient_stds = []\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        if clf in cv_results and len(cv_results[clf]) > 0:\n",
    "            f1_values = [fold['f1'] for fold in cv_results[clf]]\n",
    "            patient_f1s.append(np.mean(f1_values))\n",
    "            patient_stds.append(np.std(f1_values))\n",
    "        else:\n",
    "            patient_f1s.append(0)\n",
    "            patient_stds.append(0)\n",
    "    \n",
    "    # 绘制条形图\n",
    "    x = np.arange(len(classifiers))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, channel_f1s, width, \n",
    "                   label='Channel-Level', color=colors['channel'], alpha=0.8,\n",
    "                   yerr=channel_stds, capsize=5)\n",
    "    \n",
    "    bars2 = ax.bar(x + width/2, patient_f1s, width,\n",
    "                   label='Patient-Level', color=colors['patient'], alpha=0.8,\n",
    "                   yerr=patient_stds, capsize=5)\n",
    "    \n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('F1 Score by Classifier (with std)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(classifiers, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, max(max(channel_f1s), max(patient_f1s)) * 1.1)\n",
    "\n",
    "def plot_accuracy_comparison(ax, channel_results, classifiers, colors):\n",
    "    \"\"\"绘制Accuracy对比图\"\"\"\n",
    "    \n",
    "    accuracies = []\n",
    "    for clf in classifiers:\n",
    "        if clf in channel_results:\n",
    "            accuracies.append(channel_results[clf]['accuracy'])\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "    \n",
    "    bars = ax.bar(classifiers, accuracies, color=colors['accuracy'], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy by Classifier')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        if acc > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "def plot_roc_comparison(ax, channel_results, classifiers, colors):\n",
    "    \"\"\"绘制ROC AUC对比图\"\"\"\n",
    "    \n",
    "    roc_aucs = []\n",
    "    for clf in classifiers:\n",
    "        if clf in channel_results:\n",
    "            roc_aucs.append(channel_results[clf]['roc_auc'])\n",
    "        else:\n",
    "            roc_aucs.append(0.5)  # 默认值\n",
    "    \n",
    "    bars = ax.bar(classifiers, roc_aucs, color=colors['roc'], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel('ROC AUC')\n",
    "    ax.set_title('ROC AUC by Classifier')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 添加基准线\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_patient_wise_results(ax, cv_results, colors):\n",
    "    \"\"\"绘制Patient-wise结果\"\"\"\n",
    "    \n",
    "    # 找到最佳分类器（这里以Naive Bayes为例，你可以根据实际情况调整）\n",
    "    best_clf = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for clf_name, folds in cv_results.items():\n",
    "        if len(folds) > 0:\n",
    "            avg_f1 = np.mean([fold['f1'] for fold in folds])\n",
    "            if avg_f1 > best_score:\n",
    "                best_score = avg_f1\n",
    "                best_clf = clf_name\n",
    "    \n",
    "    if best_clf and best_clf in cv_results:\n",
    "        # 提取每个患者（fold）的结果\n",
    "        folds = cv_results[best_clf]\n",
    "        patients = [fold['test_patient'] for fold in folds]\n",
    "        f1_scores = [fold['f1'] for fold in folds]\n",
    "        \n",
    "        bars = ax.bar(patients, f1_scores, color=colors['patient'], alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Patients')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.set_title(f'Patient-wise F1 Score ({best_clf})')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # 添加平均线\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        ax.axhline(y=mean_f1, color='red', linestyle='--', alpha=0.7, \n",
    "                  label=f'Mean: {mean_f1:.3f}')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No patient-wise results available', \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Patient-wise F1 Score')\n",
    "\n",
    "def plot_classification_results_simple(analysis_results, save_path=None, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    简化版本的结果可视化（适用于analyzer.analyze_results的输出）\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: analyzer.analyze_results()的输出\n",
    "        save_path: 保存路径\n",
    "        figsize: 图片尺寸\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_results = analysis_results.get('channel', {}).get('results', {})\n",
    "    patient_summary = analysis_results.get('patient', {}).get('results', {})\n",
    "    \n",
    "    # 创建2x2子图\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    classifiers = list(channel_results.keys())\n",
    "    colors = ['#3498db', '#f39c12', '#2ecc71', '#9b59b6', '#e74c3c', '#1abc9c', '#f1c40f']\n",
    "    \n",
    "    # 1. F1 Score对比\n",
    "    if channel_results and patient_summary:\n",
    "        channel_f1s = [channel_results[clf]['f1'] for clf in classifiers]\n",
    "        patient_f1s = [patient_summary[clf]['overall_f1'] if clf in patient_summary else 0 \n",
    "                      for clf in classifiers]\n",
    "        patient_stds = [patient_summary[clf]['cv_f1_std'] if clf in patient_summary else 0 \n",
    "                       for clf in classifiers]\n",
    "        \n",
    "        x = np.arange(len(classifiers))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, channel_f1s, width, label='Channel-Level', \n",
    "               color='#3498db', alpha=0.8)\n",
    "        ax1.bar(x + width/2, patient_f1s, width, label='Patient-Level', \n",
    "               color='#9b59b6', alpha=0.8, yerr=patient_stds, capsize=5)\n",
    "        \n",
    "        ax1.set_xlabel('Classifiers')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('F1 Score by Classifier (with std)')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(classifiers, rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy\n",
    "    if channel_results:\n",
    "        accuracies = [channel_results[clf]['accuracy'] for clf in classifiers]\n",
    "        bars = ax2.bar(classifiers, accuracies, color='#f39c12', alpha=0.8)\n",
    "        ax2.set_xlabel('Classifiers')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy by Classifier')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # 3. ROC AUC\n",
    "    if channel_results:\n",
    "        roc_aucs = [channel_results[clf]['roc_auc'] for clf in classifiers]\n",
    "        ax3.bar(classifiers, roc_aucs, color='#2ecc71', alpha=0.8)\n",
    "        ax3.set_xlabel('Classifiers')\n",
    "        ax3.set_ylabel('ROC AUC')\n",
    "        ax3.set_title('ROC AUC by Classifier')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        ax3.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 4. 模型对比雷达图\n",
    "    if channel_results:\n",
    "        metrics = ['F1', 'Accuracy', 'Precision', 'Recall', 'ROC AUC']\n",
    "        \n",
    "        # 选择最佳模型显示\n",
    "        best_clf = max(channel_results.keys(), \n",
    "                      key=lambda x: channel_results[x]['f1'])\n",
    "        \n",
    "        values = [\n",
    "            channel_results[best_clf]['f1'],\n",
    "            channel_results[best_clf]['accuracy'],\n",
    "            channel_results[best_clf]['precision'],\n",
    "            channel_results[best_clf]['recall'],\n",
    "            channel_results[best_clf]['roc_auc']\n",
    "        ]\n",
    "        \n",
    "        # 简单的条形图代替雷达图\n",
    "        ax4.barh(metrics, values, color='#e74c3c', alpha=0.8)\n",
    "        ax4.set_xlabel('Score')\n",
    "        ax4.set_title(f'Best Model Performance ({best_clf})')\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"📊 Results visualization saved to: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_summary_table(analysis_results, save_path=None):\n",
    "    \"\"\"\n",
    "    创建结果汇总表\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: 分析结果字典\n",
    "        save_path: 保存路径\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 结果汇总表\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_results = analysis_results.get('channel', {}).get('results', {})\n",
    "    patient_results = analysis_results.get('patient', {}).get('results', {})\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for clf_name in channel_results.keys():\n",
    "        row = {\n",
    "            'Classifier': clf_name,\n",
    "            'Channel_F1': channel_results[clf_name]['f1'],\n",
    "            'Channel_Accuracy': channel_results[clf_name]['accuracy'],\n",
    "            'Channel_Precision': channel_results[clf_name]['precision'],\n",
    "            'Channel_Recall': channel_results[clf_name]['recall'],\n",
    "            'Channel_ROC_AUC': channel_results[clf_name]['roc_auc'],\n",
    "            'Channel_Balanced_Acc': channel_results[clf_name]['balanced_acc']\n",
    "        }\n",
    "        \n",
    "        if clf_name in patient_results:\n",
    "            row.update({\n",
    "                'Patient_F1_Mean': patient_results[clf_name]['cv_f1_mean'],\n",
    "                'Patient_F1_Std': patient_results[clf_name]['cv_f1_std'],\n",
    "                'Patient_Overall_F1': patient_results[clf_name]['overall_f1'],\n",
    "                'Patient_Overall_Acc': patient_results[clf_name]['overall_acc']\n",
    "            })\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if save_path:\n",
    "        summary_df.to_csv(save_path, index=False)\n",
    "        print(f\"📋 Summary table saved to: {save_path}\")\n",
    "    \n",
    "    return summary_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc025c670df4ca6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "channel_results = classifier.channel_level_validation()\n",
    "patient_results = classifier.patient_level_validation()\n",
    "analysis_results = classifier.analyze_results(channel_results, patient_results)\n",
    "\n",
    "fig2 = plot_classification_results_simple(\n",
    "    analysis_results,\n",
    "    save_path='classification_results_simple.png'\n",
    ")\n",
    "fig = plot_classification_results_comprehensive(\n",
    "    channel_results, patient_results,\n",
    "    save_path='detailed_results.png'\n",
    ")\n",
    "summary_table = create_summary_table(\n",
    "    analysis_results,\n",
    "    save_path='results_summary.csv'\n",
    ")\n",
    "\n",
    "print(summary_table)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db2b49b35507f239",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2871b03135502f0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
