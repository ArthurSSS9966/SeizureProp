{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                           balanced_accuracy_score, roc_auc_score, roc_curve, auc,\n",
    "                           confusion_matrix, classification_report)\n",
    "from scipy import stats\n",
    "from scipy.fftpack import fft\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MultiPatientClassifier:\n",
    "    \"\"\"\n",
    "    Channel-Levelæ ‡å‡†åŒ–çš„Grey/White Matteråˆ†ç±»ç³»ç»Ÿ\n",
    "    \n",
    "    æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "    1. æ¯ä¸ªchannelä½œä¸ºä¸€ä¸ªåˆ†ç±»å•ä½\n",
    "    2. å¯¹æ¯ä¸ªchannelçš„æ—¶é—´çª—å£ç‰¹å¾è¿›è¡Œchannel-specificæ ‡å‡†åŒ–\n",
    "    3. ä½¿ç”¨å¹³å‡æ¦‚ç‡æ¥åˆ†ç±»æ¯ä¸ªchannel\n",
    "    4. éªŒè¯æ—¶è®¡ç®—æ¯ä¸ªpatientçš„channel-level accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_folder, output_folder='results'):\n",
    "        self.processed_folder = processed_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.patients_data = {}\n",
    "        self.channel_features_data = {}  # æ”¹ä¸ºchannel-centricå­˜å‚¨\n",
    "        self.normalized_channel_features_data = {}\n",
    "        \n",
    "        self.classifiers = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "            'SVM': SVC(probability=True, random_state=42),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'MLP': MLPClassifier(max_iter=1000, random_state=42, hidden_layer_sizes=(50,)),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'LDA': LDA(),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        print(\"Initializing Channel-Level Normalized Classifier\")\n",
    "        print(f\"Processed folder: {processed_folder}\")\n",
    "        print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    def load_all_patients(self):\n",
    "        \"\"\"Load all patients from the processed folder.\"\"\"\n",
    "        pkl_files = glob.glob(os.path.join(self.processed_folder, \"P*_processed.pkl\"))\n",
    "        print(f\"\\nFound {len(pkl_files)} patient files\")\n",
    "        for pkl_file in pkl_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                pid = data['patient_id']\n",
    "                self.patients_data[pid] = data\n",
    "                duration = data['processing_summary']['total_duration_seconds'] / 60\n",
    "                print(f\"âœ“ {pid}: {len(data['recordings'])} recordings, {duration:.1f} minutes\")\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Failed to load {os.path.basename(pkl_file)}: {e}\")\n",
    "        print(f\"Successfully loaded {len(self.patients_data)} patients\")\n",
    "        return bool(self.patients_data)\n",
    "    \n",
    "    def extract_electrode_classification(self, matter_data):\n",
    "        \"\"\"ä»matteræ•°æ®æå–ç”µæåˆ†ç±»\"\"\"\n",
    "        \n",
    "        matter_columns = ['MatterType', 'matter', 'Matter', 'mattertype', 'tissue_type', 'type']\n",
    "        matter_col = None\n",
    "        \n",
    "        for col in matter_columns:\n",
    "            if col in matter_data.columns:\n",
    "                matter_col = col\n",
    "                break\n",
    "        \n",
    "        if matter_col is None:\n",
    "            raise ValueError(f\"Matter type column not found. Available: {matter_data.columns.tolist()}\")\n",
    "        \n",
    "        matter_values = matter_data[matter_col].astype(str).str.lower()\n",
    "        \n",
    "        grey_mask = matter_values.isin(['G', 'g', 'Grey', 'grey', 'Gray', 'gray'])\n",
    "        white_mask = matter_values.isin(['W', 'w', 'White', 'white'])\n",
    "        \n",
    "        if np.sum(grey_mask) == 0 or np.sum(white_mask) == 0:\n",
    "            print(f\"    G/W format not found, trying other methods...\")\n",
    "            matter_values_lower = matter_values.str.lower()\n",
    "            \n",
    "            if np.sum(grey_mask) == 0:\n",
    "                grey_patterns = ['grey', 'gray', 'cortex', 'cortical']\n",
    "                grey_mask = matter_values_lower.str.contains('|'.join(grey_patterns), na=False, case=False)\n",
    "            \n",
    "            if np.sum(white_mask) == 0:\n",
    "                white_patterns = ['white']\n",
    "                white_mask = matter_values_lower.str.contains('|'.join(white_patterns), na=False, case=False)\n",
    "        \n",
    "        grey_indices = matter_data.index[grey_mask].tolist()\n",
    "        white_indices = matter_data.index[white_mask].tolist()\n",
    "        \n",
    "        classification_info = {\n",
    "            'matter_column': matter_col,\n",
    "            'total_electrodes': len(matter_data),\n",
    "            'grey_electrodes': len(grey_indices),\n",
    "            'white_electrodes': len(white_indices),\n",
    "            'grey_indices': grey_indices,\n",
    "            'white_indices': white_indices,\n",
    "            'matter_distribution': matter_data[matter_col].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        return grey_indices, white_indices, classification_info\n",
    "    \n",
    "    def extract_signal_features(self, signal, fs):\n",
    "        \"\"\"æå–å•ä¸ªä¿¡å·çš„ç‰¹å¾\"\"\"\n",
    "        \n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # æ—¶åŸŸç‰¹å¾\n",
    "        features['std'] = np.std(signal)\n",
    "        features['mad'] = np.median(np.abs(signal - np.median(signal)))\n",
    "        features['range'] = np.ptp(signal)\n",
    "        features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        \n",
    "        # ä¿¡å·å¤æ‚åº¦\n",
    "        features['area'] = np.sum(np.abs(signal))\n",
    "        \n",
    "        # é¢‘åŸŸç‰¹å¾\n",
    "        try:\n",
    "            n_fft = len(signal)\n",
    "            windowed_signal = signal * np.hamming(n_fft)\n",
    "            fft_vals = fft(windowed_signal)\n",
    "            fft_mag = np.abs(fft_vals[:n_fft//2])\n",
    "            freqs = np.fft.fftfreq(n_fft, 1/fs)[:n_fft//2]\n",
    "            \n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 100),\n",
    "                'high_gamma': (100, min(200, fs/2))\n",
    "            }\n",
    "            \n",
    "            total_power = np.sum(fft_mag**2)\n",
    "            for band_name, (low, high) in bands.items():\n",
    "                band_mask = (freqs >= low) & (freqs <= high)\n",
    "                if np.any(band_mask):\n",
    "                    band_power = np.sum(fft_mag[band_mask]**2)\n",
    "                    features[f'power_{band_name}'] = band_power\n",
    "                    features[f'rel_power_{band_name}'] = band_power / total_power if total_power > 0 else 0\n",
    "                else:\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                    features[f'rel_power_{band_name}'] = 0\n",
    "            \n",
    "            features['total_power'] = total_power\n",
    "                \n",
    "        except Exception as e:\n",
    "            bands = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high_gamma']\n",
    "            for band in bands:\n",
    "                features[f'power_{band}'] = 0\n",
    "                features[f'rel_power_{band}'] = 0\n",
    "            features['total_power'] = 0\n",
    "            features['peak_frequency'] = 0\n",
    "            features['spectral_centroid'] = 0\n",
    "            features['spectral_entropy'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_channel_centric_features(self, use_windowing=True, \n",
    "                                       window_size_ms=500, step_size_ms=250, \n",
    "                                       max_windows_per_channel=200):\n",
    "        \"\"\"\n",
    "        æå–ä»¥channelä¸ºä¸­å¿ƒçš„ç‰¹å¾\n",
    "        \n",
    "        æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "        - æ¯ä¸ªchannelä½œä¸ºä¸€ä¸ªåˆ†ç±»å•ä½\n",
    "        - æ¯ä¸ªchannelæœ‰å¤šä¸ªæ—¶é—´çª—å£çš„ç‰¹å¾\n",
    "        - å­˜å‚¨ç»“æ„ï¼š{channel_id: {features: [], label: 0/1, patient_id: str}}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ”„ Extracting Channel-Centric Features...\")\n",
    "        \n",
    "        channel_id = 0  # å…¨å±€channel ID\n",
    "        \n",
    "        for patient_id, patient_data in self.patients_data.items():\n",
    "            print(f\"\\nå¤„ç† {patient_id}...\")\n",
    "            \n",
    "            matter_data = patient_data['matter_data']\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            # æå–ç”µæåˆ†ç±»\n",
    "            try:\n",
    "                grey_indices, white_indices, classification_info = self.extract_electrode_classification(matter_data)\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Electrode classification failed: {e}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Electrode Classification: {classification_info['grey_electrodes']} grey, {classification_info['white_electrodes']} white\")\n",
    "            \n",
    "            if len(grey_indices) == 0 or len(white_indices) == 0:\n",
    "                print(f\"  âœ— Missing Grey or White Matter Electrode\")\n",
    "                continue\n",
    "            \n",
    "            # åˆå¹¶æ‰€æœ‰recordingsçš„æ•°æ®\n",
    "            all_grey_data = []\n",
    "            all_white_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                neural_data = recording['neural_data_processed']\n",
    "                grey_data = neural_data[:, grey_indices]\n",
    "                white_data = neural_data[:, white_indices]\n",
    "                all_grey_data.append(grey_data)\n",
    "                all_white_data.append(white_data)\n",
    "            \n",
    "            combined_grey = np.vstack(all_grey_data) if all_grey_data else np.array([])\n",
    "            combined_white = np.vstack(all_white_data) if all_white_data else np.array([])\n",
    "            \n",
    "            print(f\"  Merging...: Grey {combined_grey.shape}, White {combined_white.shape}\")\n",
    "            \n",
    "            # å¤„ç†Grey Matter Channels\n",
    "            for ch_idx, electrode_idx in enumerate(grey_indices):\n",
    "                channel_data = combined_grey[:, ch_idx]\n",
    "                \n",
    "                # ä¸ºè¿™ä¸ªchannelæå–å¤šä¸ªæ—¶é—´çª—å£çš„ç‰¹å¾\n",
    "                channel_features = self._extract_features_for_single_channel(\n",
    "                    channel_data, fs, use_windowing, window_size_ms, step_size_ms, max_windows_per_channel\n",
    "                )\n",
    "                \n",
    "                if len(channel_features) > 0:\n",
    "                    self.channel_features_data[channel_id] = {\n",
    "                        'features': channel_features,  # List of feature dicts from different windows\n",
    "                        'label': 1,  # Grey matter\n",
    "                        'patient_id': patient_id,\n",
    "                        'electrode_idx': electrode_idx,\n",
    "                        'channel_idx': ch_idx,\n",
    "                        'matter_type': 'grey'\n",
    "                    }\n",
    "                    channel_id += 1\n",
    "            \n",
    "            # å¤„ç†White Matter Channels\n",
    "            for ch_idx, electrode_idx in enumerate(white_indices):\n",
    "                channel_data = combined_white[:, ch_idx]\n",
    "                \n",
    "                channel_features = self._extract_features_for_single_channel(\n",
    "                    channel_data, fs, use_windowing, window_size_ms, step_size_ms, max_windows_per_channel\n",
    "                )\n",
    "                \n",
    "                if len(channel_features) > 0:\n",
    "                    self.channel_features_data[channel_id] = {\n",
    "                        'features': channel_features,\n",
    "                        'label': 0,  # White matter\n",
    "                        'patient_id': patient_id,\n",
    "                        'electrode_idx': electrode_idx,\n",
    "                        'channel_idx': ch_idx,\n",
    "                        'matter_type': 'white'\n",
    "                    }\n",
    "                    channel_id += 1\n",
    "        \n",
    "        print(f\"\\nâœ… Extracted features for {len(self.channel_features_data)} channels\")\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        grey_channels = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 1)\n",
    "        white_channels = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 0)\n",
    "        \n",
    "        print(f\"   Grey matter channels: {grey_channels}\")\n",
    "        print(f\"   White matter channels: {white_channels}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _extract_features_for_single_channel(self, channel_data, fs, use_windowing, \n",
    "                                           window_size_ms, step_size_ms, max_windows_per_channel):\n",
    "        \"\"\"ä¸ºå•ä¸ªchannelæå–ç‰¹å¾\"\"\"\n",
    "        \n",
    "        channel_features = []\n",
    "        \n",
    "        if use_windowing:\n",
    "            # æ—¶é—´çª—å£æ–¹æ³•\n",
    "            window_samples = int(window_size_ms * fs / 1000)\n",
    "            step_samples = int(step_size_ms * fs / 1000)\n",
    "            n_time = len(channel_data)\n",
    "            \n",
    "            window_count = 0\n",
    "            for start in range(0, n_time - window_samples + 1, step_samples):\n",
    "                if window_count >= max_windows_per_channel:\n",
    "                    break\n",
    "                    \n",
    "                end = start + window_samples\n",
    "                window_data = channel_data[start:end]\n",
    "                \n",
    "                features = self.extract_signal_features(window_data, fs)\n",
    "                if features is not None:\n",
    "                    channel_features.append(features)\n",
    "                    window_count += 1\n",
    "        else:\n",
    "            # æ•´ä¸ªä¿¡å·ä½œä¸ºä¸€ä¸ªç‰¹å¾\n",
    "            features = self.extract_signal_features(channel_data, fs)\n",
    "            if features is not None:\n",
    "                channel_features.append(features)\n",
    "        \n",
    "        return channel_features\n",
    "    \n",
    "    def apply_channel_specific_normalization(self, normalization_method='robust'):\n",
    "        \"\"\"\n",
    "        å¯¹æ¯ä¸ªchannelçš„ç‰¹å¾è¿›è¡Œchannel-specificæ ‡å‡†åŒ–\n",
    "        \n",
    "        æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "        1. æ¯ä¸ªchannelæœ‰å¤šä¸ªæ—¶é—´çª—å£çš„ç‰¹å¾\n",
    "        2. åœ¨æ¯ä¸ªchannelå†…éƒ¨ï¼Œå¯¹è¿™äº›æ—¶é—´çª—å£çš„ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–\n",
    "        3. è¿™æ ·æ¶ˆé™¤äº†channelå†…éƒ¨çš„ç³»ç»Ÿæ€§å·®å¼‚ï¼Œä¿ç•™äº†grey/white matterçš„å·®å¼‚\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ”„ Applying Channel-Specific Normalization ({normalization_method})...\")\n",
    "        \n",
    "        if not self.channel_features_data:\n",
    "            print(\"  âŒ No channel features found. Run extract_channel_centric_features() first.\")\n",
    "            return False\n",
    "        \n",
    "        # è·å–ç‰¹å¾åç§°ï¼ˆä»ç¬¬ä¸€ä¸ªchannelçš„ç¬¬ä¸€ä¸ªçª—å£è·å–ï¼‰\n",
    "        first_channel = list(self.channel_features_data.values())[0]\n",
    "        if len(first_channel['features']) == 0:\n",
    "            print(\"  âŒ No features found in channels.\")\n",
    "            return False\n",
    "        \n",
    "        feature_names = list(first_channel['features'][0].keys())\n",
    "        print(f\"  Normalizing {len(feature_names)} features across {len(self.channel_features_data)} channels\")\n",
    "        \n",
    "        normalization_stats = {}\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªchannelè¿›è¡Œæ ‡å‡†åŒ–\n",
    "        for channel_id, channel_data in self.channel_features_data.items():\n",
    "            patient_id = channel_data['patient_id']\n",
    "            matter_type = channel_data['matter_type']\n",
    "            \n",
    "            # å°†è¿™ä¸ªchannelçš„æ‰€æœ‰çª—å£ç‰¹å¾è½¬æ¢ä¸ºDataFrame\n",
    "            features_list = channel_data['features']\n",
    "            if len(features_list) == 0:\n",
    "                continue\n",
    "                \n",
    "            features_df = pd.DataFrame(features_list)\n",
    "            \n",
    "            # å¯¹è¿™ä¸ªchannelçš„ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–\n",
    "            normalized_features_df = self._normalize_features(features_df, normalization_method)\n",
    "            \n",
    "            # è®¡ç®—æ ‡å‡†åŒ–ç»Ÿè®¡ä¿¡æ¯\n",
    "            before_stats = self._get_feature_stats(features_df)\n",
    "            after_stats = self._get_feature_stats(normalized_features_df)\n",
    "            \n",
    "            # è½¬æ¢å›list of dicts\n",
    "            normalized_features_list = normalized_features_df.to_dict('records')\n",
    "            \n",
    "            # å­˜å‚¨æ ‡å‡†åŒ–åçš„æ•°æ®\n",
    "            normalized_channel_data = channel_data.copy()\n",
    "            normalized_channel_data['features'] = normalized_features_list\n",
    "            normalized_channel_data['normalization_method'] = normalization_method\n",
    "            normalized_channel_data['normalization_stats'] = {\n",
    "                'before': before_stats,\n",
    "                'after': after_stats\n",
    "            }\n",
    "            \n",
    "            self.normalized_channel_features_data[channel_id] = normalized_channel_data\n",
    "            \n",
    "            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯\n",
    "            key = f\"{patient_id}_{matter_type}\"\n",
    "            if key not in normalization_stats:\n",
    "                normalization_stats[key] = {\n",
    "                    'channels': 0,\n",
    "                    'before_std_mean': 0,\n",
    "                    'after_std_mean': 0\n",
    "                }\n",
    "            \n",
    "            normalization_stats[key]['channels'] += 1\n",
    "            normalization_stats[key]['before_std_mean'] += before_stats['std_mean']\n",
    "            normalization_stats[key]['after_std_mean'] += after_stats['std_mean']\n",
    "        \n",
    "        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "        print(f\"\\nğŸ“Š Normalization Statistics by Patient and Matter Type:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Patient_Matter':<20} {'Channels':<10} {'Before_StdMean':<15} {'After_StdMean':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for key, stats in normalization_stats.items():\n",
    "            n_channels = stats['channels']\n",
    "            before_mean = stats['before_std_mean'] / n_channels\n",
    "            after_mean = stats['after_std_mean'] / n_channels\n",
    "            print(f\"{key:<20} {n_channels:<10} {before_mean:<15.3f} {after_mean:<15.3f}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Channel-specific normalization completed!\")\n",
    "        print(f\"   Normalized {len(self.normalized_channel_features_data)} channels\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _normalize_features(self, features_df, method='robust', outlier_clip=True, iqr_multiplier=1.5):\n",
    "        \"\"\"\n",
    "        åº”ç”¨ç‰¹å®šçš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œåœ¨æ ‡å‡†åŒ–å‰ä½¿ç”¨IQRæ–¹æ³•è¿‡æ»¤outliers\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_df : pd.DataFrame\n",
    "            ç‰¹å¾æ•°æ®æ¡†\n",
    "        method : str\n",
    "            æ ‡å‡†åŒ–æ–¹æ³• ('robust', 'standard', 'minmax', 'quantile')\n",
    "        outlier_clip : bool\n",
    "            æ˜¯å¦åœ¨æ ‡å‡†åŒ–å‰clip outliers\n",
    "        iqr_multiplier : float\n",
    "            IQRå€æ•°ï¼Œç”¨äºå®šä¹‰outlieré˜ˆå€¼ (1.5ä¸ºæ ‡å‡†å€¼ï¼Œ3.0ä¸ºæç«¯å€¼)\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized_df = features_df.copy()\n",
    "        outlier_stats = {}  # è®°å½•outlierç»Ÿè®¡ä¿¡æ¯\n",
    "        \n",
    "        for column in features_df.columns:\n",
    "            values = features_df[column].values\n",
    "            original_values = values.copy()\n",
    "            \n",
    "            # Step 1: ä½¿ç”¨IQRæ–¹æ³•clip outliers\n",
    "            if outlier_clip:\n",
    "                q25 = np.percentile(values, 25)\n",
    "                q75 = np.percentile(values, 75)\n",
    "                iqr = q75 - q25\n",
    "                \n",
    "                if iqr > 0:  # é¿å…é™¤é›¶é”™è¯¯\n",
    "                    lower_bound = q25 - iqr_multiplier * iqr\n",
    "                    upper_bound = q75 + iqr_multiplier * iqr\n",
    "                    \n",
    "                    # è®°å½•outlierç»Ÿè®¡\n",
    "                    n_outliers = np.sum((values < lower_bound) | (values > upper_bound))\n",
    "                    outlier_stats[column] = {\n",
    "                        'n_outliers': n_outliers,\n",
    "                        'outlier_ratio': n_outliers / len(values),\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound,\n",
    "                        'original_range': [np.min(values), np.max(values)]\n",
    "                    }\n",
    "                    \n",
    "                    # Clip outliers\n",
    "                    values = np.clip(values, lower_bound, upper_bound)\n",
    "                else:\n",
    "                    # å¦‚æœIQRä¸º0ï¼Œè¯´æ˜å€¼éƒ½ç›¸åŒï¼Œä¸éœ€è¦clip\n",
    "                    outlier_stats[column] = {\n",
    "                        'n_outliers': 0,\n",
    "                        'outlier_ratio': 0.0,\n",
    "                        'lower_bound': values[0],\n",
    "                        'upper_bound': values[0],\n",
    "                        'original_range': [np.min(values), np.max(values)]\n",
    "                    }\n",
    "            \n",
    "            # Step 2: åº”ç”¨æ ‡å‡†åŒ–\n",
    "            if method == 'standard':\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                if std_val > 0:\n",
    "                    normalized_df[column] = (values - mean_val) / std_val\n",
    "                else:\n",
    "                    normalized_df[column] = values - mean_val\n",
    "                    \n",
    "            elif method == 'robust':\n",
    "                median_val = np.median(values)\n",
    "                mad_val = np.median(np.abs(values - median_val))\n",
    "                if mad_val > 0:\n",
    "                    normalized_df[column] = (values - median_val) / (1.4826 * mad_val)\n",
    "                else:\n",
    "                    normalized_df[column] = values - median_val\n",
    "                    \n",
    "            elif method == 'minmax':\n",
    "                min_val = np.min(values)\n",
    "                max_val = np.max(values)\n",
    "                if max_val > min_val:\n",
    "                    normalized_df[column] = (values - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    normalized_df[column] = np.zeros_like(values)\n",
    "                    \n",
    "            elif method == 'quantile':\n",
    "                q25 = np.percentile(values, 25)\n",
    "                q75 = np.percentile(values, 75)\n",
    "                iqr = q75 - q25\n",
    "                median_val = np.median(values)\n",
    "                if iqr > 0:\n",
    "                    normalized_df[column] = (values - median_val) / iqr\n",
    "                else:\n",
    "                    normalized_df[column] = values - median_val\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "        \n",
    "        # å¯é€‰ï¼šå­˜å‚¨outlierç»Ÿè®¡ä¿¡æ¯ä»¥ä¾¿åç»­åˆ†æ\n",
    "        if outlier_clip and hasattr(self, '_outlier_stats'):\n",
    "            if not hasattr(self, '_outlier_stats'):\n",
    "                self._outlier_stats = {}\n",
    "            self._outlier_stats[f'normalize_{len(self._outlier_stats)}'] = outlier_stats\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    def _get_feature_stats(self, features_df):\n",
    "        \"\"\"è·å–ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        means = features_df.mean()\n",
    "        stds = features_df.std()\n",
    "        \n",
    "        return {\n",
    "            'mean_min': means.min(),\n",
    "            'mean_max': means.max(),\n",
    "            'std_min': stds.min(),\n",
    "            'std_max': stds.max(),\n",
    "            'mean_mean': means.mean(),\n",
    "            'std_mean': stds.mean()\n",
    "        }\n",
    "    \n",
    "    def prepare_dataset_for_channel_classification(self, use_normalized=True):\n",
    "        \"\"\"\n",
    "        å‡†å¤‡ç”¨äºchannel-levelåˆ†ç±»çš„æ•°æ®é›†\n",
    "        \n",
    "        æ ¸å¿ƒä¿®æ”¹ï¼š\n",
    "        1. æ¯ä¸ªchannelçš„å¤šä¸ªæ—¶é—´çª—å£æ ·æœ¬ä¼šè¢«åˆ†åˆ«è®­ç»ƒ\n",
    "        2. åœ¨éªŒè¯æ—¶ï¼Œå¯¹åŒä¸€ä¸ªchannelçš„æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡å–å¹³å‡\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        samples_data : list\n",
    "            æ¯ä¸ªsampleçš„ä¿¡æ¯ï¼ŒåŒ…å«ï¼š\n",
    "            - features: ç‰¹å¾å‘é‡\n",
    "            - label: æ ‡ç­¾\n",
    "            - channel_id: æ‰€å±channel ID\n",
    "            - patient_id: æ‰€å±patient ID\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Preparing Dataset for Channel Classification...\")\n",
    "        \n",
    "        # é€‰æ‹©æ•°æ®æº\n",
    "        if use_normalized and self.normalized_channel_features_data:\n",
    "            data_source = self.normalized_channel_features_data\n",
    "        else:\n",
    "            data_source = self.channel_features_data\n",
    "        \n",
    "        if not data_source:\n",
    "            print(\"  âŒ No channel features available.\")\n",
    "            return None\n",
    "        \n",
    "        samples_data = []\n",
    "        \n",
    "        for channel_id, channel_data in data_source.items():\n",
    "            # æ¯ä¸ªchannelçš„æ¯ä¸ªæ—¶é—´çª—å£éƒ½ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„è®­ç»ƒæ ·æœ¬\n",
    "            for window_idx, window_features in enumerate(channel_data['features']):\n",
    "                sample_info = {\n",
    "                    'features': np.array(list(window_features.values())),\n",
    "                    'label': channel_data['label'],\n",
    "                    'channel_id': channel_id,\n",
    "                    'patient_id': channel_data['patient_id'],\n",
    "                    'matter_type': channel_data['matter_type'],\n",
    "                    'window_idx': window_idx\n",
    "                }\n",
    "                samples_data.append(sample_info)\n",
    "        \n",
    "        print(f\"   Total samples: {len(samples_data)}\")\n",
    "        print(f\"   From channels: {len(data_source)} channels\")\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        grey_samples = sum(1 for s in samples_data if s['label'] == 1)\n",
    "        white_samples = sum(1 for s in samples_data if s['label'] == 0)\n",
    "        \n",
    "        print(f\"   Grey matter samples: {grey_samples}\")\n",
    "        print(f\"   White matter samples: {white_samples}\")\n",
    "        \n",
    "        return samples_data\n",
    "    \n",
    "    def leave_one_patient_out_validation_channel_level(self, use_normalized=True):\n",
    "        \"\"\"\n",
    "        Channel-levelçš„Leave-one-patient-outäº¤å‰éªŒè¯\n",
    "        \n",
    "        æ ¸å¿ƒä¿®æ”¹ï¼š\n",
    "        1. è®­ç»ƒæ—¶ä½¿ç”¨æ‰€æœ‰çš„æ—¶é—´çª—å£æ ·æœ¬\n",
    "        2. éªŒè¯æ—¶è®¡ç®—æ¯ä¸ªpatientçš„æ¯ä¸ªchannelçš„å¹³å‡é¢„æµ‹æ¦‚ç‡\n",
    "        3. åŸºäºå¹³å‡æ¦‚ç‡è¿›è¡Œchannel-levelåˆ†ç±»\n",
    "        4. è®¡ç®—æ¯ä¸ªpatientçš„channel-level accuracy\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ”„ Channel-Level Leave-One-Patient-Out Validation\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        samples_data = self.prepare_dataset_for_channel_classification(use_normalized)\n",
    "        \n",
    "        if samples_data is None:\n",
    "            print(\"âŒ Failed to prepare dataset\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # è·å–æ‚£è€…åˆ—è¡¨\n",
    "        patients = list(set([sample['patient_id'] for sample in samples_data]))\n",
    "        n_patients = len(patients)\n",
    "        \n",
    "        if n_patients < 3:\n",
    "            raise ValueError(f\"Need at least 3 patients, {n_patients} patients found\")\n",
    "        \n",
    "        print(f\"   {n_patients} patients, {len(samples_data)} samples total\")\n",
    "        \n",
    "        # å­˜å‚¨ç»“æœ\n",
    "        cv_results = {name: [] for name in self.classifiers.keys()}\n",
    "        all_predictions = {name: {'y_true': [], 'y_pred': [], 'y_proba': [], 'test_patients': [], 'channel_ids': []} \n",
    "                          for name in self.classifiers.keys()}\n",
    "        \n",
    "        # Leave-one-patient-outå¾ªç¯\n",
    "        for fold, test_patient in enumerate(patients):\n",
    "            train_patients = [p for p in patients if p != test_patient]\n",
    "            \n",
    "            print(f\"\\nFold {fold+1}/{n_patients}: æµ‹è¯• {test_patient}\")\n",
    "            \n",
    "            # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬\n",
    "            train_samples = [s for s in samples_data if s['patient_id'] in train_patients]\n",
    "            test_samples = [s for s in samples_data if s['patient_id'] == test_patient]\n",
    "            \n",
    "            # å‡†å¤‡è®­ç»ƒæ•°æ® (sample-level)\n",
    "            X_train = np.array([s['features'] for s in train_samples])\n",
    "            y_train = np.array([s['label'] for s in train_samples])\n",
    "            \n",
    "            # å‡†å¤‡æµ‹è¯•æ•°æ® (sample-level)  \n",
    "            X_test = np.array([s['features'] for s in test_samples])\n",
    "            y_test = np.array([s['label'] for s in test_samples])\n",
    "            \n",
    "            # è·å–æµ‹è¯•é›†çš„channelä¿¡æ¯\n",
    "            test_channel_ids = [s['channel_id'] for s in test_samples]\n",
    "            test_channel_labels = [s['label'] for s in test_samples]\n",
    "            \n",
    "            print(f\"   Training samples: {len(X_train)} ({np.sum(y_train)} grey, {np.sum(y_train==0)} white)\")\n",
    "            print(f\"   Testing samples: {len(X_test)} ({np.sum(y_test)} grey, {np.sum(y_test==0)} white)\")\n",
    "            \n",
    "            # è·¨æ‚£è€…æ ‡å‡†åŒ–ï¼ˆå¦‚æœä½¿ç”¨åŸå§‹ç‰¹å¾ï¼‰\n",
    "            if not use_normalized:\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªåˆ†ç±»å™¨\n",
    "            for clf_name, clf in self.classifiers.items():\n",
    "                try:\n",
    "                    # è®­ç»ƒåˆ†ç±»å™¨ (sample-level)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    \n",
    "                    # é¢„æµ‹æµ‹è¯•é›† (sample-level)\n",
    "                    y_pred_samples = clf.predict(X_test)\n",
    "                    \n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        y_proba_samples = clf.predict_proba(X_test)[:, 1]\n",
    "                    else:\n",
    "                        y_proba_samples = np.zeros_like(y_pred_samples, dtype=float)\n",
    "                    \n",
    "                    # å°†sample-levelé¢„æµ‹èšåˆä¸ºchannel-levelé¢„æµ‹\n",
    "                    channel_predictions = self._aggregate_predictions_to_channel_level(\n",
    "                        test_samples, y_pred_samples, y_proba_samples\n",
    "                    )\n",
    "                    \n",
    "                    # æå–channel-levelçš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹\n",
    "                    channel_y_true = [pred['true_label'] for pred in channel_predictions]\n",
    "                    channel_y_pred = [pred['pred_label'] for pred in channel_predictions]\n",
    "                    channel_y_proba = [pred['avg_proba'] for pred in channel_predictions]\n",
    "                    channel_ids = [pred['channel_id'] for pred in channel_predictions]\n",
    "                    \n",
    "                    # è®¡ç®—channel-levelæŒ‡æ ‡\n",
    "                    fold_results = {\n",
    "                        'fold': fold,\n",
    "                        'test_patient': test_patient,\n",
    "                        'accuracy': accuracy_score(channel_y_true, channel_y_pred),\n",
    "                        'f1_score': f1_score(channel_y_true, channel_y_pred, zero_division=0),\n",
    "                        'precision': precision_score(channel_y_true, channel_y_pred, zero_division=0),\n",
    "                        'recall': recall_score(channel_y_true, channel_y_pred, zero_division=0),\n",
    "                        'balanced_accuracy': balanced_accuracy_score(channel_y_true, channel_y_pred),\n",
    "                        'n_test_channels': len(channel_predictions),\n",
    "                        'n_test_samples': len(y_test)\n",
    "                    }\n",
    "                    \n",
    "                    if len(np.unique(channel_y_true)) > 1:\n",
    "                        fold_results['roc_auc'] = roc_auc_score(channel_y_true, channel_y_proba)\n",
    "                    else:\n",
    "                        fold_results['roc_auc'] = 0.5\n",
    "                    \n",
    "                    cv_results[clf_name].append(fold_results)\n",
    "                    \n",
    "                    # å­˜å‚¨channel-levelé¢„æµ‹ç»“æœ\n",
    "                    all_predictions[clf_name]['y_true'].extend(channel_y_true)\n",
    "                    all_predictions[clf_name]['y_pred'].extend(channel_y_pred)\n",
    "                    all_predictions[clf_name]['y_proba'].extend(channel_y_proba)\n",
    "                    all_predictions[clf_name]['test_patients'].extend([test_patient] * len(channel_predictions))\n",
    "                    all_predictions[clf_name]['channel_ids'].extend(channel_ids)\n",
    "                    \n",
    "                    print(f\"    {clf_name}: Channels - F1={fold_results['f1_score']:.3f}, \"\n",
    "                          f\"Acc={fold_results['accuracy']:.3f}, AUC={fold_results['roc_auc']:.3f} \"\n",
    "                          f\"({fold_results['n_test_channels']} channels)\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"    {clf_name}: Error - {e}\")\n",
    "        \n",
    "        return cv_results, all_predictions, samples_data\n",
    "    \n",
    "    def _aggregate_predictions_to_channel_level(self, test_samples, y_pred_samples, y_proba_samples):\n",
    "        \"\"\"\n",
    "        å°†sample-levelçš„é¢„æµ‹èšåˆä¸ºchannel-levelçš„é¢„æµ‹\n",
    "        \n",
    "        å¯¹æ¯ä¸ªchannelçš„æ‰€æœ‰æ ·æœ¬ï¼ˆæ—¶é—´çª—å£ï¼‰çš„é¢„æµ‹æ¦‚ç‡å–å¹³å‡ï¼Œç„¶ååŸºäºå¹³å‡æ¦‚ç‡è¿›è¡Œåˆ†ç±»\n",
    "        \"\"\"\n",
    "        \n",
    "        # æŒ‰channel_idåˆ†ç»„\n",
    "        channel_groups = {}\n",
    "        for i, sample in enumerate(test_samples):\n",
    "            channel_id = sample['channel_id']\n",
    "            if channel_id not in channel_groups:\n",
    "                channel_groups[channel_id] = {\n",
    "                    'sample_indices': [],\n",
    "                    'true_label': sample['label'],  # åŒä¸€ä¸ªchannelçš„æ‰€æœ‰æ ·æœ¬åº”è¯¥æœ‰ç›¸åŒçš„æ ‡ç­¾\n",
    "                    'patient_id': sample['patient_id']\n",
    "                }\n",
    "            channel_groups[channel_id]['sample_indices'].append(i)\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªchannelè®¡ç®—å¹³å‡é¢„æµ‹æ¦‚ç‡\n",
    "        channel_predictions = []\n",
    "        for channel_id, group_info in channel_groups.items():\n",
    "            sample_indices = group_info['sample_indices']\n",
    "            \n",
    "            # è·å–è¿™ä¸ªchannelçš„æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡\n",
    "            channel_probas = y_proba_samples[sample_indices]\n",
    "            \n",
    "            # è®¡ç®—å¹³å‡æ¦‚ç‡\n",
    "            avg_proba = np.mean(channel_probas)\n",
    "            \n",
    "            # åŸºäºå¹³å‡æ¦‚ç‡è¿›è¡Œåˆ†ç±» (threshold = 0.5)\n",
    "            pred_label = 1 if avg_proba > 0.5 else 0\n",
    "            \n",
    "            channel_predictions.append({\n",
    "                'channel_id': channel_id,\n",
    "                'true_label': group_info['true_label'],\n",
    "                'pred_label': pred_label,\n",
    "                'avg_proba': avg_proba,\n",
    "                'patient_id': group_info['patient_id'],\n",
    "                'n_samples': len(sample_indices)\n",
    "            })\n",
    "        \n",
    "        return channel_predictions\n",
    "    \n",
    "    def analyze_channel_level_results(self, cv_results, all_predictions):\n",
    "        \"\"\"åˆ†æchannel-leveläº¤å‰éªŒè¯ç»“æœ\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Channel-Level Analysis Results...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        final_results = {}\n",
    "        \n",
    "        for clf_name in self.classifiers.keys():\n",
    "            if len(cv_results[clf_name]) > 0:\n",
    "                # CVæŒ‡æ ‡ç»Ÿè®¡\n",
    "                cv_metrics = {}\n",
    "                for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'balanced_accuracy', 'roc_auc']:\n",
    "                    values = [fold[metric] for fold in cv_results[clf_name]]\n",
    "                    cv_metrics[f'{metric}_mean'] = np.mean(values)\n",
    "                    cv_metrics[f'{metric}_std'] = np.std(values)\n",
    "                \n",
    "                # æ•´ä½“é¢„æµ‹æŒ‡æ ‡\n",
    "                y_true_all = np.array(all_predictions[clf_name]['y_true'])\n",
    "                y_pred_all = np.array(all_predictions[clf_name]['y_pred'])\n",
    "                y_proba_all = np.array(all_predictions[clf_name]['y_proba'])\n",
    "                \n",
    "                overall_metrics = {\n",
    "                    'overall_accuracy': accuracy_score(y_true_all, y_pred_all),\n",
    "                    'overall_f1': f1_score(y_true_all, y_pred_all),\n",
    "                    'overall_precision': precision_score(y_true_all, y_pred_all),\n",
    "                    'overall_recall': recall_score(y_true_all, y_pred_all),\n",
    "                    'overall_balanced_acc': balanced_accuracy_score(y_true_all, y_pred_all)\n",
    "                }\n",
    "                \n",
    "                if len(np.unique(y_true_all)) > 1:\n",
    "                    overall_metrics['overall_roc_auc'] = roc_auc_score(y_true_all, y_proba_all)\n",
    "                else:\n",
    "                    overall_metrics['overall_roc_auc'] = 0.5\n",
    "                \n",
    "                # åˆå¹¶ç»“æœ\n",
    "                final_results[clf_name] = {\n",
    "                    **cv_metrics,\n",
    "                    **overall_metrics,\n",
    "                    'cv_folds': cv_results[clf_name],\n",
    "                    'predictions': all_predictions[clf_name],\n",
    "                    'confusion_matrix': confusion_matrix(y_true_all, y_pred_all)\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{clf_name}:\")\n",
    "                print(f\"  CV F1: {cv_metrics['f1_score_mean']:.3f} Â± {cv_metrics['f1_score_std']:.3f}\")\n",
    "                print(f\"  Overall F1: {overall_metrics['overall_f1']:.3f}\")\n",
    "                print(f\"  Overall Balanced Acc: {overall_metrics['overall_balanced_acc']:.3f}\")\n",
    "                print(f\"  Overall ROC AUC: {overall_metrics['overall_roc_auc']:.3f}\")\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³åˆ†ç±»å™¨\n",
    "        if final_results:\n",
    "            best_classifier = max(final_results.items(), key=lambda x: x[1]['overall_f1'])\n",
    "            best_name, best_metrics = best_classifier\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ğŸ† Best Channel-Level Classifier: {best_name}\")\n",
    "            print(f\"   Overall F1 Score: {best_metrics['overall_f1']:.3f}\")\n",
    "            print(f\"   CV F1: {best_metrics['f1_score_mean']:.3f} Â± {best_metrics['f1_score_std']:.3f}\")\n",
    "            print(f\"   Overall Balanced Acc: {best_metrics['overall_balanced_acc']:.3f}\")\n",
    "            print(f\"   Overall ROC AUC: {best_metrics['overall_roc_auc']:.3f}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        else:\n",
    "            best_name, best_metrics = None, None\n",
    "        \n",
    "        return final_results, best_name, best_metrics\n",
    "    \n",
    "    def analyze_patient_level_performance(self, cv_results, all_predictions):\n",
    "        \"\"\"\n",
    "        åˆ†ææ¯ä¸ªpatientçš„channel-levelæ€§èƒ½\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Patient-Level Channel Classification Performance\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªåˆ†ç±»å™¨åˆ†æpatient-levelæ€§èƒ½\n",
    "        for clf_name in self.classifiers.keys():\n",
    "            if len(cv_results[clf_name]) == 0:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{clf_name}:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Patient':<12} {'Channels':<10} {'Accuracy':<10} {'F1':<8} {'Precision':<10} {'Recall':<8}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            total_channels = 0\n",
    "            total_correct = 0\n",
    "            patient_f1_scores = []\n",
    "            \n",
    "            for fold_result in cv_results[clf_name]:\n",
    "                test_patient = fold_result['test_patient']\n",
    "                n_channels = fold_result['n_test_channels']\n",
    "                accuracy = fold_result['accuracy']\n",
    "                f1 = fold_result['f1_score']\n",
    "                precision = fold_result['precision']\n",
    "                recall = fold_result['recall']\n",
    "                \n",
    "                print(f\"{test_patient:<12} {n_channels:<10} {accuracy:<10.3f} {f1:<8.3f} {precision:<10.3f} {recall:<8.3f}\")\n",
    "                \n",
    "                total_channels += n_channels\n",
    "                total_correct += int(accuracy * n_channels)\n",
    "                patient_f1_scores.append(f1)\n",
    "            \n",
    "            # è®¡ç®—æ€»ä½“ç»Ÿè®¡\n",
    "            overall_accuracy = total_correct / total_channels if total_channels > 0 else 0\n",
    "            mean_patient_f1 = np.mean(patient_f1_scores) if patient_f1_scores else 0\n",
    "            std_patient_f1 = np.std(patient_f1_scores) if patient_f1_scores else 0\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{'Total':<12} {total_channels:<10} {overall_accuracy:<10.3f} {mean_patient_f1:<8.3f}\")\n",
    "            print(f\"Patient F1 Std: {std_patient_f1:.3f}\")\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def create_channel_level_visualization(self, use_normalized=True, cv_results=None, all_predictions=None):\n",
    "        \"\"\"åˆ›å»ºchannel-levelçš„å¯è§†åŒ–\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Creating Channel-Level Visualizations...\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        samples_data = self.prepare_dataset_for_channel_classification(use_normalized)\n",
    "        \n",
    "        if samples_data is None:\n",
    "            print(\"âŒ No data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # åˆ›å»ºæ‚£è€…ä¿¡æ¯æ˜ å°„\n",
    "        unique_patients = list(set([s['patient_id'] for s in samples_data]))\n",
    "        patient_colors = {}\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_patients)))\n",
    "        \n",
    "        for i, patient in enumerate(unique_patients):\n",
    "            patient_colors[patient] = colors[i]\n",
    "        \n",
    "        # 1. ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”ï¼ˆæ ‡å‡†åŒ–å‰åï¼‰\n",
    "        if use_normalized and self.channel_features_data:\n",
    "            self._create_normalization_effect_plot()\n",
    "        \n",
    "        # 2. æ‚£è€…é—´channelåˆ†å¸ƒ\n",
    "        self._create_patient_channel_distribution_plot(samples_data, patient_colors)\n",
    "        \n",
    "        # 3. ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
    "        self._create_feature_importance_plot(samples_data)\n",
    "        \n",
    "        # 4. å¦‚æœæœ‰CVç»“æœï¼Œåˆ›å»ºæ€§èƒ½å¯è§†åŒ–\n",
    "        if cv_results and all_predictions:\n",
    "            self._create_performance_visualization(cv_results, all_predictions)\n",
    "        \n",
    "        print(f\"  ğŸ’¾ Visualizations saved to: {self.output_folder}\")\n",
    "    \n",
    "    def _create_normalization_effect_plot(self):\n",
    "        \"\"\"åˆ›å»ºæ ‡å‡†åŒ–æ•ˆæœå¯¹æ¯”å›¾\"\"\"\n",
    "        \n",
    "        print(\"  ğŸ“ˆ Creating normalization effect plots...\")\n",
    "        \n",
    "        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§çš„channelsè¿›è¡Œå¯¹æ¯”\n",
    "        sample_channels = list(self.channel_features_data.keys())[:6]  # å‰6ä¸ªchannel\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, channel_id in enumerate(sample_channels):\n",
    "            if i >= 6:\n",
    "                break\n",
    "                \n",
    "            # åŸå§‹æ•°æ®\n",
    "            orig_features = pd.DataFrame(self.channel_features_data[channel_id]['features'])\n",
    "            \n",
    "            # æ ‡å‡†åŒ–æ•°æ®\n",
    "            if channel_id in self.normalized_channel_features_data:\n",
    "                norm_features = pd.DataFrame(self.normalized_channel_features_data[channel_id]['features'])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§ç‰¹å¾è¿›è¡Œå¯è§†åŒ–\n",
    "            feature_name = 'rel_power_gamma'  # æˆ–è€…é€‰æ‹©å…¶ä»–ç‰¹å¾\n",
    "            if feature_name in orig_features.columns:\n",
    "                \n",
    "                ax = axes[i]\n",
    "                \n",
    "                # ç»˜åˆ¶åŸå§‹æ•°æ®åˆ†å¸ƒ\n",
    "                ax.hist(orig_features[feature_name], alpha=0.6, label='Original', bins=20, color='blue')\n",
    "                \n",
    "                # ç»˜åˆ¶æ ‡å‡†åŒ–æ•°æ®åˆ†å¸ƒ\n",
    "                ax.hist(norm_features[feature_name], alpha=0.6, label='Normalized', bins=20, color='red')\n",
    "                \n",
    "                # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜\n",
    "                patient_id = self.channel_features_data[channel_id]['patient_id']\n",
    "                matter_type = self.channel_features_data[channel_id]['matter_type']\n",
    "                \n",
    "                ax.set_title(f'{patient_id}_{matter_type}\\n{feature_name}', fontsize=10)\n",
    "                ax.set_xlabel('Feature Value')\n",
    "                ax.set_ylabel('Frequency')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # éšè—å¤šä½™çš„å­å›¾\n",
    "        for i in range(len(sample_channels), 6):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Channel-Level Normalization Effect', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'channel_normalization_effect.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_patient_channel_distribution_plot(self, samples_data, patient_colors):\n",
    "        \"\"\"åˆ›å»ºæ‚£è€…é—´channelåˆ†å¸ƒå›¾\"\"\"\n",
    "        \n",
    "        print(\"  ğŸ“Š Creating patient channel distribution plot...\")\n",
    "        \n",
    "        # ç»Ÿè®¡æ¯ä¸ªæ‚£è€…çš„channelæ•°é‡\n",
    "        patient_stats = {}\n",
    "        channel_counts = {}  # ç»Ÿè®¡unique channels\n",
    "        \n",
    "        for sample in samples_data:\n",
    "            patient_id = sample['patient_id']\n",
    "            matter_type = sample['matter_type']\n",
    "            channel_id = sample['channel_id']\n",
    "            \n",
    "            if patient_id not in patient_stats:\n",
    "                patient_stats[patient_id] = {'grey': set(), 'white': set()}\n",
    "            \n",
    "            patient_stats[patient_id][matter_type].add(channel_id)\n",
    "            \n",
    "        # è½¬æ¢ä¸ºè®¡æ•°\n",
    "        for patient_id in patient_stats:\n",
    "            patient_stats[patient_id]['grey'] = len(patient_stats[patient_id]['grey'])\n",
    "            patient_stats[patient_id]['white'] = len(patient_stats[patient_id]['white'])\n",
    "        \n",
    "        # åˆ›å»ºå›¾è¡¨\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 1. æ¯ä¸ªæ‚£è€…çš„channelåˆ†å¸ƒ\n",
    "        patients = list(patient_stats.keys())\n",
    "        grey_counts = [patient_stats[p]['grey'] for p in patients]\n",
    "        white_counts = [patient_stats[p]['white'] for p in patients]\n",
    "        \n",
    "        x = np.arange(len(patients))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, grey_counts, width, label='Grey Matter', alpha=0.8, color='red')\n",
    "        ax1.bar(x + width/2, white_counts, width, label='White Matter', alpha=0.8, color='blue')\n",
    "        \n",
    "        ax1.set_xlabel('Patient ID')\n",
    "        ax1.set_ylabel('Number of Channels')\n",
    "        ax1.set_title('Channel Distribution by Patient')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(patients, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æ€»ä½“åˆ†å¸ƒé¥¼å›¾\n",
    "        total_grey = sum(grey_counts)\n",
    "        total_white = sum(white_counts)\n",
    "        \n",
    "        ax2.pie([total_grey, total_white], \n",
    "               labels=['Grey Matter', 'White Matter'],\n",
    "               colors=['red', 'blue'],\n",
    "               autopct='%1.1f%%')\n",
    "        ax2.set_title(f'Overall Channel Distribution\\n(Total: {total_grey + total_white} channels)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'patient_channel_distribution.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_patient_feature_visualization(self, samples_data, selected_features=None, max_patients_per_plot=8):\n",
    "        \"\"\"\n",
    "        ä¸ºæ¯ä¸ªfeatureåˆ›å»ºå•ç‹¬çš„å¯è§†åŒ–å›¾ï¼Œæ¯ä¸ªå›¾åŒ…å«æ‰€æœ‰æ‚£è€…çš„è¯¥featureåˆ†å¸ƒ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        samples_data : list\n",
    "            æ ·æœ¬æ•°æ®åˆ—è¡¨\n",
    "        selected_features : list, optional\n",
    "            è¦å¯è§†åŒ–çš„ç‰¹å¾åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¾\n",
    "        max_patients_per_plot : int\n",
    "            æ¯ä¸ªå›¾æœ€å¤šæ˜¾ç¤ºçš„æ‚£è€…æ•°é‡\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"  ğŸ“Š Creating per-feature patient visualization...\")\n",
    "        \n",
    "        # è·å–ç‰¹å¾åç§°\n",
    "        if not samples_data:\n",
    "            print(\"    âŒ No samples data available\")\n",
    "            return\n",
    "            \n",
    "        # ä»ç¬¬ä¸€ä¸ªsampleè·å–ç‰¹å¾åç§°\n",
    "        first_channel = list(self.channel_features_data.values())[0]\n",
    "        all_feature_names = list(first_channel['features'][0].keys())\n",
    "        \n",
    "        if selected_features is None:\n",
    "            # é€‰æ‹©ä¸€äº›ä»£è¡¨æ€§ç‰¹å¾è¿›è¡Œå¯è§†åŒ–\n",
    "            selected_features = [\n",
    "                'std', 'rms', 'area',\n",
    "                'rel_power_delta', 'rel_power_theta', 'rel_power_alpha', \n",
    "                'rel_power_beta', 'rel_power_gamma', 'rel_power_high_gamma',\n",
    "                'range', 'mad'\n",
    "            ]\n",
    "            # åªä¿ç•™å®é™…å­˜åœ¨çš„ç‰¹å¾\n",
    "            selected_features = [f for f in selected_features if f in all_feature_names]\n",
    "        \n",
    "        # ç»„ç»‡æ•°æ®ï¼šæŒ‰patientå’Œmatter_typeåˆ†ç»„\n",
    "        patient_feature_data = {}\n",
    "        feature_names = list(samples_data[0]['features'].keys()) if samples_data else []\n",
    "        \n",
    "        for sample in samples_data:\n",
    "            patient_id = sample['patient_id']\n",
    "            matter_type = sample['matter_type']\n",
    "            \n",
    "            if patient_id not in patient_feature_data:\n",
    "                patient_feature_data[patient_id] = {'grey': [], 'white': []}\n",
    "            \n",
    "            # å°†featuresè½¬æ¢ä¸ºdictæ ¼å¼ï¼ˆå¦‚æœæ˜¯arrayï¼‰\n",
    "            if isinstance(sample['features'], np.ndarray):\n",
    "                feature_dict = {name: sample['features'][i] for i, name in enumerate(feature_names)}\n",
    "            else:\n",
    "                feature_dict = sample['features']\n",
    "            \n",
    "            patient_feature_data[patient_id][matter_type].append(feature_dict)\n",
    "        \n",
    "        # è·å–æ‚£è€…åˆ—è¡¨å¹¶æ’åº\n",
    "        patients = sorted(list(patient_feature_data.keys()))\n",
    "        \n",
    "        # å¦‚æœæ‚£è€…å¤ªå¤šï¼Œåˆ†æ‰¹å¤„ç†\n",
    "        patient_batches = [patients[i:i+max_patients_per_plot] \n",
    "                          for i in range(0, len(patients), max_patients_per_plot)]\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªé€‰å®šçš„ç‰¹å¾åˆ›å»ºå¯è§†åŒ–\n",
    "        for feature_name in selected_features:\n",
    "            print(f\"    Creating visualization for feature: {feature_name}\")\n",
    "            \n",
    "            for batch_idx, patient_batch in enumerate(patient_batches):\n",
    "                # è®¡ç®—subplotå¸ƒå±€\n",
    "                n_patients = len(patient_batch)\n",
    "                n_cols = min(4, n_patients)  # æœ€å¤š4åˆ—\n",
    "                n_rows = (n_patients + n_cols - 1) // n_cols  # å‘ä¸Šå–æ•´\n",
    "                \n",
    "                # åˆ›å»ºfigure\n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "                \n",
    "                # ç¡®ä¿axesæ˜¯2Dæ•°ç»„\n",
    "                if n_rows == 1 and n_cols == 1:\n",
    "                    axes = np.array([[axes]])\n",
    "                elif n_rows == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                elif n_cols == 1:\n",
    "                    axes = axes.reshape(-1, 1)\n",
    "                \n",
    "                # ä¸ºæ¯ä¸ªæ‚£è€…åˆ›å»ºsubplot\n",
    "                for i, patient_id in enumerate(patient_batch):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    ax = axes[row, col]\n",
    "                    \n",
    "                    # æå–è¯¥æ‚£è€…è¯¥ç‰¹å¾çš„æ•°æ®\n",
    "                    grey_values = []\n",
    "                    white_values = []\n",
    "                    \n",
    "                    for sample_dict in patient_feature_data[patient_id]['grey']:\n",
    "                        if feature_name in sample_dict:\n",
    "                            grey_values.append(sample_dict[feature_name])\n",
    "                    \n",
    "                    for sample_dict in patient_feature_data[patient_id]['white']:\n",
    "                        if feature_name in sample_dict:\n",
    "                            white_values.append(sample_dict[feature_name])\n",
    "                    \n",
    "                    # ç»˜åˆ¶åˆ†å¸ƒ\n",
    "                    if grey_values:\n",
    "                        ax.hist(grey_values, bins=20, alpha=0.6, label='Grey', color='red', density=True)\n",
    "                    if white_values:\n",
    "                        ax.hist(white_values, bins=20, alpha=0.6, label='White', color='blue', density=True)\n",
    "                    \n",
    "                    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "                    ax.set_title(f'{patient_id}\\n(G:{len(grey_values)}, W:{len(white_values)})', fontsize=10)\n",
    "                    ax.set_xlabel(f'{feature_name}')\n",
    "                    ax.set_ylabel('Density')\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # è®¾ç½®åˆé€‚çš„xè½´èŒƒå›´\n",
    "                    all_values = grey_values + white_values\n",
    "                    if all_values:\n",
    "                        ax.set_xlim(np.percentile(all_values, [1, 99]))\n",
    "                \n",
    "                # éšè—å¤šä½™çš„subplots\n",
    "                for i in range(len(patient_batch), n_rows * n_cols):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    axes[row, col].set_visible(False)\n",
    "                \n",
    "                # è®¾ç½®æ•´ä½“æ ‡é¢˜\n",
    "                batch_suffix = f\"_batch{batch_idx+1}\" if len(patient_batches) > 1 else \"\"\n",
    "                fig.suptitle(f'Feature Distribution: {feature_name}{batch_suffix}', fontsize=16, y=0.98)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.93)  # ä¸ºsuptitleç•™å‡ºç©ºé—´\n",
    "                \n",
    "                # ä¿å­˜å›¾ç‰‡\n",
    "                safe_feature_name = feature_name.replace('/', '_').replace('\\\\', '_')\n",
    "                filename = f'feature_{safe_feature_name}{batch_suffix}.png'\n",
    "                plt.savefig(os.path.join(self.output_folder, filename), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        print(f\"    âœ… Created visualizations for {len(selected_features)} features\")\n",
    "        return True\n",
    "    \n",
    "    def _create_feature_importance_plot(self, samples_data):\n",
    "        \"\"\"åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾\"\"\"\n",
    "        \n",
    "        print(\"  ğŸ¯ Creating feature importance plot...\")\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        X = np.array([s['features'] for s in samples_data])\n",
    "        y = np.array([s['label'] for s in samples_data])\n",
    "        \n",
    "        # ä½¿ç”¨éšæœºæ£®æ—è®¡ç®—ç‰¹å¾é‡è¦æ€§\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # è·å–ç‰¹å¾é‡è¦æ€§\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        # è·å–ç‰¹å¾åç§°\n",
    "        first_channel = list(self.channel_features_data.values())[0]\n",
    "        feature_names = list(first_channel['features'][0].keys())\n",
    "        \n",
    "        # æ’åº\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾\n",
    "        n_features = min(20, len(importances))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(n_features), importances[indices[:n_features]], alpha=0.8)\n",
    "        plt.yticks(range(n_features), [feature_names[i] for i in indices[:n_features]])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'feature_importance.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_performance_visualization(self, cv_results, all_predictions):\n",
    "        \"\"\"åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨\"\"\"\n",
    "        \n",
    "        print(\"  ğŸ“ˆ Creating performance visualization...\")\n",
    "        \n",
    "        # 1. åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        classifiers = []\n",
    "        f1_means = []\n",
    "        f1_stds = []\n",
    "        accuracies = []\n",
    "        aucs = []\n",
    "        \n",
    "        for clf_name in self.classifiers.keys():\n",
    "            if len(cv_results[clf_name]) > 0:\n",
    "                classifiers.append(clf_name)\n",
    "                \n",
    "                f1_values = [fold['f1_score'] for fold in cv_results[clf_name]]\n",
    "                f1_means.append(np.mean(f1_values))\n",
    "                f1_stds.append(np.std(f1_values))\n",
    "                \n",
    "                acc_values = [fold['accuracy'] for fold in cv_results[clf_name]]\n",
    "                accuracies.append(np.mean(acc_values))\n",
    "                \n",
    "                auc_values = [fold['roc_auc'] for fold in cv_results[clf_name]]\n",
    "                aucs.append(np.mean(auc_values))\n",
    "        \n",
    "        # F1 Score with error bars\n",
    "        x_pos = np.arange(len(classifiers))\n",
    "        ax1.bar(x_pos, f1_means, yerr=f1_stds, capsize=5, alpha=0.8)\n",
    "        ax1.set_xlabel('Classifiers')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('F1 Score by Classifier (with std)')\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(classifiers, rotation=45)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        ax2.bar(x_pos, accuracies, alpha=0.8, color='orange')\n",
    "        ax2.set_xlabel('Classifiers')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy by Classifier')\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(classifiers, rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC AUC comparison\n",
    "        ax3.bar(x_pos, aucs, alpha=0.8, color='green')\n",
    "        ax3.set_xlabel('Classifiers')\n",
    "        ax3.set_ylabel('ROC AUC')\n",
    "        ax3.set_title('ROC AUC by Classifier')\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(classifiers, rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Patient-wise performance for best classifier\n",
    "        if f1_means:\n",
    "            best_clf_idx = np.argmax(f1_means)\n",
    "            best_clf_name = classifiers[best_clf_idx]\n",
    "            \n",
    "            patient_f1s = [fold['f1_score'] for fold in cv_results[best_clf_name]]\n",
    "            patient_names = [fold['test_patient'] for fold in cv_results[best_clf_name]]\n",
    "            \n",
    "            ax4.bar(range(len(patient_f1s)), patient_f1s, alpha=0.8, color='purple')\n",
    "            ax4.set_xlabel('Patients')\n",
    "            ax4.set_ylabel('F1 Score')\n",
    "            ax4.set_title(f'Patient-wise F1 Score ({best_clf_name})')\n",
    "            ax4.set_xticks(range(len(patient_names)))\n",
    "            ax4.set_xticklabels(patient_names, rotation=45)\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'performance_comparison.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def run_complete_channel_level_analysis(self, \n",
    "                                          use_windowing=True,\n",
    "                                          window_size_ms=500,\n",
    "                                          step_size_ms=250,\n",
    "                                          max_windows_per_channel=200,\n",
    "                                          normalization_method='robust'):\n",
    "        \"\"\"\n",
    "        è¿è¡Œå®Œæ•´çš„channel-levelåˆ†æ\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ§  Channel-Level Grey/White Matter Classification Analysis\")\n",
    "        print(f\"=\" * 80)\n",
    "        \n",
    "        # 1. åŠ è½½æ‚£è€…æ•°æ®\n",
    "        if not self.load_all_patients():\n",
    "            print(\"âŒ Loading patient data failed\")\n",
    "            return None\n",
    "        \n",
    "        # 2. æå–channel-centricç‰¹å¾\n",
    "        if not self.extract_channel_centric_features(\n",
    "            use_windowing=use_windowing,\n",
    "            window_size_ms=window_size_ms,\n",
    "            step_size_ms=step_size_ms,\n",
    "            max_windows_per_channel=max_windows_per_channel\n",
    "        ):\n",
    "            print(\"âŒ Channel feature extraction failed\")\n",
    "            return None\n",
    "        \n",
    "        # 3. åº”ç”¨channel-specificæ ‡å‡†åŒ–\n",
    "        if not self.apply_channel_specific_normalization(normalization_method):\n",
    "            print(\"âŒ Channel-specific normalization failed\")\n",
    "            return None\n",
    "        \n",
    "        # 4. ä¸»è¦åˆ†æï¼šchannel-levelåˆ†ç±»\n",
    "        cv_results, all_predictions, samples_data = self.leave_one_patient_out_validation_channel_level(\n",
    "            use_normalized=True\n",
    "        )\n",
    "        \n",
    "        final_results, best_name, best_metrics = self.analyze_channel_level_results(cv_results, all_predictions)\n",
    "        \n",
    "        # 5. åˆ†æpatient-levelæ€§èƒ½\n",
    "        self.analyze_patient_level_performance(cv_results, all_predictions)\n",
    "        \n",
    "        # 6. åˆ›å»ºå¯è§†åŒ–\n",
    "        self.create_channel_level_visualization(use_normalized=True, cv_results=cv_results, all_predictions=all_predictions)\n",
    "        \n",
    "        # 7. ä¿å­˜ç»“æœ\n",
    "        self._save_channel_level_results(final_results, best_name, normalization_method, cv_results)\n",
    "        \n",
    "        # 8. æ±‡æ€»è¾“å‡º\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Channel-Level Analysis Complete\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        n_channels = len(self.channel_features_data)\n",
    "        n_grey = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 1)\n",
    "        n_white = sum(1 for ch in self.channel_features_data.values() if ch['label'] == 0)\n",
    "        n_patients = len(set([ch['patient_id'] for ch in self.channel_features_data.values()]))\n",
    "        \n",
    "        print(f\"Number of Patients: {n_patients}\")\n",
    "        print(f\"Total Channels: {n_channels}\")\n",
    "        print(f\"Grey Matter Channels: {n_grey}\")\n",
    "        print(f\"White Matter Channels: {n_white}\")\n",
    "        print(f\"Normalization Method: {normalization_method}\")\n",
    "        \n",
    "        if best_name and best_metrics:\n",
    "            print(f\"\\nğŸ† Best Channel-Level Classifier: {best_name}\")\n",
    "            print(f\"   Overall F1 Score: {best_metrics['overall_f1']:.3f}\")\n",
    "            print(f\"   CV F1: {best_metrics['f1_score_mean']:.3f} Â± {best_metrics['f1_score_std']:.3f}\")\n",
    "            print(f\"   Overall Balanced Acc: {best_metrics['overall_balanced_acc']:.3f}\")\n",
    "            print(f\"   Overall ROC AUC: {best_metrics['overall_roc_auc']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ Results saved to: {self.output_folder}\")\n",
    "        \n",
    "        return {\n",
    "            'final_results': final_results,\n",
    "            'best_classifier': best_name,\n",
    "            'best_metrics': best_metrics,\n",
    "            'samples_data': samples_data,\n",
    "            'cv_results': cv_results,\n",
    "            'all_predictions': all_predictions,\n",
    "            'n_channels': n_channels,\n",
    "            'n_patients': n_patients\n",
    "        }\n",
    "    \n",
    "    def _save_channel_level_results(self, final_results, best_name, normalization_method, cv_results):\n",
    "        \"\"\"ä¿å­˜channel-levelç»“æœ\"\"\"\n",
    "        \n",
    "        # ä¿å­˜åˆ†ç±»ç»“æœ\n",
    "        if final_results:\n",
    "            summary_data = []\n",
    "            for clf_name, metrics in final_results.items():\n",
    "                summary_data.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'CV_F1_Mean': metrics['f1_score_mean'],\n",
    "                    'CV_F1_Std': metrics['f1_score_std'],\n",
    "                    'Overall_F1': metrics['overall_f1'],\n",
    "                    'Overall_Balanced_Acc': metrics['overall_balanced_acc'],\n",
    "                    'Overall_ROC_AUC': metrics['overall_roc_auc'],\n",
    "                    'Normalization_Method': normalization_method,\n",
    "                    'Analysis_Level': 'channel'\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_csv(\n",
    "                os.path.join(self.output_folder, f'channel_level_classification_summary_{normalization_method}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # ä¿å­˜patient-levelè¯¦ç»†ç»“æœ\n",
    "        if cv_results:\n",
    "            patient_results = []\n",
    "            for clf_name, folds in cv_results.items():\n",
    "                for fold in folds:\n",
    "                    patient_results.append({\n",
    "                        'Classifier': clf_name,\n",
    "                        'Patient': fold['test_patient'],\n",
    "                        'Fold': fold['fold'],\n",
    "                        'N_Channels': fold['n_test_channels'],\n",
    "                        'N_Samples': fold['n_test_samples'],\n",
    "                        'Accuracy': fold['accuracy'],\n",
    "                        'F1_Score': fold['f1_score'],\n",
    "                        'Precision': fold['precision'],\n",
    "                        'Recall': fold['recall'],\n",
    "                        'Balanced_Accuracy': fold['balanced_accuracy'],\n",
    "                        'ROC_AUC': fold['roc_auc']\n",
    "                    })\n",
    "            \n",
    "            patient_df = pd.DataFrame(patient_results)\n",
    "            patient_df.to_csv(\n",
    "                os.path.join(self.output_folder, f'patient_level_channel_results_{normalization_method}.csv'),\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        print(f\"  ğŸ’¾ Channel-level results saved\")\n",
    "\n",
    "\n",
    "def compare_windowing_strategies(processed_folder, output_base='comparison_results'):\n",
    "    \"\"\"æ¯”è¾ƒä¸åŒçª—å£ç­–ç•¥çš„æ€§èƒ½\"\"\"\n",
    "    \n",
    "    strategies = [\n",
    "        {'use_windowing': True, 'window_size_ms': 10000, 'step_size_ms': 5000, 'name': 'window_1000ms'},\n",
    "        {'use_windowing': True, 'window_size_ms': 5000, 'step_size_ms': 2500, 'name': 'window_500ms'},\n",
    "        {'use_windowing': True, 'window_size_ms': 2500, 'step_size_ms': 1250, 'name': 'window_250ms'}\n",
    "    ]\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        strategy_name = strategy.pop('name')\n",
    "        output_folder = f\"{output_base}_{strategy_name}\"\n",
    "        \n",
    "        print(f\"\\nStrategy Testing: {strategy_name}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        classifier = MultiPatientClassifier(processed_folder, output_folder)\n",
    "        results = classifier.run_complete_analysis(**strategy)\n",
    "        \n",
    "        if results:\n",
    "            comparison_results[strategy_name] = {\n",
    "                'best_classifier': results['best_classifier'],\n",
    "                'best_f1': results['best_metrics']['overall_f1'],\n",
    "                'best_balanced_acc': results['best_metrics']['overall_balanced_acc'],\n",
    "                'n_features': results['data_summary']['n_features'],\n",
    "                'total_samples': results['data_summary']['total_samples']\n",
    "            }\n",
    "    \n",
    "    # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š\n",
    "    if comparison_results:\n",
    "        comparison_df = pd.DataFrame(comparison_results).T\n",
    "        comparison_df.to_csv(os.path.join(output_base, 'strategy_comparison.csv'))\n",
    "        \n",
    "        print(f\"\\nç­–ç•¥å¯¹æ¯”ç»“æœ:\")\n",
    "        print(comparison_df.to_string())\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³ç­–ç•¥\n",
    "        best_strategy = comparison_df['best_f1'].idxmax()\n",
    "        print(f\"\\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy}\")\n",
    "        print(f\"   F1åˆ†æ•°: {comparison_df.loc[best_strategy, 'best_f1']:.3f}\")\n",
    "    \n",
    "    return comparison_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6965b16e6e1782a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# è®¾ç½®è·¯å¾„\n",
    "processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_test\"\n",
    "output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_alt\"\n",
    "\n",
    "print(\"å¤šPatient Grey/White Matteråˆ†ç±»ç³»ç»Ÿ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åˆ›å»ºåˆ†ç±»å™¨å®ä¾‹\n",
    "classifier = MultiPatientClassifier(\n",
    "    processed_folder=processed_folder,\n",
    "    output_folder=output_folder\n",
    ")\n",
    "\n",
    "# è¿è¡Œå®Œæ•´åˆ†æ\n",
    "results = classifier.run_complete_channel_level_analysis(\n",
    "    use_windowing=True,\n",
    "    window_size_ms=10000,\n",
    "    step_size_ms=5000,\n",
    "    max_windows_per_channel=200,\n",
    "    normalization_method='robust'\n",
    ")\n",
    "\n",
    "classifier._create_patient_feature_visualization(samples_data=results['samples_data'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a120d07483055b14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\"\n",
    "output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results\"\n",
    "\n",
    "results_2 = compare_windowing_strategies(processed_folder, output_base=r'D:\\BlcRepo\\LabCode\\SeizureProp\\result\\gw_comparison_results')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aac45254a5c1905a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PatientFeatureVisualizer:\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–æ¯ä¸ªæ‚£è€…çš„ç‰¹å¾åˆ†å¸ƒï¼Œç”¨äºåˆ†æä¸ºä»€ä¹ˆgrey/white matteråˆ†ç±»æ€§èƒ½ä¸å¥½\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_folder, output_folder='feature_visualization'):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å¯è§†åŒ–å™¨\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        processed_folder : str\n",
    "            åŒ…å«é¢„å¤„ç†pklæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        output_folder : str\n",
    "            å¯è§†åŒ–ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "        \"\"\"\n",
    "        self.processed_folder = processed_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.patients_data = {}\n",
    "        self.combined_features = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # è®¾ç½®é¢œè‰²è°ƒè‰²æ¿\n",
    "        self.patient_colors = plt.cm.Set3(np.linspace(0, 1, 20))  # æ”¯æŒæœ€å¤š20ä¸ªæ‚£è€…\n",
    "        \n",
    "        print(f\"Initializing Patient Feature Visualizer\")\n",
    "        print(f\"Processed folder: {processed_folder}\")\n",
    "        print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    def load_and_extract_features(self):\n",
    "        \"\"\"åŠ è½½æ‰€æœ‰æ‚£è€…æ•°æ®å¹¶æå–ç‰¹å¾\"\"\"\n",
    "        \n",
    "        pkl_files = glob.glob(os.path.join(self.processed_folder, \"P*_processed.pkl\"))\n",
    "        print(f\"\\nFound {len(pkl_files)} patient files\")\n",
    "        \n",
    "        all_patient_features = []\n",
    "        \n",
    "        for pkl_file in pkl_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                pid = data['patient_id']\n",
    "                self.patients_data[pid] = data\n",
    "                \n",
    "                # æå–ç‰¹å¾ï¼ˆé‡ç”¨ä¹‹å‰çš„ä»£ç é€»è¾‘ï¼‰\n",
    "                patient_features = self._extract_patient_features(pid, data)\n",
    "                \n",
    "                if patient_features is not None:\n",
    "                    all_patient_features.append(patient_features)\n",
    "                    print(f\"âœ“ {pid}: {patient_features['n_samples']} samples\")\n",
    "                else:\n",
    "                    print(f\"âœ— {pid}: Feature extraction failed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Failed to load {os.path.basename(pkl_file)}: {e}\")\n",
    "        \n",
    "        # åˆå¹¶æ‰€æœ‰æ‚£è€…çš„ç‰¹å¾\n",
    "        if all_patient_features:\n",
    "            self._combine_all_features(all_patient_features)\n",
    "            print(f\"\\nSuccessfully processed {len(all_patient_features)} patients\")\n",
    "            print(f\"Total samples: {len(self.combined_features)}\")\n",
    "            print(f\"Total features: {len(self.feature_names)}\")\n",
    "        else:\n",
    "            print(\"No valid patient features extracted\")\n",
    "        \n",
    "        return len(all_patient_features) > 0\n",
    "    \n",
    "    def _extract_patient_features(self, patient_id, patient_data):\n",
    "        \"\"\"ä¸ºå•ä¸ªæ‚£è€…æå–ç‰¹å¾ï¼ˆä»MultiPatientClassifieré€‚é…ï¼‰\"\"\"\n",
    "        \n",
    "        matter_data = patient_data['matter_data']\n",
    "        recordings = patient_data['recordings']\n",
    "        \n",
    "        # æå–ç”µæåˆ†ç±»\n",
    "        try:\n",
    "            grey_indices, white_indices, classification_info = self._extract_electrode_classification(matter_data)\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— {patient_id} electrode classification failed: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if len(grey_indices) == 0 or len(white_indices) == 0:\n",
    "            print(f\"  âœ— {patient_id} missing grey or white matter electrodes\")\n",
    "            return None\n",
    "        \n",
    "        # åˆå¹¶æ‰€æœ‰recordingsçš„æ•°æ®\n",
    "        all_grey_data = []\n",
    "        all_white_data = []\n",
    "        fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "        \n",
    "        for recording in recordings:\n",
    "            neural_data = recording['neural_data_processed']\n",
    "            grey_data = neural_data[:, grey_indices]\n",
    "            white_data = neural_data[:, white_indices]\n",
    "            all_grey_data.append(grey_data)\n",
    "            all_white_data.append(white_data)\n",
    "        \n",
    "        combined_grey = np.vstack(all_grey_data) if all_grey_data else np.array([])\n",
    "        combined_white = np.vstack(all_white_data) if all_white_data else np.array([])\n",
    "        \n",
    "        # åˆ›å»ºçª—å£æ ·æœ¬\n",
    "        all_samples = []\n",
    "        \n",
    "        # Grey matter samples\n",
    "        grey_samples = self._create_windowed_samples(\n",
    "            combined_grey, grey_indices, 1, patient_id, fs=fs\n",
    "        )\n",
    "        \n",
    "        # White matter samples  \n",
    "        white_samples = self._create_windowed_samples(\n",
    "            combined_white, white_indices, 0, patient_id, fs=fs\n",
    "        )\n",
    "        \n",
    "        all_samples = grey_samples + white_samples\n",
    "        \n",
    "        if len(all_samples) == 0:\n",
    "            return None\n",
    "        \n",
    "        # è½¬æ¢ä¸ºDataFrame\n",
    "        features_df = pd.DataFrame(all_samples)\n",
    "        \n",
    "        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n",
    "        meta_columns = ['patient_id', 'electrode_idx', 'channel_idx', 'window_start', 'label']\n",
    "        feature_columns = [col for col in features_df.columns if col not in meta_columns]\n",
    "        \n",
    "        # æ•°æ®æ¸…ç†\n",
    "        features_df[feature_columns] = features_df[feature_columns].fillna(0)\n",
    "        features_df[feature_columns] = features_df[feature_columns].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        return {\n",
    "            'patient_id': patient_id,\n",
    "            'features': features_df[feature_columns],\n",
    "            'labels': features_df['label'].values,\n",
    "            'meta': features_df[meta_columns],\n",
    "            'n_samples': len(features_df),\n",
    "            'n_grey_samples': np.sum(features_df['label'] == 1),\n",
    "            'n_white_samples': np.sum(features_df['label'] == 0)\n",
    "        }\n",
    "    \n",
    "    def _extract_electrode_classification(self, matter_data):\n",
    "        \"\"\"æå–ç”µæåˆ†ç±»ï¼ˆä»MultiPatientClassifieré€‚é…ï¼‰\"\"\"\n",
    "        \n",
    "        matter_columns = ['MatterType', 'matter', 'Matter', 'mattertype', 'tissue_type', 'type']\n",
    "        matter_col = None\n",
    "        \n",
    "        for col in matter_columns:\n",
    "            if col in matter_data.columns:\n",
    "                matter_col = col\n",
    "                break\n",
    "        \n",
    "        if matter_col is None:\n",
    "            raise ValueError(f\"Matter type column not found. Available: {matter_data.columns.tolist()}\")\n",
    "        \n",
    "        matter_values = matter_data[matter_col].astype(str).str.lower()\n",
    "        \n",
    "        grey_mask = matter_values.isin(['G', 'g', 'Grey', 'grey', 'Gray', 'gray'])\n",
    "        white_mask = matter_values.isin(['W', 'w', 'White', 'white'])\n",
    "        \n",
    "        # å¦‚æœG/Wæ ¼å¼æ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ…å«åŒ¹é…\n",
    "        if np.sum(grey_mask) == 0 or np.sum(white_mask) == 0:\n",
    "            matter_values_lower = matter_values.str.lower()\n",
    "            \n",
    "            if np.sum(grey_mask) == 0:\n",
    "                grey_patterns = ['grey', 'gray', 'cortex', 'cortical']\n",
    "                grey_mask = matter_values_lower.str.contains('|'.join(grey_patterns), na=False, case=False)\n",
    "            \n",
    "            if np.sum(white_mask) == 0:\n",
    "                white_patterns = ['white']\n",
    "                white_mask = matter_values_lower.str.contains('|'.join(white_patterns), na=False, case=False)\n",
    "        \n",
    "        grey_indices = matter_data.index[grey_mask].tolist()\n",
    "        white_indices = matter_data.index[white_mask].tolist()\n",
    "        \n",
    "        classification_info = {\n",
    "            'matter_column': matter_col,\n",
    "            'total_electrodes': len(matter_data),\n",
    "            'grey_electrodes': len(grey_indices),\n",
    "            'white_electrodes': len(white_indices),\n",
    "            'grey_indices': grey_indices,\n",
    "            'white_indices': white_indices\n",
    "        }\n",
    "        \n",
    "        return grey_indices, white_indices, classification_info\n",
    "    \n",
    "    def _create_windowed_samples(self, data, electrode_indices, label, patient_id, \n",
    "                                window_size_ms=50000, step_size_ms=25000, max_windows_per_channel=200, fs=512):\n",
    "        \"\"\"åˆ›å»ºæ—¶é—´çª—å£æ ·æœ¬\"\"\"\n",
    "        \n",
    "        samples = []\n",
    "        window_samples = int(window_size_ms * fs / 1000)\n",
    "        step_samples = int(step_size_ms * fs / 1000)\n",
    "        \n",
    "        n_time, n_channels = data.shape\n",
    "        \n",
    "        for ch_idx, electrode_idx in enumerate(electrode_indices):\n",
    "            channel_data = data[:, ch_idx]\n",
    "            channel_windows = []\n",
    "            \n",
    "            for start in range(0, n_time - window_samples + 1, step_samples):\n",
    "                end = start + window_samples\n",
    "                window_data = channel_data[start:end]\n",
    "                \n",
    "                features = self._extract_signal_features(window_data, fs)\n",
    "                if features is not None:\n",
    "                    features['patient_id'] = patient_id\n",
    "                    features['electrode_idx'] = electrode_idx\n",
    "                    features['channel_idx'] = ch_idx\n",
    "                    features['window_start'] = start\n",
    "                    features['label'] = label\n",
    "                    \n",
    "                    channel_windows.append(features)\n",
    "                \n",
    "                if len(channel_windows) >= max_windows_per_channel:\n",
    "                    break\n",
    "            \n",
    "            # éšæœºé‡‡æ ·é™åˆ¶çª—å£æ•°\n",
    "            if len(channel_windows) > max_windows_per_channel:\n",
    "                indices = np.random.choice(len(channel_windows), max_windows_per_channel, replace=False)\n",
    "                channel_windows = [channel_windows[i] for i in sorted(indices)]\n",
    "            \n",
    "            samples.extend(channel_windows)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _extract_signal_features(self, signal, fs):\n",
    "        \"\"\"æå–å•ä¸ªä¿¡å·çš„ç‰¹å¾ï¼ˆä»MultiPatientClassifieré€‚é…ï¼‰\"\"\"\n",
    "        \n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # æ—¶åŸŸç‰¹å¾\n",
    "        features['mean'] = np.mean(signal)\n",
    "        features['std'] = np.std(signal)\n",
    "        features['var'] = np.var(signal)\n",
    "        features['median'] = np.median(signal)\n",
    "        features['mad'] = np.median(np.abs(signal - np.median(signal)))\n",
    "        features['range'] = np.ptp(signal)\n",
    "        features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        \n",
    "        # ç»Ÿè®¡çŸ©\n",
    "        try:\n",
    "            features['skewness'] = stats.skew(signal)\n",
    "            features['kurtosis'] = stats.kurtosis(signal)\n",
    "        except:\n",
    "            features['skewness'] = 0\n",
    "            features['kurtosis'] = 0\n",
    "        \n",
    "        # ä¿¡å·å¤æ‚åº¦\n",
    "        features['zero_crossings'] = np.sum(np.diff(np.signbit(signal)))\n",
    "        features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "        features['area'] = np.sum(np.abs(signal))\n",
    "        features['energy'] = np.sum(signal**2)\n",
    "        \n",
    "        # é¢‘åŸŸç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…é”™è¯¯ï¼‰\n",
    "        try:\n",
    "            from scipy.fftpack import fft\n",
    "            n_fft = len(signal)\n",
    "            windowed_signal = signal * np.hamming(n_fft)\n",
    "            fft_vals = fft(windowed_signal)\n",
    "            fft_mag = np.abs(fft_vals[:n_fft//2])\n",
    "            freqs = np.fft.fftfreq(n_fft, 1/fs)[:n_fft//2]\n",
    "            \n",
    "            # é¢‘å¸¦åŠŸç‡\n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 100),\n",
    "                'high_gamma': (100, min(200, fs/2))\n",
    "            }\n",
    "            \n",
    "            total_power = np.sum(fft_mag**2)\n",
    "            for band_name, (low, high) in bands.items():\n",
    "                band_mask = (freqs >= low) & (freqs <= high)\n",
    "                if np.any(band_mask):\n",
    "                    band_power = np.sum(fft_mag[band_mask]**2)\n",
    "                    features[f'power_{band_name}'] = band_power\n",
    "                    features[f'rel_power_{band_name}'] = band_power / total_power if total_power > 0 else 0\n",
    "                else:\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                    features[f'rel_power_{band_name}'] = 0\n",
    "            \n",
    "            features['total_power'] = total_power\n",
    "            \n",
    "        except Exception as e:\n",
    "            # å¦‚æœé¢‘åŸŸåˆ†æå¤±è´¥ï¼Œè®¾ç½®é»˜è®¤å€¼\n",
    "            bands = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high_gamma']\n",
    "            for band in bands:\n",
    "                features[f'power_{band}'] = 0\n",
    "                features[f'rel_power_{band}'] = 0\n",
    "            features['total_power'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _combine_all_features(self, all_patient_features):\n",
    "        \"\"\"åˆå¹¶æ‰€æœ‰æ‚£è€…çš„ç‰¹å¾\"\"\"\n",
    "        \n",
    "        # æ‰¾åˆ°å…±åŒçš„ç‰¹å¾åˆ—\n",
    "        all_feature_columns = []\n",
    "        for patient_data in all_patient_features:\n",
    "            all_feature_columns.append(set(patient_data['features'].columns))\n",
    "        \n",
    "        common_features = set.intersection(*all_feature_columns)\n",
    "        self.feature_names = list(common_features)\n",
    "        \n",
    "        # åˆå¹¶æ•°æ®\n",
    "        combined_data = []\n",
    "        for patient_data in all_patient_features:\n",
    "            patient_id = patient_data['patient_id']\n",
    "            features = patient_data['features'][self.feature_names]\n",
    "            labels = patient_data['labels']\n",
    "            \n",
    "            # æ·»åŠ æ‚£è€…IDåˆ°æ¯ä¸€è¡Œ\n",
    "            for i in range(len(features)):\n",
    "                row = {\n",
    "                    'patient_id': patient_id,\n",
    "                    'label': labels[i],\n",
    "                    'matter_type': 'Grey' if labels[i] == 1 else 'White'\n",
    "                }\n",
    "                # æ·»åŠ ç‰¹å¾å€¼\n",
    "                for feature_name in self.feature_names:\n",
    "                    row[feature_name] = features.iloc[i][feature_name]\n",
    "                \n",
    "                combined_data.append(row)\n",
    "        \n",
    "        self.combined_features = pd.DataFrame(combined_data)\n",
    "        \n",
    "        # æ•°æ®æ¸…ç†\n",
    "        feature_cols = [col for col in self.combined_features.columns \n",
    "                       if col not in ['patient_id', 'label', 'matter_type']]\n",
    "        self.combined_features[feature_cols] = self.combined_features[feature_cols].fillna(0)\n",
    "        self.combined_features[feature_cols] = self.combined_features[feature_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    def create_individual_feature_plots(self, n_features_per_page=9):\n",
    "        \"\"\"ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºæ‚£è€…åˆ†å¸ƒçš„æ•£ç‚¹å›¾\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating individual feature distribution plots...\")\n",
    "        \n",
    "        # è·å–æ‚£è€…åˆ—è¡¨å’Œé¢œè‰²æ˜ å°„\n",
    "        patients = self.combined_features['patient_id'].unique()\n",
    "        patient_color_map = {patient: self.patient_colors[i % len(self.patient_colors)] \n",
    "                           for i, patient in enumerate(patients)}\n",
    "        \n",
    "        # åˆ†é¡µå¤„ç†ç‰¹å¾\n",
    "        n_pages = (len(self.feature_names) + n_features_per_page - 1) // n_features_per_page\n",
    "        \n",
    "        for page in range(n_pages):\n",
    "            start_idx = page * n_features_per_page\n",
    "            end_idx = min(start_idx + n_features_per_page, len(self.feature_names))\n",
    "            page_features = self.feature_names[start_idx:end_idx]\n",
    "            \n",
    "            # è®¡ç®—å­å›¾å¸ƒå±€\n",
    "            n_features_in_page = len(page_features)\n",
    "            n_cols = 3\n",
    "            n_rows = (n_features_in_page + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "            if n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            if n_features_in_page == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            \n",
    "            fig.suptitle(f'Feature Distribution by Patient - Page {page+1}/{n_pages}', \n",
    "                        fontsize=16, y=0.98)\n",
    "            \n",
    "            for i, feature_name in enumerate(page_features):\n",
    "                row = i // n_cols\n",
    "                col = i % n_cols\n",
    "                ax = axes[row, col]\n",
    "                \n",
    "                # ä¸ºæ¯ä¸ªæ‚£è€…åˆ›å»ºæ•£ç‚¹å›¾\n",
    "                for j, patient in enumerate(patients):\n",
    "                    patient_data = self.combined_features[self.combined_features['patient_id'] == patient]\n",
    "                    \n",
    "                    # Grey matter points\n",
    "                    grey_data = patient_data[patient_data['matter_type'] == 'Grey']\n",
    "                    if len(grey_data) > 0:\n",
    "                        y_grey = np.random.normal(1, 0.1, len(grey_data))  # æ·»åŠ å‚ç›´æŠ–åŠ¨\n",
    "                        ax.scatter(grey_data[feature_name], y_grey, \n",
    "                                 c=[patient_color_map[patient]], alpha=0.6, s=20,\n",
    "                                 label=f'{patient}_Grey' if j == 0 else \"\")\n",
    "                    \n",
    "                    # White matter points\n",
    "                    white_data = patient_data[patient_data['matter_type'] == 'White']\n",
    "                    if len(white_data) > 0:\n",
    "                        y_white = np.random.normal(0, 0.1, len(white_data))  # æ·»åŠ å‚ç›´æŠ–åŠ¨\n",
    "                        ax.scatter(white_data[feature_name], y_white, \n",
    "                                 c=[patient_color_map[patient]], alpha=0.6, s=20, \n",
    "                                 marker='s', label=f'{patient}_White' if j == 0 else \"\")\n",
    "                \n",
    "                ax.set_xlabel(feature_name)\n",
    "                ax.set_ylabel('Matter Type')\n",
    "                ax.set_yticks([0, 1])\n",
    "                ax.set_yticklabels(['White', 'Grey'])\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_title(f'{feature_name}', fontsize=10)\n",
    "                \n",
    "                # æ·»åŠ ç®±çº¿å›¾å±‚è¦†ç›–æ˜¾ç¤ºåˆ†å¸ƒ\n",
    "                grey_values = self.combined_features[self.combined_features['matter_type'] == 'Grey'][feature_name]\n",
    "                white_values = self.combined_features[self.combined_features['matter_type'] == 'White'][feature_name]\n",
    "                \n",
    "                if len(grey_values) > 0 and len(white_values) > 0:\n",
    "                    # è®¡ç®—ç®€å•çš„åˆ†å¸ƒç»Ÿè®¡\n",
    "                    grey_median = np.median(grey_values)\n",
    "                    white_median = np.median(white_values)\n",
    "                    \n",
    "                    ax.axvline(grey_median, ymax=1.25, color='red', linestyle='--', alpha=0.7, linewidth=2)\n",
    "                    ax.axvline(white_median, ymax=0.25, color='blue', linestyle='--', alpha=0.7, linewidth=2)\n",
    "            \n",
    "            # éšè—ç©ºçš„å­å›¾\n",
    "            for i in range(n_features_in_page, n_rows * n_cols):\n",
    "                row = i // n_cols\n",
    "                col = i % n_cols\n",
    "                axes[row, col].set_visible(False)\n",
    "            \n",
    "            # æ·»åŠ æ‚£è€…é¢œè‰²å›¾ä¾‹\n",
    "            handles = []\n",
    "            labels = []\n",
    "            for patient in patients:\n",
    "                handles.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                        markerfacecolor=patient_color_map[patient], markersize=8))\n",
    "                labels.append(patient)\n",
    "            \n",
    "            fig.legend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.02), \n",
    "                      ncol=min(6, len(patients)), fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(bottom=0.15, top=0.92)\n",
    "            plt.savefig(os.path.join(self.output_folder, f'feature_distributions_page_{page+1}.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  âœ“ Page {page+1}/{n_pages} saved\")\n",
    "    \n",
    "    def create_feature_summary_statistics(self):\n",
    "        \"\"\"åˆ›å»ºç‰¹å¾ç»Ÿè®¡æ±‡æ€»è¡¨\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating feature summary statistics...\")\n",
    "        \n",
    "        summary_stats = []\n",
    "        \n",
    "        for feature_name in self.feature_names:\n",
    "            # æ•´ä½“ç»Ÿè®¡\n",
    "            overall_mean = self.combined_features[feature_name].mean()\n",
    "            overall_std = self.combined_features[feature_name].std()\n",
    "            \n",
    "            # æŒ‰matter typeåˆ†ç»„ç»Ÿè®¡\n",
    "            grey_data = self.combined_features[self.combined_features['matter_type'] == 'Grey'][feature_name]\n",
    "            white_data = self.combined_features[self.combined_features['matter_type'] == 'White'][feature_name]\n",
    "            \n",
    "            grey_mean = grey_data.mean()\n",
    "            grey_std = grey_data.std()\n",
    "            white_mean = white_data.mean()\n",
    "            white_std = white_data.std()\n",
    "            \n",
    "            # ç»Ÿè®¡æ£€éªŒ\n",
    "            try:\n",
    "                t_stat, p_value = stats.ttest_ind(grey_data, white_data)\n",
    "            except:\n",
    "                t_stat, p_value = 0, 1\n",
    "            \n",
    "            # æ•ˆåº”é‡ (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(grey_data) - 1) * grey_std**2 + \n",
    "                                 (len(white_data) - 1) * white_std**2) / \n",
    "                                (len(grey_data) + len(white_data) - 2))\n",
    "            cohens_d = (grey_mean - white_mean) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            # æŒ‰æ‚£è€…åˆ†ç»„çš„å˜å¼‚ç³»æ•°\n",
    "            patient_means = []\n",
    "            for patient in self.combined_features['patient_id'].unique():\n",
    "                patient_data = self.combined_features[self.combined_features['patient_id'] == patient][feature_name]\n",
    "                if len(patient_data) > 0:\n",
    "                    patient_means.append(patient_data.mean())\n",
    "            \n",
    "            between_patient_cv = np.std(patient_means) / np.mean(patient_means) if len(patient_means) > 0 and np.mean(patient_means) != 0 else 0\n",
    "            \n",
    "            summary_stats.append({\n",
    "                'feature_name': feature_name,\n",
    "                'overall_mean': overall_mean,\n",
    "                'overall_std': overall_std,\n",
    "                'grey_mean': grey_mean,\n",
    "                'grey_std': grey_std,\n",
    "                'white_mean': white_mean,\n",
    "                'white_std': white_std,\n",
    "                'mean_difference': grey_mean - white_mean,\n",
    "                'cohens_d': cohens_d,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'between_patient_cv': between_patient_cv\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df = summary_df.sort_values('p_value')  # æŒ‰på€¼æ’åº\n",
    "        \n",
    "        # ä¿å­˜ç»Ÿè®¡ç»“æœ\n",
    "        summary_df.to_csv(os.path.join(self.output_folder, 'feature_statistics.csv'), index=False)\n",
    "        \n",
    "        # åˆ›å»ºç»Ÿè®¡å¯è§†åŒ–\n",
    "        self._plot_feature_statistics(summary_df)\n",
    "        \n",
    "        print(f\"  âœ“ Feature statistics saved\")\n",
    "        return summary_df\n",
    "    \n",
    "    def _plot_feature_statistics(self, summary_df):\n",
    "        \"\"\"å¯è§†åŒ–ç‰¹å¾ç»Ÿè®¡ç»“æœ\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Effect size (Cohen's d) åˆ†å¸ƒ\n",
    "        axes[0, 0].hist(summary_df['cohens_d'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[0, 0].set_xlabel(\"Cohen's d\")\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Effect Size Distribution')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. P-value åˆ†å¸ƒ\n",
    "        axes[0, 1].hist(summary_df['p_value'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(0.05, color='red', linestyle='--', alpha=0.7, label='p=0.05')\n",
    "        axes[0, 1].set_xlabel('P-value')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('P-value Distribution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. æ‚£è€…é—´å˜å¼‚ç³»æ•°\n",
    "        axes[1, 0].hist(summary_df['between_patient_cv'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Between-Patient CV')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Between-Patient Variability')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Effect size vs P-value\n",
    "        significant = summary_df['significant']\n",
    "        axes[1, 1].scatter(summary_df.loc[~significant, 'cohens_d'], \n",
    "                          summary_df.loc[~significant, 'p_value'], \n",
    "                          alpha=0.6, label='Non-significant', color='gray')\n",
    "        axes[1, 1].scatter(summary_df.loc[significant, 'cohens_d'], \n",
    "                          summary_df.loc[significant, 'p_value'], \n",
    "                          alpha=0.8, label='Significant', color='red')\n",
    "        axes[1, 1].axhline(0.05, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1, 1].axvline(0, color='gray', linestyle='-', alpha=0.5)\n",
    "        axes[1, 1].set_xlabel(\"Cohen's d\")\n",
    "        axes[1, 1].set_ylabel('P-value')\n",
    "        axes[1, 1].set_title('Effect Size vs Statistical Significance')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'feature_statistics_plots.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def create_patient_comparison_plots(self):\n",
    "        \"\"\"åˆ›å»ºæ‚£è€…é—´æ¯”è¾ƒå›¾\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating patient comparison plots...\")\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªæ‚£è€…çš„ç‰¹å¾å‡å€¼\n",
    "        patient_means = []\n",
    "        patients = self.combined_features['patient_id'].unique()\n",
    "        \n",
    "        for patient in patients:\n",
    "            patient_data = self.combined_features[self.combined_features['patient_id'] == patient]\n",
    "            \n",
    "            # åˆ†åˆ«è®¡ç®—greyå’Œwhite matterçš„å‡å€¼\n",
    "            grey_means = patient_data[patient_data['matter_type'] == 'Grey'][self.feature_names].mean()\n",
    "            white_means = patient_data[patient_data['matter_type'] == 'White'][self.feature_names].mean()\n",
    "            \n",
    "            patient_means.append({\n",
    "                'patient_id': patient,\n",
    "                'matter_type': 'Grey',\n",
    "                **grey_means.to_dict()\n",
    "            })\n",
    "            \n",
    "            patient_means.append({\n",
    "                'patient_id': patient,\n",
    "                'matter_type': 'White', \n",
    "                **white_means.to_dict()\n",
    "            })\n",
    "        \n",
    "        patient_means_df = pd.DataFrame(patient_means)\n",
    "        \n",
    "        # 1. æ‚£è€…ç›¸ä¼¼æ€§çƒ­å›¾\n",
    "        self._create_patient_similarity_heatmap(patient_means_df)\n",
    "        \n",
    "        # 2. PCAåˆ†æ\n",
    "        self._create_pca_analysis()\n",
    "        \n",
    "        # 3. æœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾\n",
    "        self._create_discriminative_features_plot()\n",
    "        \n",
    "        print(f\"  âœ“ Patient comparison plots saved\")\n",
    "    \n",
    "    def _create_patient_similarity_heatmap(self, patient_means_df):\n",
    "        \"\"\"åˆ›å»ºæ‚£è€…ç›¸ä¼¼æ€§çƒ­å›¾\"\"\"\n",
    "        \n",
    "        # åªä½¿ç”¨grey matteræ•°æ®è¿›è¡Œæ‚£è€…ç›¸ä¼¼æ€§åˆ†æ\n",
    "        grey_means = patient_means_df[patient_means_df['matter_type'] == 'Grey']\n",
    "        \n",
    "        # è®¡ç®—æ‚£è€…é—´çš„ç›¸å…³æ€§\n",
    "        patients = grey_means['patient_id'].values\n",
    "        feature_matrix = grey_means[self.feature_names].values\n",
    "        \n",
    "        # æ ‡å‡†åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "        \n",
    "        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ\n",
    "        correlation_matrix = np.corrcoef(feature_matrix_scaled)\n",
    "        \n",
    "        # åˆ›å»ºçƒ­å›¾\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   xticklabels=patients, yticklabels=patients, fmt='.2f')\n",
    "        plt.title('Patient Similarity (Grey Matter Features)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'patient_similarity_heatmap.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_pca_analysis(self):\n",
    "        \"\"\"åˆ›å»ºPCAåˆ†æå›¾\"\"\"\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        feature_data = self.combined_features[self.feature_names]\n",
    "        labels = self.combined_features['matter_type']\n",
    "        patients = self.combined_features['patient_id']\n",
    "        \n",
    "        # æ ‡å‡†åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        feature_data_scaled = scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # PCA\n",
    "        pca = PCA(n_components=10)\n",
    "        pca_result = pca.fit_transform(feature_data_scaled)\n",
    "        \n",
    "        # åˆ›å»ºå›¾è¡¨\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 1. æŒ‰matter typeç€è‰²\n",
    "        unique_patients = patients.unique()\n",
    "        patient_color_map = {patient: self.patient_colors[i % len(self.patient_colors)] \n",
    "                           for i, patient in enumerate(unique_patients)}\n",
    "        \n",
    "        for matter_type in ['Grey', 'White']:\n",
    "            mask = labels == matter_type\n",
    "            marker = 'o' if matter_type == 'Grey' else 's'\n",
    "            axes[0].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                          alpha=0.6, label=matter_type, s=30, marker=marker)\n",
    "        \n",
    "        axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        axes[0].set_title('PCA: Grey vs White Matter')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æŒ‰æ‚£è€…ç€è‰²\n",
    "        for patient in unique_patients:\n",
    "            mask = patients == patient\n",
    "            if np.any(mask):\n",
    "                axes[1].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                              c=[patient_color_map[patient]], alpha=0.6, label=patient, s=30)\n",
    "        \n",
    "        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        axes[1].set_title('PCA: By Patient')\n",
    "        axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'pca_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # ä¿å­˜ä¸»æˆåˆ†ä¿¡æ¯\n",
    "        component_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'PC1': pca.components_[0],\n",
    "            'PC2': pca.components_[1]\n",
    "        })\n",
    "        component_df = component_df.reindex(component_df['PC1'].abs().sort_values(ascending=False).index)\n",
    "        component_df.to_csv(os.path.join(self.output_folder, 'pca_components.csv'), index=False)\n",
    "    \n",
    "    def _create_discriminative_features_plot(self):\n",
    "        \"\"\"åˆ›å»ºæœ€å…·åŒºåˆ†æ€§ç‰¹å¾çš„å¯è§†åŒ–\"\"\"\n",
    "        \n",
    "        # è¯»å–ä¹‹å‰è®¡ç®—çš„ç‰¹å¾ç»Ÿè®¡\n",
    "        stats_file = os.path.join(self.output_folder, 'feature_statistics.csv')\n",
    "        if os.path.exists(stats_file):\n",
    "            stats_df = pd.read_csv(stats_file)\n",
    "        else:\n",
    "            stats_df = self.create_feature_summary_statistics()\n",
    "        \n",
    "        # é€‰æ‹©æœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾ï¼ˆæŒ‰Cohen's dæ’åºï¼‰\n",
    "        top_features = stats_df.nlargest(12, 'cohens_d')['feature_name'].tolist()\n",
    "        \n",
    "        # åˆ›å»ºç®±çº¿å›¾\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, feature in enumerate(top_features):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            grey_data = self.combined_features[self.combined_features['matter_type'] == 'Grey'][feature]\n",
    "            white_data = self.combined_features[self.combined_features['matter_type'] == 'White'][feature]\n",
    "            \n",
    "            # ç®±çº¿å›¾\n",
    "            box_data = [white_data, grey_data]\n",
    "            bp = ax.boxplot(box_data, labels=['White', 'Grey'], patch_artist=True)\n",
    "            bp['boxes'][0].set_facecolor('lightblue')\n",
    "            bp['boxes'][1].set_facecolor('lightcoral')\n",
    "            \n",
    "            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
    "            stat_info = stats_df[stats_df['feature_name'] == feature].iloc[0]\n",
    "            ax.set_title(f'{feature}\\nCohen\\'s d: {stat_info[\"cohens_d\"]:.3f}\\np: {stat_info[\"p_value\"]:.3e}',\n",
    "                        fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # éšè—ç©ºçš„å­å›¾\n",
    "        for i in range(len(top_features), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Most Discriminative Features (Top 12 by Cohen\\'s d)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'discriminative_features.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def create_patient_specific_analysis(self):\n",
    "        \"\"\"åˆ›å»ºæ‚£è€…ç‰¹å¼‚æ€§åˆ†æ\"\"\"\n",
    "        \n",
    "        if self.combined_features is None:\n",
    "            print(\"No features loaded. Run load_and_extract_features() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nCreating patient-specific analysis...\")\n",
    "        \n",
    "        patients = self.combined_features['patient_id'].unique()\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªæ‚£è€…çš„ç‰¹å¾åˆ†å¸ƒç‰¹æ€§\n",
    "        patient_analysis = []\n",
    "        \n",
    "        for patient in patients:\n",
    "            patient_data = self.combined_features[self.combined_features['patient_id'] == patient]\n",
    "            \n",
    "            grey_data = patient_data[patient_data['matter_type'] == 'Grey'][self.feature_names]\n",
    "            white_data = patient_data[patient_data['matter_type'] == 'White'][self.feature_names]\n",
    "            \n",
    "            # è®¡ç®—åˆ†ç¦»æ€§æŒ‡æ ‡\n",
    "            separability_scores = []\n",
    "            significant_features = 0\n",
    "            \n",
    "            for feature in self.feature_names:\n",
    "                if len(grey_data) > 5 and len(white_data) > 5:\n",
    "                    try:\n",
    "                        # tæ£€éªŒ\n",
    "                        t_stat, p_val = stats.ttest_ind(grey_data[feature], white_data[feature])\n",
    "                        \n",
    "                        # Cohen's d\n",
    "                        pooled_std = np.sqrt(((len(grey_data) - 1) * grey_data[feature].std()**2 + \n",
    "                                            (len(white_data) - 1) * white_data[feature].std()**2) / \n",
    "                                           (len(grey_data) + len(white_data) - 2))\n",
    "                        cohens_d = abs((grey_data[feature].mean() - white_data[feature].mean()) / pooled_std) if pooled_std > 0 else 0\n",
    "                        \n",
    "                        separability_scores.append(cohens_d)\n",
    "                        if p_val < 0.05:\n",
    "                            significant_features += 1\n",
    "                    except:\n",
    "                        separability_scores.append(0)\n",
    "                else:\n",
    "                    separability_scores.append(0)\n",
    "            \n",
    "            patient_analysis.append({\n",
    "                'patient_id': patient,\n",
    "                'n_grey_samples': len(grey_data),\n",
    "                'n_white_samples': len(white_data),\n",
    "                'mean_separability': np.mean(separability_scores),\n",
    "                'max_separability': np.max(separability_scores),\n",
    "                'significant_features': significant_features,\n",
    "                'separability_std': np.std(separability_scores)\n",
    "            })\n",
    "        \n",
    "        patient_analysis_df = pd.DataFrame(patient_analysis)\n",
    "        patient_analysis_df.to_csv(os.path.join(self.output_folder, 'patient_analysis.csv'), index=False)\n",
    "        \n",
    "        # å¯è§†åŒ–æ‚£è€…åˆ†æç»“æœ\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # 1. å¹³å‡åˆ†ç¦»æ€§\n",
    "        axes[0, 0].bar(patient_analysis_df['patient_id'], patient_analysis_df['mean_separability'])\n",
    "        axes[0, 0].set_xlabel('Patient ID')\n",
    "        axes[0, 0].set_ylabel('Mean Separability (Cohen\\'s d)')\n",
    "        axes[0, 0].set_title('Average Feature Separability by Patient')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. æ˜¾è‘—ç‰¹å¾æ•°é‡\n",
    "        axes[0, 1].bar(patient_analysis_df['patient_id'], patient_analysis_df['significant_features'])\n",
    "        axes[0, 1].set_xlabel('Patient ID')\n",
    "        axes[0, 1].set_ylabel('Number of Significant Features')\n",
    "        axes[0, 1].set_title('Significant Features by Patient (p<0.05)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. æ ·æœ¬æ•°é‡\n",
    "        x = np.arange(len(patient_analysis_df))\n",
    "        width = 0.35\n",
    "        axes[1, 0].bar(x - width/2, patient_analysis_df['n_grey_samples'], width, label='Grey', alpha=0.8)\n",
    "        axes[1, 0].bar(x + width/2, patient_analysis_df['n_white_samples'], width, label='White', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('Patient ID')\n",
    "        axes[1, 0].set_ylabel('Sample Count')\n",
    "        axes[1, 0].set_title('Sample Distribution by Patient')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(patient_analysis_df['patient_id'], rotation=45)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. åˆ†ç¦»æ€§vsæ ·æœ¬æ•°é‡\n",
    "        total_samples = patient_analysis_df['n_grey_samples'] + patient_analysis_df['n_white_samples']\n",
    "        axes[1, 1].scatter(total_samples, patient_analysis_df['mean_separability'], s=60, alpha=0.7)\n",
    "        for i, patient in enumerate(patient_analysis_df['patient_id']):\n",
    "            axes[1, 1].annotate(patient, (total_samples.iloc[i], patient_analysis_df['mean_separability'].iloc[i]),\n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        axes[1, 1].set_xlabel('Total Samples')\n",
    "        axes[1, 1].set_ylabel('Mean Separability')\n",
    "        axes[1, 1].set_title('Separability vs Sample Size')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'patient_specific_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  âœ“ Patient-specific analysis saved\")\n",
    "        return patient_analysis_df\n",
    "    \n",
    "    def run_complete_visualization(self):\n",
    "        \"\"\"è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–åˆ†æ\"\"\"\n",
    "        \n",
    "        print(f\"Starting Complete Patient Feature Visualization\")\n",
    "        print(f\"=\" * 60)\n",
    "        \n",
    "        # 1. åŠ è½½å’Œæå–ç‰¹å¾\n",
    "        if not self.load_and_extract_features():\n",
    "            print(\"âŒ Failed to load and extract features\")\n",
    "            return None\n",
    "        \n",
    "        # 2. åˆ›å»ºä¸ªåˆ«ç‰¹å¾åˆ†å¸ƒå›¾\n",
    "        self.create_individual_feature_plots()\n",
    "        \n",
    "        # 3. åˆ›å»ºç‰¹å¾ç»Ÿè®¡æ±‡æ€»\n",
    "        stats_df = self.create_feature_summary_statistics()\n",
    "        \n",
    "        # 4. åˆ›å»ºæ‚£è€…æ¯”è¾ƒå›¾\n",
    "        self.create_patient_comparison_plots()\n",
    "        \n",
    "        # 5. åˆ›å»ºæ‚£è€…ç‰¹å¼‚æ€§åˆ†æ\n",
    "        patient_analysis = self.create_patient_specific_analysis()\n",
    "        \n",
    "        # 6. åˆ›å»ºæ±‡æ€»æŠ¥å‘Š\n",
    "        self._create_summary_report(stats_df, patient_analysis)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Visualization Analysis Complete\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ğŸ“ Results saved to: {self.output_folder}\")\n",
    "        print(f\"   - feature_distributions_page_*.png: Individual feature distributions\")\n",
    "        print(f\"   - feature_statistics.csv: Feature statistics summary\")\n",
    "        print(f\"   - feature_statistics_plots.png: Statistical analysis plots\")\n",
    "        print(f\"   - patient_similarity_heatmap.png: Patient similarity analysis\")\n",
    "        print(f\"   - pca_analysis.png: PCA analysis\")\n",
    "        print(f\"   - discriminative_features.png: Most discriminative features\")\n",
    "        print(f\"   - patient_specific_analysis.png: Patient-specific analysis\")\n",
    "        print(f\"   - summary_report.txt: Text summary report\")\n",
    "        \n",
    "        return {\n",
    "            'feature_stats': stats_df,\n",
    "            'patient_analysis': patient_analysis,\n",
    "            'n_patients': len(self.combined_features['patient_id'].unique()),\n",
    "            'n_features': len(self.feature_names),\n",
    "            'total_samples': len(self.combined_features)\n",
    "        }\n",
    "    \n",
    "    def _create_summary_report(self, stats_df, patient_analysis):\n",
    "        \"\"\"åˆ›å»ºæ–‡æœ¬æ±‡æ€»æŠ¥å‘Š\"\"\"\n",
    "        \n",
    "        report_path = os.path.join(self.output_folder, 'summary_report.txt')\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"PATIENT FEATURE VISUALIZATION ANALYSIS REPORT\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            \n",
    "            # åŸºæœ¬ä¿¡æ¯\n",
    "            f.write(\"BASIC INFORMATION:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Number of patients: {len(self.combined_features['patient_id'].unique())}\\n\")\n",
    "            f.write(f\"Number of features: {len(self.feature_names)}\\n\")\n",
    "            f.write(f\"Total samples: {len(self.combined_features)}\\n\")\n",
    "            \n",
    "            grey_samples = len(self.combined_features[self.combined_features['matter_type'] == 'Grey'])\n",
    "            white_samples = len(self.combined_features[self.combined_features['matter_type'] == 'White'])\n",
    "            f.write(f\"Grey matter samples: {grey_samples}\\n\")\n",
    "            f.write(f\"White matter samples: {white_samples}\\n\\n\")\n",
    "            \n",
    "            # ç‰¹å¾åˆ†æ\n",
    "            f.write(\"FEATURE ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            significant_features = len(stats_df[stats_df['significant'] == True])\n",
    "            f.write(f\"Statistically significant features (p<0.05): {significant_features}/{len(stats_df)}\\n\")\n",
    "            \n",
    "            high_effect_features = len(stats_df[stats_df['cohens_d'].abs() > 0.5])\n",
    "            f.write(f\"Features with large effect size (|Cohen's d| > 0.5): {high_effect_features}/{len(stats_df)}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nTop 10 most discriminative features (by Cohen's d):\\n\")\n",
    "            top_10 = stats_df.nlargest(10, 'cohens_d')\n",
    "            for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "                f.write(f\"  {i:2d}. {row['feature_name']:20s} (d={row['cohens_d']:6.3f}, p={row['p_value']:8.3e})\\n\")\n",
    "            \n",
    "            # æ‚£è€…åˆ†æ\n",
    "            f.write(f\"\\nPATIENT ANALYSIS:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Average separability across patients: {patient_analysis['mean_separability'].mean():.3f} Â± {patient_analysis['mean_separability'].std():.3f}\\n\")\n",
    "            f.write(f\"Patient with highest separability: {patient_analysis.loc[patient_analysis['mean_separability'].idxmax(), 'patient_id']} ({patient_analysis['mean_separability'].max():.3f})\\n\")\n",
    "            f.write(f\"Patient with lowest separability: {patient_analysis.loc[patient_analysis['mean_separability'].idxmin(), 'patient_id']} ({patient_analysis['mean_separability'].min():.3f})\\n\")\n",
    "            \n",
    "            f.write(f\"\\nAverage significant features per patient: {patient_analysis['significant_features'].mean():.1f} Â± {patient_analysis['significant_features'].std():.1f}\\n\")\n",
    "            \n",
    "            # é—®é¢˜è¯Šæ–­\n",
    "            f.write(f\"\\nPOTENTIAL ISSUES IDENTIFIED:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            if significant_features < len(stats_df) * 0.1:\n",
    "                f.write(\"âš ï¸  Very few features show statistical significance between grey/white matter\\n\")\n",
    "            \n",
    "            if patient_analysis['mean_separability'].std() > patient_analysis['mean_separability'].mean() * 0.5:\n",
    "                f.write(\"âš ï¸  High variability in separability across patients\\n\")\n",
    "            \n",
    "            low_sep_patients = len(patient_analysis[patient_analysis['mean_separability'] < 0.2])\n",
    "            if low_sep_patients > len(patient_analysis) * 0.3:\n",
    "                f.write(f\"âš ï¸  {low_sep_patients} patients show very low feature separability (<0.2)\\n\")\n",
    "            \n",
    "            high_cv_features = len(stats_df[stats_df['between_patient_cv'] > 1.0])\n",
    "            if high_cv_features > len(stats_df) * 0.2:\n",
    "                f.write(f\"âš ï¸  {high_cv_features} features show high between-patient variability (CV>1.0)\\n\")\n",
    "            \n",
    "            f.write(f\"\\nRECOMMENDations:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(\"1. Consider patient-specific normalization or feature selection\\n\")\n",
    "            f.write(\"2. Investigate data preprocessing consistency across patients\\n\")\n",
    "            f.write(\"3. Consider hierarchical models that account for patient variability\\n\")\n",
    "            f.write(\"4. Focus on the most discriminative features for model training\\n\")\n",
    "            f.write(\"5. Consider removing patients with very low separability\\n\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°\n",
    "def analyze_patient_features(processed_folder, output_folder='feature_analysis_results'):\n",
    "    \"\"\"\n",
    "    ä¾¿æ·çš„åˆ†æå‡½æ•°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_folder : str\n",
    "        åŒ…å«æ‚£è€…æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    output_folder : str\n",
    "        è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: åˆ†æç»“æœå­—å…¸\n",
    "    \"\"\"\n",
    "    \n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    results = visualizer.run_complete_visualization()\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ec7bae6632f405b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# æ‚£è€…ç‰¹å¾å¯è§†åŒ–åˆ†æ - ä½¿ç”¨ç¤ºä¾‹\n",
    "# æ–¹æ³•1: ä½¿ç”¨ä¾¿æ·å‡½æ•°\n",
    "def quick_analysis():\n",
    "    \"\"\"å¿«é€Ÿåˆ†æç¤ºä¾‹\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\feature_visualization\"\n",
    "    \n",
    "    print(\"ğŸ” Starting Patient Feature Visualization Analysis...\")\n",
    "    \n",
    "    # è¿è¡Œå®Œæ•´åˆ†æ\n",
    "    results = analyze_patient_features(processed_folder, output_folder)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nâœ… Analysis completed successfully!\")\n",
    "        print(f\"ğŸ“Š Key findings:\")\n",
    "        print(f\"   - {results['n_patients']} patients analyzed\")\n",
    "        print(f\"   - {results['n_features']} features extracted\")\n",
    "        print(f\"   - {results['total_samples']} total samples\")\n",
    "        \n",
    "        # æ˜¾ç¤ºä¸€äº›å…³é”®ç»Ÿè®¡\n",
    "        stats_df = results['feature_stats']\n",
    "        significant_count = len(stats_df[stats_df['significant'] == True])\n",
    "        print(f\"   - {significant_count}/{len(stats_df)} features are statistically significant\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæœ€æœ‰åŒºåˆ†æ€§çš„ç‰¹å¾\n",
    "        top_features = stats_df.nlargest(5, 'cohens_d')['feature_name'].tolist()\n",
    "        print(f\"   - Top discriminative features: {', '.join(top_features[:3])}...\")\n",
    "    else:\n",
    "        print(\"âŒ Analysis failed\")\n",
    "\n",
    "# æ–¹æ³•2: è¯¦ç»†çš„æ­¥éª¤æ§åˆ¶\n",
    "def detailed_analysis():\n",
    "    \"\"\"è¯¦ç»†åˆ†æç¤ºä¾‹ï¼Œå¯ä»¥æ§åˆ¶æ¯ä¸ªæ­¥éª¤\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\detailed_feature_analysis\"\n",
    "    \n",
    "    # åˆ›å»ºå¯è§†åŒ–å™¨\n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    \n",
    "    # æ­¥éª¤1: åŠ è½½æ•°æ®å’Œæå–ç‰¹å¾\n",
    "    print(\"ğŸ“‚ Loading patient data and extracting features...\")\n",
    "    if not visualizer.load_and_extract_features():\n",
    "        print(\"âŒ Failed to load data\")\n",
    "        return\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(visualizer.combined_features['patient_id'].unique())} patients\")\n",
    "    print(f\"ğŸ“Š Total samples: {len(visualizer.combined_features)}\")\n",
    "    print(f\"ğŸ”¢ Features: {len(visualizer.feature_names)}\")\n",
    "    \n",
    "    # æ­¥éª¤2: åˆ›å»ºä¸ªåˆ«ç‰¹å¾åˆ†å¸ƒå›¾\n",
    "    print(\"\\nğŸ“ˆ Creating individual feature distribution plots...\")\n",
    "    visualizer.create_individual_feature_plots(n_features_per_page=12)  # æ¯é¡µ12ä¸ªç‰¹å¾\n",
    "    \n",
    "    # æ­¥éª¤3: ç»Ÿè®¡åˆ†æ\n",
    "    print(\"\\nğŸ“Š Creating feature statistics...\")\n",
    "    stats_df = visualizer.create_feature_summary_statistics()\n",
    "    \n",
    "    # æ˜¾ç¤ºä¸€äº›å…³é”®å‘ç°\n",
    "    significant_features = stats_df[stats_df['significant'] == True]\n",
    "    print(f\"   ğŸ¯ {len(significant_features)} features are statistically significant\")\n",
    "    \n",
    "    if len(significant_features) > 0:\n",
    "        best_feature = significant_features.loc[significant_features['cohens_d'].abs().idxmax()]\n",
    "        print(f\"   ğŸ† Best discriminative feature: {best_feature['feature_name']}\")\n",
    "        print(f\"      - Cohen's d: {best_feature['cohens_d']:.3f}\")\n",
    "        print(f\"      - p-value: {best_feature['p_value']:.3e}\")\n",
    "    \n",
    "    # æ­¥éª¤4: æ‚£è€…æ¯”è¾ƒåˆ†æ\n",
    "    print(\"\\nğŸ‘¥ Creating patient comparison analysis...\")\n",
    "    visualizer.create_patient_comparison_plots()\n",
    "    \n",
    "    # æ­¥éª¤5: æ‚£è€…ç‰¹å¼‚æ€§åˆ†æ\n",
    "    print(\"\\nğŸ” Creating patient-specific analysis...\")\n",
    "    patient_analysis = visualizer.create_patient_specific_analysis()\n",
    "    \n",
    "    # æ˜¾ç¤ºæ‚£è€…ç›¸å…³å‘ç°\n",
    "    best_patient = patient_analysis.loc[patient_analysis['mean_separability'].idxmax()]\n",
    "    worst_patient = patient_analysis.loc[patient_analysis['mean_separability'].idxmin()]\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Best separability: {best_patient['patient_id']} (d={best_patient['mean_separability']:.3f})\")\n",
    "    print(f\"   ğŸ“‰ Worst separability: {worst_patient['patient_id']} (d={worst_patient['mean_separability']:.3f})\")\n",
    "    \n",
    "    # è¯†åˆ«æ½œåœ¨é—®é¢˜\n",
    "    low_sep_patients = patient_analysis[patient_analysis['mean_separability'] < 0.2]\n",
    "    if len(low_sep_patients) > 0:\n",
    "        print(f\"   âš ï¸  {len(low_sep_patients)} patients have very low separability:\")\n",
    "        for _, patient in low_sep_patients.iterrows():\n",
    "            print(f\"      - {patient['patient_id']}: {patient['mean_separability']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ All results saved to: {output_folder}\")\n",
    "    \n",
    "    return {\n",
    "        'visualizer': visualizer,\n",
    "        'stats': stats_df,\n",
    "        'patient_analysis': patient_analysis\n",
    "    }\n",
    "\n",
    "# æ–¹æ³•3: é’ˆå¯¹æ€§åˆ†æç‰¹å®šç‰¹å¾\n",
    "def analyze_specific_features(feature_list=None):\n",
    "    \"\"\"åˆ†æç‰¹å®šç‰¹å¾çš„ç¤ºä¾‹\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\specific_features\"\n",
    "    \n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    if not visualizer.load_and_extract_features():\n",
    "        return\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æŒ‡å®šç‰¹å¾ï¼Œä½¿ç”¨æœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾\n",
    "    if feature_list is None:\n",
    "        stats_df = visualizer.create_feature_summary_statistics()\n",
    "        feature_list = stats_df.nlargest(8, 'cohens_d')['feature_name'].tolist()\n",
    "    \n",
    "    print(f\"ğŸ¯ Analyzing specific features: {feature_list}\")\n",
    "    \n",
    "    # åˆ›å»ºé’ˆå¯¹æ€§å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    patients = visualizer.combined_features['patient_id'].unique()\n",
    "    patient_colors = plt.cm.Set3(np.linspace(0, 1, len(patients)))\n",
    "    patient_color_map = {patient: patient_colors[i] for i, patient in enumerate(patients)}\n",
    "    \n",
    "    for i, feature in enumerate(feature_list[:8]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªæ‚£è€…ç»˜åˆ¶æ•£ç‚¹\n",
    "        for j, patient in enumerate(patients):\n",
    "            patient_data = visualizer.combined_features[visualizer.combined_features['patient_id'] == patient]\n",
    "            \n",
    "            grey_data = patient_data[patient_data['matter_type'] == 'Grey']\n",
    "            white_data = patient_data[patient_data['matter_type'] == 'White']\n",
    "            \n",
    "            if len(grey_data) > 0:\n",
    "                y_grey = np.random.normal(1, 0.05, len(grey_data))\n",
    "                ax.scatter(grey_data[feature], y_grey, c=[patient_color_map[patient]], \n",
    "                          alpha=0.6, s=15, marker='o')\n",
    "            \n",
    "            if len(white_data) > 0:\n",
    "                y_white = np.random.normal(0, 0.05, len(white_data))\n",
    "                ax.scatter(white_data[feature], y_white, c=[patient_color_map[patient]], \n",
    "                          alpha=0.6, s=15, marker='s')\n",
    "        \n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Matter Type')\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_yticklabels(['White', 'Grey'])\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_title(feature, fontsize=10)\n",
    "    \n",
    "    # éšè—ç©ºçš„å­å›¾\n",
    "    for i in range(len(feature_list), 8):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # æ·»åŠ å›¾ä¾‹\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for patient in patients:\n",
    "        handles.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                markerfacecolor=patient_color_map[patient], markersize=8))\n",
    "        labels.append(patient)\n",
    "    \n",
    "    plt.figlegend(handles, labels, loc='center', bbox_to_anchor=(0.5, 0.02), \n",
    "                 ncol=min(6, len(patients)), fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Patient-Specific Feature Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig(os.path.join(output_folder, 'specific_features_analysis.png'), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def troubleshoot_classification_issues():\n",
    "    \"\"\"è¯Šæ–­åˆ†ç±»é—®é¢˜çš„ä¸“é—¨åˆ†æ\"\"\"\n",
    "    \n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline\"\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\troubleshooting\"\n",
    "    \n",
    "    visualizer = PatientFeatureVisualizer(processed_folder, output_folder)\n",
    "    \n",
    "    if not visualizer.load_and_extract_features():\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ”§ TROUBLESHOOTING CLASSIFICATION ISSUES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§\n",
    "    patients = visualizer.combined_features['patient_id'].unique()\n",
    "    print(f\"\\n1. DATA BALANCE CHECK:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    total_grey = 0\n",
    "    total_white = 0\n",
    "    imbalanced_patients = []\n",
    "    \n",
    "    for patient in patients:\n",
    "        patient_data = visualizer.combined_features[visualizer.combined_features['patient_id'] == patient]\n",
    "        n_grey = len(patient_data[patient_data['matter_type'] == 'Grey'])\n",
    "        n_white = len(patient_data[patient_data['matter_type'] == 'White'])\n",
    "        \n",
    "        total_grey += n_grey\n",
    "        total_white += n_white\n",
    "        \n",
    "        ratio = min(n_grey, n_white) / max(n_grey, n_white) if max(n_grey, n_white) > 0 else 0\n",
    "        \n",
    "        print(f\"   {patient}: Grey={n_grey:4d}, White={n_white:4d}, Ratio={ratio:.2f}\")\n",
    "        \n",
    "        if ratio < 0.3:  # ä¸¥é‡ä¸å¹³è¡¡\n",
    "            imbalanced_patients.append(patient)\n",
    "    \n",
    "    print(f\"\\n   Overall: Grey={total_grey}, White={total_white}\")\n",
    "    overall_ratio = min(total_grey, total_white) / max(total_grey, total_white)\n",
    "    print(f\"   Overall Ratio: {overall_ratio:.3f}\")\n",
    "    \n",
    "    if imbalanced_patients:\n",
    "        print(f\"   âš ï¸  Severely imbalanced patients: {imbalanced_patients}\")\n",
    "    \n",
    "    # 2. æ£€æŸ¥ç‰¹å¾è´¨é‡\n",
    "    print(f\"\\n2. FEATURE QUALITY CHECK:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    stats_df = visualizer.create_feature_summary_statistics()\n",
    "    \n",
    "    # æ— åŒºåˆ†æ€§çš„ç‰¹å¾\n",
    "    weak_features = stats_df[stats_df['cohens_d'].abs() < 0.1]\n",
    "    print(f\"   Weak features (|Cohen's d| < 0.1): {len(weak_features)}/{len(stats_df)}\")\n",
    "    \n",
    "    # é«˜å˜å¼‚æ€§ç‰¹å¾\n",
    "    high_var_features = stats_df[stats_df['between_patient_cv'] > 1.5]\n",
    "    print(f\"   High variability features (CV > 1.5): {len(high_var_features)}/{len(stats_df)}\")\n",
    "    \n",
    "    # æ˜¾è‘—æ€§ç‰¹å¾\n",
    "    significant_features = stats_df[stats_df['significant'] == True]\n",
    "    print(f\"   Statistically significant features: {len(significant_features)}/{len(stats_df)}\")\n",
    "    \n",
    "    if len(significant_features) < len(stats_df) * 0.1:\n",
    "        print(\"   âš ï¸  Very few significant features - possible data quality issue!\")\n",
    "    \n",
    "    # 3. æ‚£è€…å¼‚è´¨æ€§æ£€æŸ¥\n",
    "    print(f\"\\n3. PATIENT HETEROGENEITY CHECK:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    patient_analysis = visualizer.create_patient_specific_analysis()\n",
    "    \n",
    "    low_sep_patients = patient_analysis[patient_analysis['mean_separability'] < 0.2]\n",
    "    print(f\"   Patients with low separability (<0.2): {len(low_sep_patients)}/{len(patient_analysis)}\")\n",
    "    \n",
    "    if len(low_sep_patients) > 0:\n",
    "        print(\"   Low separability patients:\")\n",
    "        for _, patient in low_sep_patients.iterrows():\n",
    "            print(f\"      {patient['patient_id']}: {patient['mean_separability']:.3f}\")\n",
    "    \n",
    "    # è®¡ç®—æ‚£è€…é—´ç›¸ä¼¼æ€§\n",
    "    feature_matrix = []\n",
    "    patient_list = []\n",
    "    \n",
    "    for patient in patients:\n",
    "        patient_data = visualizer.combined_features[visualizer.combined_features['patient_id'] == patient]\n",
    "        grey_data = patient_data[patient_data['matter_type'] == 'Grey'][visualizer.feature_names]\n",
    "        \n",
    "        if len(grey_data) > 0:\n",
    "            patient_features = grey_data.mean().values\n",
    "            feature_matrix.append(patient_features)\n",
    "            patient_list.append(patient)\n",
    "    \n",
    "    if len(feature_matrix) > 1:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        \n",
    "        # æ ‡å‡†åŒ–å¹¶è®¡ç®—è·ç¦»\n",
    "        scaler = StandardScaler()\n",
    "        feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "        distances = pdist(feature_matrix_scaled, metric='euclidean')\n",
    "        distance_matrix = squareform(distances)\n",
    "        \n",
    "        avg_distance = np.mean(distances)\n",
    "        print(f\"   Average inter-patient distance: {avg_distance:.3f}\")\n",
    "        \n",
    "        if avg_distance > 5.0:\n",
    "            print(\"   âš ï¸  High inter-patient variability detected!\")\n",
    "    \n",
    "    # 4. å»ºè®®ç”Ÿæˆ\n",
    "    print(f\"\\n4. RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if overall_ratio < 0.7:\n",
    "        recommendations.append(\"Use balanced sampling or class weights in training\")\n",
    "    \n",
    "    if len(imbalanced_patients) > len(patients) * 0.3:\n",
    "        recommendations.append(\"Consider removing severely imbalanced patients\")\n",
    "    \n",
    "    if len(weak_features) > len(stats_df) * 0.5:\n",
    "        recommendations.append(\"Perform feature selection to remove weak features\")\n",
    "    \n",
    "    if len(high_var_features) > len(stats_df) * 0.3:\n",
    "        recommendations.append(\"Consider patient-specific normalization\")\n",
    "    \n",
    "    if len(low_sep_patients) > len(patients) * 0.3:\n",
    "        recommendations.append(\"Investigate data preprocessing consistency\")\n",
    "        recommendations.append(\"Consider hierarchical/mixed-effects models\")\n",
    "    \n",
    "    if len(significant_features) < len(stats_df) * 0.1:\n",
    "        recommendations.append(\"Review feature extraction methodology\")\n",
    "        recommendations.append(\"Consider different time windows or signal processing\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        print(\"   âœ… No major issues detected in the data\")\n",
    "    \n",
    "    # 5. ç”Ÿæˆæ”¹è¿›å»ºè®®çš„å…·ä½“ä»£ç \n",
    "    print(f\"\\n5. IMPLEMENTATION SUGGESTIONS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    print(\"   # ç‰¹å¾é€‰æ‹©ç¤ºä¾‹:\")\n",
    "    print(\"   from sklearn.feature_selection import SelectKBest, f_classif\")\n",
    "    print(\"   selector = SelectKBest(f_classif, k=20)\")\n",
    "    print(\"   X_selected = selector.fit_transform(X, y)\")\n",
    "    \n",
    "    print(\"\\n   # ç±»åˆ«å¹³è¡¡ç¤ºä¾‹:\")\n",
    "    print(\"   from sklearn.utils.class_weight import compute_class_weight\")\n",
    "    print(\"   class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\")\n",
    "    \n",
    "    print(\"\\n   # æ‚£è€…æ ‡å‡†åŒ–ç¤ºä¾‹:\")\n",
    "    print(\"   from sklearn.preprocessing import StandardScaler\")\n",
    "    print(\"   for patient in patients:\")\n",
    "    print(\"       scaler = StandardScaler()\")\n",
    "    print(\"       patient_data = scaler.fit_transform(patient_data)\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae60bef374614b48",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "detailed_analysis()\n",
    "troubleshoot_classification_issues()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "45999cf6eb4cc82f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "from scipy.integrate import trapezoid\n",
    "class ChannelClassifier:\n",
    "    \"\"\"\n",
    "    Channel-Level Grey/White Matteråˆ†ç±»å™¨\n",
    "    \n",
    "    æ ¸å¿ƒç‰¹ç‚¹ï¼š\n",
    "    1. æ¯ä¸ªchannelä½œä¸ºç‹¬ç«‹æ ·æœ¬ï¼ˆæ— moving windowï¼‰\n",
    "    2. Patient-levelæ ‡å‡†åŒ–\n",
    "    3. Training-only outlierå¤„ç†\n",
    "    4. ç²¾ç®€ç‰¹å¾é›†\n",
    "    5. åŒé‡éªŒè¯ï¼ˆChannel-level + Patient-levelï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_folder, output_folder='results'):\n",
    "        self.processed_folder = processed_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.patients_data = {}\n",
    "        self.channels_data = {}  # {channel_id: {features, label, patient_id, ...}}\n",
    "        \n",
    "        # åˆ†ç±»å™¨\n",
    "        self.classifiers = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "            'SVM': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "            'MLP': MLPClassifier(max_iter=1000, random_state=42, hidden_layer_sizes=(50,)),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'LDA': LDA(),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "        \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"ğŸ§  Channel Classifier initialized\")\n",
    "        print(f\"   Data: {processed_folder}\")\n",
    "        print(f\"   Output: {output_folder}\")\n",
    "    \n",
    "    def load_patients(self):\n",
    "        \"\"\"åŠ è½½æ‰€æœ‰æ‚£è€…æ•°æ®\"\"\"\n",
    "        pkl_files = glob.glob(os.path.join(self.processed_folder, \"P*_processed.pkl\"))\n",
    "        print(f\"\\nğŸ“‚ Loading {len(pkl_files)} patients...\")\n",
    "        \n",
    "        for pkl_file in pkl_files:\n",
    "            try:\n",
    "                with open(pkl_file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                pid = data['patient_id']\n",
    "                self.patients_data[pid] = data\n",
    "                duration = data['processing_summary']['total_duration_seconds'] / 60\n",
    "                print(f\"  âœ“ {pid}: {duration:.1f} min\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— {os.path.basename(pkl_file)}: {e}\")\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(self.patients_data)} patients\")\n",
    "        return len(self.patients_data) > 0\n",
    "    \n",
    "    def _extract_electrode_types(self, matter_data):\n",
    "        \"\"\"æå–ç”µæç±»å‹\"\"\"\n",
    "        # æŸ¥æ‰¾matteråˆ—\n",
    "        matter_cols = ['MatterType', 'matter', 'Matter', 'mattertype', 'tissue_type', 'type']\n",
    "        matter_col = None\n",
    "        for col in matter_cols:\n",
    "            if col in matter_data.columns:\n",
    "                matter_col = col\n",
    "                break\n",
    "        \n",
    "        if matter_col is None:\n",
    "            raise ValueError(f\"Matter column not found in {matter_data.columns.tolist()}\")\n",
    "        \n",
    "        # æå–grey/whiteç´¢å¼•\n",
    "        matter_values = matter_data[matter_col].astype(str).str.lower()\n",
    "        grey_mask = matter_values.isin(['g', 'grey', 'gray'])\n",
    "        white_mask = matter_values.isin(['w', 'white'])\n",
    "        \n",
    "        # å¦‚æœæ ‡å‡†æ ¼å¼æ‰¾ä¸åˆ°ï¼Œå°è¯•æ¨¡å¼åŒ¹é…\n",
    "        if grey_mask.sum() == 0:\n",
    "            grey_mask = matter_values.str.contains('grey|gray|cortex', case=False, na=False)\n",
    "        if white_mask.sum() == 0:\n",
    "            white_mask = matter_values.str.contains('white', case=False, na=False)\n",
    "        \n",
    "        grey_indices = matter_data.index[grey_mask].tolist()\n",
    "        white_indices = matter_data.index[white_mask].tolist()\n",
    "        \n",
    "        return grey_indices, white_indices\n",
    "    \n",
    "    def _extract_features(self, signal, fs, avg_power_spectrum=None, freqs=None):\n",
    "        \"\"\"æå–ç²¾ç®€ç‰¹å¾é›†\"\"\"\n",
    "        if len(signal) < 100 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        features = {}\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        features['range'] = np.ptp(signal)\n",
    "        features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "        features['energy'] = np.sum(signal**2)\n",
    "        \n",
    "        # é¢‘åŸŸç‰¹å¾ï¼šç›¸å¯¹åŠŸç‡\n",
    "        try:\n",
    "            signal_freqs, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), scaling='density')\n",
    "            total_power = np.sum(psd)\n",
    "            \n",
    "            bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 100),\n",
    "                'high_gamma': (100, min(200, fs/2))\n",
    "            }\n",
    "            \n",
    "            # è®¡ç®—æ¯ä¸ªé¢‘æ®µçš„åŠŸç‡å’Œç›¸å¯¹åç§»\n",
    "            for band_name, (low, high) in bands.items():\n",
    "                band_mask = (signal_freqs >= low) & (signal_freqs <= high)\n",
    "                if np.any(band_mask):\n",
    "                    band_power = np.sum(psd[band_mask])\n",
    "                    features[f'power_{band_name}'] = band_power\n",
    "                    \n",
    "                    # è®¡ç®—è¯¥é¢‘æ®µç›¸å¯¹äºæ‚£è€…å¹³å‡åŠŸç‡è°±çš„å‚ç›´åç§»\n",
    "                    if avg_power_spectrum is not None and freqs is not None:\n",
    "                        avg_band_mask = (freqs >= low) & (freqs <= high)\n",
    "                        if np.any(avg_band_mask):\n",
    "                            # åœ¨log scaleä¸‹è®¡ç®—è¯¥é¢‘æ®µçš„å¹³å‡åç§»\n",
    "                            signal_band_psd_log = np.log10(psd[band_mask] + 1e-12)\n",
    "                            avg_band_psd_log = np.log10(avg_power_spectrum[avg_band_mask] + 1e-12)\n",
    "                            \n",
    "                            # å¦‚æœé¢‘ç‡ç‚¹æ•°ä¸åŒï¼Œå–å¹³å‡å€¼\n",
    "                            signal_avg = np.mean(signal_band_psd_log)\n",
    "                            avg_avg = np.mean(avg_band_psd_log)\n",
    "                            \n",
    "                            features[f'rel_power_{band_name}'] = signal_avg - avg_avg\n",
    "                        else:\n",
    "                            features[f'rel_power_{band_name}'] = 0\n",
    "                    else:\n",
    "                        features[f'rel_power_{band_name}'] = 0\n",
    "                else:\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                    features[f'rel_power_{band_name}'] = 0\n",
    "            \n",
    "            features['total_power'] = total_power\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in frequency analysis: {e}\")\n",
    "            bands = ['delta', 'theta', 'alpha', 'beta', 'gamma', 'high_gamma']\n",
    "            for band in bands:\n",
    "                features[f'power_{band}'] = 0\n",
    "                features[f'rel_power_{band}'] = 0\n",
    "            features['total_power'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"æå–æ‰€æœ‰channelsçš„ç‰¹å¾\"\"\"\n",
    "        print(f\"\\nğŸ”„ Extracting channel features...\")\n",
    "        \n",
    "        channel_id = 0\n",
    "        \n",
    "        for patient_id, patient_data in self.patients_data.items():\n",
    "            print(f\"  Processing {patient_id}...\")\n",
    "            \n",
    "            try:\n",
    "                matter_data = patient_data['matter_data']\n",
    "                recordings = patient_data['recordings']\n",
    "                \n",
    "                # æå–ç”µæç±»å‹\n",
    "                grey_indices, white_indices = self._extract_electrode_types(matter_data)\n",
    "                \n",
    "                if len(grey_indices) == 0 or len(white_indices) == 0:\n",
    "                    print(f\"    âœ— Missing grey/white electrodes\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"    {len(grey_indices)} grey, {len(white_indices)} white electrodes\")\n",
    "                \n",
    "                # åˆå¹¶æ‰€æœ‰recordingæ•°æ®\n",
    "                all_data = []\n",
    "                fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "                \n",
    "                for recording in recordings:\n",
    "                    data_segment = recording['neural_data_processed']\n",
    "                    if isinstance(data_segment, list):\n",
    "                        all_data.extend(data_segment)\n",
    "                    else:\n",
    "                        all_data.append(data_segment)\n",
    "    \n",
    "                if not all_data:\n",
    "                    print(f\"    âœ— No data found\")\n",
    "                    continue\n",
    "                    \n",
    "                combined_data = np.vstack(all_data)\n",
    "                duration_min = len(combined_data) / fs / 60\n",
    "                print(f\"    Combined: {combined_data.shape} ({duration_min:.1f} min)\")\n",
    "    \n",
    "                # è®¡ç®—æ‚£è€…çº§åˆ«çš„å¹³å‡åŠŸç‡è°± (1-150Hz)\n",
    "                print(f\"    Computing patient-level average power spectrum...\")\n",
    "                all_power_spectra = []\n",
    "                \n",
    "                for ch in range(combined_data.shape[1]):\n",
    "                    sig = combined_data[:, ch]\n",
    "                    if len(sig) > 0 and not np.all(sig == 0):\n",
    "                        try:\n",
    "                            freqs_ch, psd_ch = welch(sig, fs=fs, nperseg=min(1024, len(sig)), scaling='density')\n",
    "                            all_power_spectra.append(psd_ch)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Warning: Error computing PSD for channel {ch}: {e}\")\n",
    "     \n",
    "                if all_power_spectra:\n",
    "                    all_power_spectra = np.vstack(all_power_spectra)\n",
    "                    avg_power_spectrum = np.mean(all_power_spectra, axis=0)\n",
    "                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸè®¡ç®—çš„é¢‘ç‡å‘é‡ä½œä¸ºå‚è€ƒ\n",
    "                    freqs, _ = welch(combined_data[:, 0], fs=fs, nperseg=min(1024, len(combined_data[:, 0])), scaling='density')\n",
    "                else:\n",
    "                    avg_power_spectrum = None\n",
    "                    freqs = None\n",
    "                    print(f\"    Warning: Could not compute reference power spectrum\")\n",
    "                \n",
    "                # å¤„ç†Grey Matter channels\n",
    "                for electrode_idx in grey_indices:\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        features = self._extract_features(signal, fs, avg_power_spectrum, freqs)\n",
    "                        \n",
    "                        if features is not None:\n",
    "                            self.channels_data[channel_id] = {\n",
    "                                'features': features,\n",
    "                                'label': 1,  # Grey = 1\n",
    "                                'patient_id': patient_id,\n",
    "                                'electrode_idx': electrode_idx,\n",
    "                                'matter_type': 'grey',\n",
    "                                'duration_sec': len(signal) / fs\n",
    "                            }\n",
    "                            channel_id += 1\n",
    "                \n",
    "                # å¤„ç†White Matter channels\n",
    "                for electrode_idx in white_indices:\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        features = self._extract_features(signal, fs, avg_power_spectrum, freqs)\n",
    "                        \n",
    "                        if features is not None:\n",
    "                            self.channels_data[channel_id] = {\n",
    "                                'features': features,\n",
    "                                'label': 0,  # White = 0\n",
    "                                'patient_id': patient_id,\n",
    "                                'electrode_idx': electrode_idx,\n",
    "                                'matter_type': 'white',\n",
    "                                'duration_sec': len(signal) / fs\n",
    "                            }\n",
    "                            channel_id += 1\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"    âœ— Error processing {patient_id}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        n_grey = sum(1 for ch in self.channels_data.values() if ch['label'] == 1)\n",
    "        n_white = sum(1 for ch in self.channels_data.values() if ch['label'] == 0)\n",
    "        \n",
    "        print(f\"\\nâœ… Extracted {len(self.channels_data)} channels\")\n",
    "        print(f\"   Grey: {n_grey}, White: {n_white}\")\n",
    "        \n",
    "        if self.channels_data:\n",
    "            first_ch = list(self.channels_data.values())[0]\n",
    "            print(f\"   Features: {len(first_ch['features'])}\")\n",
    "            print(f\"   Feature names: {list(first_ch['features'].keys())}\")\n",
    "        \n",
    "        return len(self.channels_data) > 0\n",
    "    \n",
    "    def normalize_by_patient(self, method='robust'):\n",
    "        \"\"\"Patient-levelæ ‡å‡†åŒ–\"\"\"\n",
    "        print(f\"\\nğŸ”„ Patient-level normalization ({method})...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            return False\n",
    "        \n",
    "        # æŒ‰patientåˆ†ç»„\n",
    "        patient_groups = {}\n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            pid = ch_data['patient_id']\n",
    "            if pid not in patient_groups:\n",
    "                patient_groups[pid] = []\n",
    "            patient_groups[pid].append((ch_id, ch_data))\n",
    "        \n",
    "        # è·å–ç‰¹å¾åç§°\n",
    "        feature_names = list(list(self.channels_data.values())[0]['features'].keys())\n",
    "        print(f\"  Normalizing {len(feature_names)} features across {len(patient_groups)} patients\")\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªpatientæ ‡å‡†åŒ–\n",
    "        for pid, channels in patient_groups.items():\n",
    "            print(f\"    {pid}: {len(channels)} channels\")\n",
    "            \n",
    "            # æ”¶é›†è¯¥patientçš„æ‰€æœ‰ç‰¹å¾\n",
    "            features_matrix = []\n",
    "            for ch_id, ch_data in channels:\n",
    "                feature_vector = [ch_data['features'][fname] for fname in feature_names]\n",
    "                features_matrix.append(feature_vector)\n",
    "            \n",
    "            features_df = pd.DataFrame(features_matrix, columns=feature_names)\n",
    "            \n",
    "            # æ ‡å‡†åŒ–\n",
    "            normalized_df = self._normalize_dataframe(features_df, method)\n",
    "            \n",
    "            # æ›´æ–°ç‰¹å¾\n",
    "            for i, (ch_id, ch_data) in enumerate(channels):\n",
    "                normalized_features = normalized_df.iloc[i].to_dict()\n",
    "                self.channels_data[ch_id]['features'] = normalized_features\n",
    "                self.channels_data[ch_id]['normalized'] = True\n",
    "        \n",
    "        print(f\"âœ… Patient-level normalization completed\")\n",
    "        return True\n",
    "    \n",
    "    def _normalize_dataframe(self, df, method):\n",
    "        \"\"\"æ ‡å‡†åŒ–DataFrame\"\"\"\n",
    "        normalized_df = df.copy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            values = df[col].values\n",
    "            \n",
    "            if method == 'robust':\n",
    "                median_val = np.median(values)\n",
    "                mad_val = np.median(np.abs(values - median_val))\n",
    "                if mad_val > 0:\n",
    "                    normalized_df[col] = (values - median_val) / (1.4826 * mad_val)\n",
    "                else:\n",
    "                    normalized_df[col] = values - median_val\n",
    "            elif method == 'standard':\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                if std_val > 0:\n",
    "                    normalized_df[col] = (values - mean_val) / std_val\n",
    "                else:\n",
    "                    normalized_df[col] = values - mean_val\n",
    "            elif method == 'minmax':\n",
    "                min_val, max_val = np.min(values), np.max(values)\n",
    "                if max_val > min_val:\n",
    "                    normalized_df[col] = (values - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    normalized_df[col] = np.zeros_like(values)\n",
    "        \n",
    "        return normalized_df\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"å‡†å¤‡è®­ç»ƒæ•°æ®\"\"\"\n",
    "        if not self.channels_data:\n",
    "            return None, None, None\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        info = []\n",
    "        \n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            features = ch_data['features']\n",
    "            feature_vector = np.array(list(features.values()))\n",
    "            \n",
    "            X.append(feature_vector)\n",
    "            y.append(ch_data['label'])\n",
    "            info.append({\n",
    "                'channel_id': ch_id,\n",
    "                'patient_id': ch_data['patient_id'],\n",
    "                'matter_type': ch_data['matter_type'],\n",
    "                'electrode_idx': ch_data['electrode_idx']\n",
    "            })\n",
    "        \n",
    "        return np.array(X), np.array(y), info\n",
    "    \n",
    "    def _clip_outliers_training_only(self, X_train, X_test, iqr_factor=1.5):\n",
    "        \"\"\"åªåŸºäºè®­ç»ƒé›†è®¡ç®—outlierè¾¹ç•Œ\"\"\"\n",
    "        X_train_clipped = X_train.copy()\n",
    "        X_test_clipped = X_test.copy()\n",
    "        \n",
    "        for feature_idx in range(X_train.shape[1]):\n",
    "            train_vals = X_train[:, feature_idx]\n",
    "            \n",
    "            q25, q75 = np.percentile(train_vals, [25, 75])\n",
    "            iqr = q75 - q25\n",
    "            \n",
    "            if iqr > 0:\n",
    "                lower = q25 - iqr_factor * iqr\n",
    "                upper = q75 + iqr_factor * iqr\n",
    "                \n",
    "                X_train_clipped[:, feature_idx] = np.clip(train_vals, lower, upper)\n",
    "                X_test_clipped[:, feature_idx] = np.clip(X_test[:, feature_idx], lower, upper)\n",
    "        \n",
    "        return X_train_clipped, X_test_clipped\n",
    "    \n",
    "    def channel_level_validation(self, test_size=0.2, outlier_clip=True, iqr_factor=1.5):\n",
    "        \"\"\"Channel-leveléªŒè¯ï¼šéšæœºåˆ†å‰²\"\"\"\n",
    "        print(f\"\\nğŸ” Channel-Level Validation (random split)\")\n",
    "        \n",
    "        X, y, info = self._prepare_data()\n",
    "        if X is None:\n",
    "            return None\n",
    "        \n",
    "        # éšæœºåˆ†å‰²\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"  Train: {len(X_train)} ({np.sum(y_train)} grey)\")\n",
    "        print(f\"  Test: {len(X_test)} ({np.sum(y_test)} grey)\")\n",
    "        \n",
    "        # Outlier clipping\n",
    "        if outlier_clip:\n",
    "            X_train, X_test = self._clip_outliers_training_only(X_train, X_test, iqr_factor)\n",
    "            print(f\"  Applied outlier clipping (IQR Ã— {iqr_factor})\")\n",
    "        \n",
    "        # è®­ç»ƒå’Œè¯„ä¼°\n",
    "        results = {}\n",
    "        for clf_name, clf in self.classifiers.items():\n",
    "            try:\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else np.zeros_like(y_pred)\n",
    "                \n",
    "                results[clf_name] = {\n",
    "                    'accuracy': accuracy_score(y_test, y_pred),\n",
    "                    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "                    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                    'balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "                    'roc_auc': roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                    'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                    'y_true': y_test,\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_proba': y_proba\n",
    "                }\n",
    "                \n",
    "                print(f\"  {clf_name}: F1={results[clf_name]['f1']:.3f}, Acc={results[clf_name]['accuracy']:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {clf_name}: Error - {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def patient_level_validation(self, outlier_clip=True, iqr_factor=1.5):\n",
    "        \"\"\"Patient-leveléªŒè¯ï¼šLeave-one-patient-out\"\"\"\n",
    "        print(f\"\\nğŸ” Patient-Level Validation (LOPO)\")\n",
    "        \n",
    "        X, y, info = self._prepare_data()\n",
    "        if X is None:\n",
    "            return None\n",
    "        \n",
    "        patients = list(set([inf['patient_id'] for inf in info]))\n",
    "        print(f\"  {len(patients)} patients for LOPO\")\n",
    "        \n",
    "        if len(patients) < 3:\n",
    "            print(\"  âŒ Need at least 3 patients\")\n",
    "            return None\n",
    "        \n",
    "        # å­˜å‚¨ç»“æœ\n",
    "        cv_results = {name: [] for name in self.classifiers.keys()}\n",
    "        all_predictions = {name: {'y_true': [], 'y_pred': [], 'y_proba': []} for name in self.classifiers.keys()}\n",
    "        \n",
    "        for fold, test_patient in enumerate(patients):\n",
    "            print(f\"\\n  Fold {fold+1}/{len(patients)}: Test {test_patient}\")\n",
    "            \n",
    "            # åˆ†ç¦»æ•°æ®\n",
    "            train_mask = np.array([inf['patient_id'] != test_patient for inf in info])\n",
    "            test_mask = ~train_mask\n",
    "            \n",
    "            X_train, y_train = X[train_mask], y[train_mask]\n",
    "            X_test, y_test = X[test_mask], y[test_mask]\n",
    "            \n",
    "            print(f\"    Train: {len(X_train)} ({np.sum(y_train)} grey)\")\n",
    "            print(f\"    Test: {len(X_test)} ({np.sum(y_test)} grey)\")\n",
    "            \n",
    "            # Outlier clipping\n",
    "            if outlier_clip:\n",
    "                X_train, X_test = self._clip_outliers_training_only(X_train, X_test, iqr_factor)\n",
    "            \n",
    "            # è®­ç»ƒè¯„ä¼°\n",
    "            for clf_name, clf in self.classifiers.items():\n",
    "                try:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_test)\n",
    "                    y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else np.zeros_like(y_pred)\n",
    "                    \n",
    "                    fold_result = {\n",
    "                        'fold': fold,\n",
    "                        'test_patient': test_patient,\n",
    "                        'accuracy': accuracy_score(y_test, y_pred),\n",
    "                        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "                        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "                        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "                        'balanced_acc': balanced_accuracy_score(y_test, y_pred),\n",
    "                        'roc_auc': roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                        'n_test': len(y_test)\n",
    "                    }\n",
    "                    \n",
    "                    cv_results[clf_name].append(fold_result)\n",
    "                    all_predictions[clf_name]['y_true'].extend(y_test)\n",
    "                    all_predictions[clf_name]['y_pred'].extend(y_pred)\n",
    "                    all_predictions[clf_name]['y_proba'].extend(y_proba)\n",
    "                    \n",
    "                    print(f\"      {clf_name}: F1={fold_result['f1']:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"      {clf_name}: Error - {e}\")\n",
    "        \n",
    "        return cv_results, all_predictions\n",
    "    \n",
    "    def analyze_results(self, channel_results, patient_results):\n",
    "        \"\"\"åˆ†æç»“æœ\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“Š Results Analysis\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        analysis = {'channel': {}, 'patient': {}}\n",
    "        \n",
    "        # Channel-levelç»“æœ\n",
    "        if channel_results:\n",
    "            print(f\"\\nğŸ” Channel-Level Results:\")\n",
    "            best_channel_f1 = 0\n",
    "            best_channel_clf = None\n",
    "            \n",
    "            for clf_name, metrics in channel_results.items():\n",
    "                f1 = metrics['f1']\n",
    "                acc = metrics['accuracy']\n",
    "                auc = metrics['roc_auc']\n",
    "                print(f\"  {clf_name}: F1={f1:.3f}, Acc={acc:.3f}, AUC={auc:.3f}\")\n",
    "                \n",
    "                if f1 > best_channel_f1:\n",
    "                    best_channel_f1 = f1\n",
    "                    best_channel_clf = clf_name\n",
    "            \n",
    "            print(f\"  ğŸ† Best: {best_channel_clf} (F1={best_channel_f1:.3f})\")\n",
    "            \n",
    "            analysis['channel'] = {\n",
    "                'results': channel_results,\n",
    "                'best_clf': best_channel_clf,\n",
    "                'best_f1': best_channel_f1\n",
    "            }\n",
    "        \n",
    "        # Patient-levelç»“æœ\n",
    "        if patient_results:\n",
    "            cv_results, all_preds = patient_results\n",
    "            print(f\"\\nğŸ” Patient-Level Results:\")\n",
    "            \n",
    "            best_patient_f1 = 0\n",
    "            best_patient_clf = None\n",
    "            patient_summary = {}\n",
    "            \n",
    "            for clf_name in self.classifiers.keys():\n",
    "                if len(cv_results[clf_name]) > 0:\n",
    "                    # CVç»Ÿè®¡\n",
    "                    f1_values = [fold['f1'] for fold in cv_results[clf_name]]\n",
    "                    f1_mean = np.mean(f1_values)\n",
    "                    f1_std = np.std(f1_values)\n",
    "                    \n",
    "                    # æ€»ä½“æŒ‡æ ‡\n",
    "                    y_true = np.array(all_preds[clf_name]['y_true'])\n",
    "                    y_pred = np.array(all_preds[clf_name]['y_pred'])\n",
    "                    overall_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "                    overall_acc = accuracy_score(y_true, y_pred)\n",
    "                    \n",
    "                    patient_summary[clf_name] = {\n",
    "                        'cv_f1_mean': f1_mean,\n",
    "                        'cv_f1_std': f1_std,\n",
    "                        'overall_f1': overall_f1,\n",
    "                        'overall_acc': overall_acc\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {clf_name}: CV F1={f1_mean:.3f}Â±{f1_std:.3f}, Overall F1={overall_f1:.3f}\")\n",
    "                    \n",
    "                    if overall_f1 > best_patient_f1:\n",
    "                        best_patient_f1 = overall_f1\n",
    "                        best_patient_clf = clf_name\n",
    "            \n",
    "            print(f\"  ğŸ† Best: {best_patient_clf} (F1={best_patient_f1:.3f})\")\n",
    "            \n",
    "            analysis['patient'] = {\n",
    "                'results': patient_summary,\n",
    "                'best_clf': best_patient_clf,\n",
    "                'best_f1': best_patient_f1\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_visualizations(self, analysis_results):\n",
    "        \"\"\"åˆ›å»ºå¯è§†åŒ–\"\"\"\n",
    "        print(f\"\\nğŸ“Š Creating visualizations...\")\n",
    "        \n",
    "        # 1. æ•°æ®é›†åˆ†å¸ƒ\n",
    "        self._plot_dataset_distribution()\n",
    "        \n",
    "        # 2. éªŒè¯å¯¹æ¯”\n",
    "        self._plot_validation_comparison(analysis_results)\n",
    "        \n",
    "        # 3. ç‰¹å¾é‡è¦æ€§\n",
    "        self._plot_feature_importance()\n",
    "        \n",
    "        # 4. æ‚£è€…ç‰¹å¾åˆ†å¸ƒ (æ–°å¢)\n",
    "        self._plot_patient_feature_distributions()\n",
    "        \n",
    "        print(f\"  ğŸ’¾ Saved to {self.output_folder}\")\n",
    "    \n",
    "    def _plot_patient_feature_distributions(self, selected_features=None, max_patients_per_plot=12):\n",
    "        \"\"\"\n",
    "        ä¸ºæ¯ä¸ªfeatureåˆ›å»ºå•ç‹¬çš„å¯è§†åŒ–å›¾ï¼Œæ¯ä¸ªå›¾åŒ…å«æ‰€æœ‰æ‚£è€…çš„è¯¥featureåˆ†å¸ƒ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        selected_features : list, optional\n",
    "            è¦å¯è§†åŒ–çš„ç‰¹å¾åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¾\n",
    "        max_patients_per_plot : int\n",
    "            æ¯ä¸ªå›¾æœ€å¤šæ˜¾ç¤ºçš„æ‚£è€…æ•°é‡\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"  ğŸ“Š Creating patient feature distributions...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            print(f\"    âŒ No channel data available\")\n",
    "            return\n",
    "        \n",
    "        # è·å–ç‰¹å¾åç§°\n",
    "        first_channel = list(self.channels_data.values())[0]\n",
    "        all_feature_names = list(first_channel['features'].keys())\n",
    "        \n",
    "        if selected_features is None:\n",
    "            # é€‰æ‹©ä¸€äº›ä»£è¡¨æ€§ç‰¹å¾è¿›è¡Œå¯è§†åŒ–\n",
    "            selected_features = [\n",
    "                'mean', 'std', 'rms', 'line_length', 'energy',\n",
    "                'rel_power_delta', 'rel_power_theta', 'rel_power_alpha', \n",
    "                'rel_power_beta', 'rel_power_gamma', 'rel_power_high_gamma',\n",
    "                'skewness', 'kurtosis'\n",
    "            ]\n",
    "            # åªä¿ç•™å®é™…å­˜åœ¨çš„ç‰¹å¾\n",
    "            selected_features = [f for f in selected_features if f in all_feature_names]\n",
    "        \n",
    "        # ç»„ç»‡æ•°æ®ï¼šæŒ‰patientå’Œmatter_typeåˆ†ç»„\n",
    "        patient_feature_data = {}\n",
    "        \n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            patient_id = ch_data['patient_id']\n",
    "            matter_type = ch_data['matter_type']\n",
    "            features = ch_data['features']\n",
    "            \n",
    "            if patient_id not in patient_feature_data:\n",
    "                patient_feature_data[patient_id] = {'grey': [], 'white': []}\n",
    "            \n",
    "            patient_feature_data[patient_id][matter_type].append(features)\n",
    "        \n",
    "        # è·å–æ‚£è€…åˆ—è¡¨å¹¶æ’åº\n",
    "        patients = sorted(list(patient_feature_data.keys()))\n",
    "        \n",
    "        # å¦‚æœæ‚£è€…å¤ªå¤šï¼Œåˆ†æ‰¹å¤„ç†\n",
    "        patient_batches = [patients[i:i+max_patients_per_plot] \n",
    "                          for i in range(0, len(patients), max_patients_per_plot)]\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªé€‰å®šçš„ç‰¹å¾åˆ›å»ºå¯è§†åŒ–\n",
    "        for feature_name in selected_features:\n",
    "            print(f\"    Creating visualization for feature: {feature_name}\")\n",
    "            \n",
    "            for batch_idx, patient_batch in enumerate(patient_batches):\n",
    "                # è®¡ç®—subplotå¸ƒå±€\n",
    "                n_patients = len(patient_batch)\n",
    "                n_cols = min(4, n_patients)  # æœ€å¤š4åˆ—\n",
    "                n_rows = (n_patients + n_cols - 1) // n_cols  # å‘ä¸Šå–æ•´\n",
    "                \n",
    "                # åˆ›å»ºfigure\n",
    "                fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "                \n",
    "                # ç¡®ä¿axesæ˜¯2Dæ•°ç»„\n",
    "                if n_rows == 1 and n_cols == 1:\n",
    "                    axes = np.array([[axes]])\n",
    "                elif n_rows == 1:\n",
    "                    axes = axes.reshape(1, -1)\n",
    "                elif n_cols == 1:\n",
    "                    axes = axes.reshape(-1, 1)\n",
    "                \n",
    "                # ä¸ºæ¯ä¸ªæ‚£è€…åˆ›å»ºsubplot\n",
    "                for i, patient_id in enumerate(patient_batch):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    ax = axes[row, col]\n",
    "                    \n",
    "                    # æå–è¯¥æ‚£è€…è¯¥ç‰¹å¾çš„æ•°æ®\n",
    "                    grey_values = []\n",
    "                    white_values = []\n",
    "                    \n",
    "                    for ch_features in patient_feature_data[patient_id]['grey']:\n",
    "                        if feature_name in ch_features:\n",
    "                            grey_values.append(ch_features[feature_name])\n",
    "                    \n",
    "                    for ch_features in patient_feature_data[patient_id]['white']:\n",
    "                        if feature_name in ch_features:\n",
    "                            white_values.append(ch_features[feature_name])\n",
    "                    \n",
    "                    # ç»˜åˆ¶åˆ†å¸ƒ\n",
    "                    if grey_values:\n",
    "                        ax.hist(grey_values, bins=15, alpha=0.6, label='Grey', color='red', density=True)\n",
    "                    if white_values:\n",
    "                        ax.hist(white_values, bins=15, alpha=0.6, label='White', color='blue', density=True)\n",
    "                    \n",
    "                    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "                    ax.set_title(f'{patient_id}\\n(G:{len(grey_values)}, W:{len(white_values)})', fontsize=10)\n",
    "                    ax.set_xlabel(f'{feature_name}', fontsize=8)\n",
    "                    ax.set_ylabel('Density', fontsize=8)\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # è®¾ç½®åˆé€‚çš„xè½´èŒƒå›´\n",
    "                    all_values = grey_values + white_values\n",
    "                    if all_values:\n",
    "                        # ä½¿ç”¨1-99ç™¾åˆ†ä½æ•°è®¾ç½®èŒƒå›´ï¼Œé¿å…æç«¯å€¼å½±å“\n",
    "                        p01, p99 = np.percentile(all_values, [1, 99])\n",
    "                        if p99 > p01:  # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ\n",
    "                            ax.set_xlim(p01, p99)\n",
    "                    \n",
    "                    # ç¾åŒ–åˆ»åº¦\n",
    "                    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "                \n",
    "                # éšè—å¤šä½™çš„subplots\n",
    "                for i in range(len(patient_batch), n_rows * n_cols):\n",
    "                    row = i // n_cols\n",
    "                    col = i % n_cols\n",
    "                    axes[row, col].set_visible(False)\n",
    "                \n",
    "                # è®¾ç½®æ•´ä½“æ ‡é¢˜\n",
    "                batch_suffix = f\"_batch{batch_idx+1}\" if len(patient_batches) > 1 else \"\"\n",
    "                fig.suptitle(f'Feature Distribution: {feature_name}{batch_suffix}', fontsize=16, y=0.98)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(top=0.93)  # ä¸ºsuptitleç•™å‡ºç©ºé—´\n",
    "                \n",
    "                # ä¿å­˜å›¾ç‰‡\n",
    "                safe_feature_name = feature_name.replace('/', '_').replace('\\\\', '_')\n",
    "                filename = f'feature_{safe_feature_name}{batch_suffix}.png'\n",
    "                plt.savefig(os.path.join(self.output_folder, filename), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "        \n",
    "        print(f\"    âœ… Created visualizations for {len(selected_features)} features\")\n",
    "    \n",
    "    def create_detailed_feature_visualization(self, features_to_plot=None):\n",
    "        \"\"\"\n",
    "        åˆ›å»ºè¯¦ç»†çš„ç‰¹å¾å¯è§†åŒ–ï¼ˆç‹¬ç«‹æ–¹æ³•ï¼Œå¯å•ç‹¬è°ƒç”¨ï¼‰\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_to_plot : list, optional\n",
    "            æŒ‡å®šè¦ç»˜åˆ¶çš„ç‰¹å¾åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ“Š Creating detailed feature visualizations...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            print(f\"âŒ No channel data available\")\n",
    "            return\n",
    "        \n",
    "        # å¦‚æœæ²¡æœ‰æŒ‡å®šç‰¹å¾ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾\n",
    "        if features_to_plot is None:\n",
    "            first_channel = list(self.channels_data.values())[0]\n",
    "            features_to_plot = list(first_channel['features'].keys())\n",
    "        \n",
    "        # åˆ›å»ºç‰¹å¾å¯è§†åŒ–å­æ–‡ä»¶å¤¹\n",
    "        feature_viz_folder = os.path.join(self.output_folder, 'feature_distributions')\n",
    "        os.makedirs(feature_viz_folder, exist_ok=True)\n",
    "        \n",
    "        # ä¸´æ—¶æ”¹å˜è¾“å‡ºæ–‡ä»¶å¤¹\n",
    "        original_folder = self.output_folder\n",
    "        self.output_folder = feature_viz_folder\n",
    "        \n",
    "        try:\n",
    "            self._plot_patient_feature_distributions(\n",
    "                selected_features=features_to_plot,\n",
    "                max_patients_per_plot=12\n",
    "            )\n",
    "        finally:\n",
    "            # æ¢å¤åŸå§‹è¾“å‡ºæ–‡ä»¶å¤¹\n",
    "            self.output_folder = original_folder\n",
    "        \n",
    "        print(f\"âœ… Feature visualizations saved to: {feature_viz_folder}\")\n",
    "    \n",
    "    def create_feature_summary_statistics(self):\n",
    "        \"\"\"åˆ›å»ºç‰¹å¾ç»Ÿè®¡æ‘˜è¦è¡¨\"\"\"\n",
    "        print(f\"\\nğŸ“Š Creating feature summary statistics...\")\n",
    "        \n",
    "        if not self.channels_data:\n",
    "            print(f\"âŒ No channel data available\")\n",
    "            return\n",
    "        \n",
    "        # æŒ‰patientå’Œmatter typeåˆ†ç»„ç»Ÿè®¡\n",
    "        summary_stats = []\n",
    "        \n",
    "        # è·å–ç‰¹å¾åç§°\n",
    "        first_channel = list(self.channels_data.values())[0]\n",
    "        feature_names = list(first_channel['features'].keys())\n",
    "        \n",
    "        # æŒ‰patientåˆ†ç»„\n",
    "        patient_groups = {}\n",
    "        for ch_id, ch_data in self.channels_data.items():\n",
    "            patient_id = ch_data['patient_id']\n",
    "            matter_type = ch_data['matter_type']\n",
    "            \n",
    "            if patient_id not in patient_groups:\n",
    "                patient_groups[patient_id] = {'grey': [], 'white': []}\n",
    "            \n",
    "            patient_groups[patient_id][matter_type].append(ch_data['features'])\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªpatientå’Œmatter typeè®¡ç®—ç»Ÿè®¡\n",
    "        for patient_id, matter_data in patient_groups.items():\n",
    "            for matter_type in ['grey', 'white']:\n",
    "                if matter_data[matter_type]:  # å¦‚æœæœ‰æ•°æ®\n",
    "                    for feature_name in feature_names:\n",
    "                        # æ”¶é›†è¯¥ç‰¹å¾çš„æ‰€æœ‰å€¼\n",
    "                        feature_values = [ch_features[feature_name] \n",
    "                                        for ch_features in matter_data[matter_type]\n",
    "                                        if feature_name in ch_features]\n",
    "                        \n",
    "                        if feature_values:\n",
    "                            summary_stats.append({\n",
    "                                'Patient': patient_id,\n",
    "                                'Matter_Type': matter_type,\n",
    "                                'Feature': feature_name,\n",
    "                                'N_Channels': len(feature_values),\n",
    "                                'Mean': np.mean(feature_values),\n",
    "                                'Std': np.std(feature_values),\n",
    "                                'Median': np.median(feature_values),\n",
    "                                'Min': np.min(feature_values),\n",
    "                                'Max': np.max(feature_values),\n",
    "                                'Q25': np.percentile(feature_values, 25),\n",
    "                                'Q75': np.percentile(feature_values, 75)\n",
    "                            })\n",
    "        \n",
    "        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦\n",
    "        if summary_stats:\n",
    "            summary_df = pd.DataFrame(summary_stats)\n",
    "            summary_df.to_csv(\n",
    "                os.path.join(self.output_folder, 'feature_summary_statistics.csv'),\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            print(f\"  âœ… Feature summary statistics saved to feature_summary_statistics.csv\")\n",
    "        else:\n",
    "            print(f\"  âŒ No statistics to save\")\n",
    "            \n",
    "    def _plot_dataset_distribution(self):\n",
    "        \"\"\"æ•°æ®é›†åˆ†å¸ƒå›¾\"\"\"\n",
    "        # ç»Ÿè®¡æ‚£è€…åˆ†å¸ƒ\n",
    "        patient_stats = {}\n",
    "        for ch_data in self.channels_data.values():\n",
    "            pid = ch_data['patient_id']\n",
    "            matter = ch_data['matter_type']\n",
    "            \n",
    "            if pid not in patient_stats:\n",
    "                patient_stats[pid] = {'grey': 0, 'white': 0}\n",
    "            patient_stats[pid][matter] += 1\n",
    "        \n",
    "        # ç»˜å›¾\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # æ‚£è€…åˆ†å¸ƒæ¡å½¢å›¾\n",
    "        patients = list(patient_stats.keys())\n",
    "        grey_counts = [patient_stats[p]['grey'] for p in patients]\n",
    "        white_counts = [patient_stats[p]['white'] for p in patients]\n",
    "        \n",
    "        x = np.arange(len(patients))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, grey_counts, width, label='Grey', color='red', alpha=0.7)\n",
    "        ax1.bar(x + width/2, white_counts, width, label='White', color='blue', alpha=0.7)\n",
    "        ax1.set_xlabel('Patients')\n",
    "        ax1.set_ylabel('Channel Count')\n",
    "        ax1.set_title('Channel Distribution by Patient')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(patients, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ€»ä½“é¥¼å›¾\n",
    "        total_grey = sum(grey_counts)\n",
    "        total_white = sum(white_counts)\n",
    "        \n",
    "        ax2.pie([total_grey, total_white], \n",
    "               labels=['Grey Matter', 'White Matter'],\n",
    "               colors=['red', 'blue'], \n",
    "               autopct='%1.1f%%')\n",
    "        ax2.set_title(f'Overall Distribution\\n({total_grey + total_white} channels)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'dataset_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_validation_comparison(self, analysis_results):\n",
    "        \"\"\"éªŒè¯æ–¹æ³•å¯¹æ¯”å›¾\"\"\"\n",
    "        channel_res = analysis_results.get('channel', {}).get('results', {})\n",
    "        patient_res = analysis_results.get('patient', {}).get('results', {})\n",
    "        \n",
    "        if not channel_res or not patient_res:\n",
    "            return\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        classifiers = list(self.classifiers.keys())\n",
    "        channel_f1s = [channel_res.get(clf, {}).get('f1', 0) for clf in classifiers]\n",
    "        patient_f1s = [patient_res.get(clf, {}).get('overall_f1', 0) for clf in classifiers]\n",
    "        \n",
    "        # ç»˜å›¾\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        x = np.arange(len(classifiers))\n",
    "        width = 0.35\n",
    "        \n",
    "        # F1å¯¹æ¯”\n",
    "        ax1.bar(x - width/2, channel_f1s, width, label='Channel-Level', alpha=0.7, color='green')\n",
    "        ax1.bar(x + width/2, patient_f1s, width, label='Patient-Level', alpha=0.7, color='orange')\n",
    "        ax1.set_xlabel('Classifiers')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('F1 Score Comparison')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(classifiers, rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # å·®å€¼å›¾\n",
    "        f1_diff = np.array(channel_f1s) - np.array(patient_f1s)\n",
    "        colors = ['green' if d > 0 else 'red' for d in f1_diff]\n",
    "        \n",
    "        ax2.bar(x, f1_diff, alpha=0.7, color=colors)\n",
    "        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax2.set_xlabel('Classifiers')\n",
    "        ax2.set_ylabel('F1 Difference (Channel - Patient)')\n",
    "        ax2.set_title('F1 Score Difference')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(classifiers, rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'validation_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_feature_importance(self):\n",
    "        \"\"\"ç‰¹å¾é‡è¦æ€§å›¾\"\"\"\n",
    "        X, y, _ = self._prepare_data()\n",
    "        if X is None:\n",
    "            return\n",
    "        \n",
    "        # ä½¿ç”¨éšæœºæ£®æ—è®¡ç®—é‡è¦æ€§\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        importances = rf.feature_importances_\n",
    "        feature_names = list(list(self.channels_data.values())[0]['features'].keys())\n",
    "        \n",
    "        # æ’åº\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # ç»˜å›¾\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(range(len(importances)), importances[indices], alpha=0.7)\n",
    "        plt.yticks(range(len(importances)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Feature Importance (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_folder, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def save_results(self, analysis_results, norm_method):\n",
    "        \"\"\"ä¿å­˜ç»“æœ\"\"\"\n",
    "        print(f\"\\nğŸ’¾ Saving results...\")\n",
    "        \n",
    "        # Channel-levelç»“æœ\n",
    "        channel_res = analysis_results.get('channel', {}).get('results', {})\n",
    "        if channel_res:\n",
    "            channel_data = []\n",
    "            for clf_name, metrics in channel_res.items():\n",
    "                channel_data.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'Validation': 'Channel_Level',\n",
    "                    'F1': metrics['f1'],\n",
    "                    'Accuracy': metrics['accuracy'],\n",
    "                    'Balanced_Acc': metrics['balanced_acc'],\n",
    "                    'ROC_AUC': metrics['roc_auc'],\n",
    "                    'Precision': metrics['precision'],\n",
    "                    'Recall': metrics['recall']\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(channel_data).to_csv(\n",
    "                os.path.join(self.output_folder, f'channel_results_{norm_method}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # Patient-levelç»“æœ\n",
    "        patient_res = analysis_results.get('patient', {}).get('results', {})\n",
    "        if patient_res:\n",
    "            patient_data = []\n",
    "            for clf_name, metrics in patient_res.items():\n",
    "                patient_data.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'Validation': 'Patient_Level',\n",
    "                    'CV_F1_Mean': metrics['cv_f1_mean'],\n",
    "                    'CV_F1_Std': metrics['cv_f1_std'],\n",
    "                    'Overall_F1': metrics['overall_f1'],\n",
    "                    'Overall_Acc': metrics['overall_acc']\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(patient_data).to_csv(\n",
    "                os.path.join(self.output_folder, f'patient_results_{norm_method}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # æ€»ç»“å¯¹æ¯”\n",
    "        summary_data = []\n",
    "        for clf_name in self.classifiers.keys():\n",
    "            row = {'Classifier': clf_name}\n",
    "            \n",
    "            if channel_res and clf_name in channel_res:\n",
    "                row['Channel_F1'] = channel_res[clf_name]['f1']\n",
    "                row['Channel_Acc'] = channel_res[clf_name]['accuracy']\n",
    "            else:\n",
    "                row['Channel_F1'] = 0\n",
    "                row['Channel_Acc'] = 0\n",
    "            \n",
    "            if patient_res and clf_name in patient_res:\n",
    "                row['Patient_F1'] = patient_res[clf_name]['overall_f1']\n",
    "                row['Patient_CV_F1'] = patient_res[clf_name]['cv_f1_mean']\n",
    "                row['Patient_CV_Std'] = patient_res[clf_name]['cv_f1_std']\n",
    "            else:\n",
    "                row['Patient_F1'] = 0\n",
    "                row['Patient_CV_F1'] = 0\n",
    "                row['Patient_CV_Std'] = 0\n",
    "            \n",
    "            summary_data.append(row)\n",
    "        \n",
    "        pd.DataFrame(summary_data).to_csv(\n",
    "            os.path.join(self.output_folder, f'summary_{norm_method}.csv'), \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # æ•°æ®é›†ä¿¡æ¯\n",
    "        dataset_info = {\n",
    "            'total_channels': len(self.channels_data),\n",
    "            'grey_channels': sum(1 for ch in self.channels_data.values() if ch['label'] == 1),\n",
    "            'white_channels': sum(1 for ch in self.channels_data.values() if ch['label'] == 0),\n",
    "            'n_patients': len(set(ch['patient_id'] for ch in self.channels_data.values())),\n",
    "            'normalization': norm_method\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.output_folder, 'dataset_info.txt'), 'w') as f:\n",
    "            for key, value in dataset_info.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        print(f\"  âœ… Results saved to {self.output_folder}\")\n",
    "    \n",
    "    def run_analysis(self, normalization='robust', outlier_clip=True, iqr_factor=1.5, \n",
    "                   create_feature_viz=True, features_to_visualize=None):\n",
    "        \"\"\"è¿è¡Œå®Œæ•´åˆ†æ\"\"\"\n",
    "        print(f\"ğŸ§  Channel-Level Grey/White Matter Classification\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 1. åŠ è½½æ•°æ®\n",
    "        if not self.load_patients():\n",
    "            print(\"âŒ Failed to load patients\")\n",
    "            return None\n",
    "        \n",
    "        # 2. æå–ç‰¹å¾\n",
    "        if not self.extract_all_features():\n",
    "            print(\"âŒ Failed to extract features\")\n",
    "            return None\n",
    "        \n",
    "        # 3. æ ‡å‡†åŒ–\n",
    "        if not self.normalize_by_patient(normalization):\n",
    "            print(\"âŒ Failed to normalize\")\n",
    "            return None\n",
    "        \n",
    "        # 4. Channel-leveléªŒè¯\n",
    "        channel_results = self.channel_level_validation(\n",
    "            outlier_clip=outlier_clip, \n",
    "            iqr_factor=iqr_factor\n",
    "        )\n",
    "        \n",
    "        # 5. Patient-leveléªŒè¯\n",
    "        patient_results = self.patient_level_validation(\n",
    "            outlier_clip=outlier_clip, \n",
    "            iqr_factor=iqr_factor\n",
    "        )\n",
    "        \n",
    "        # 6. åˆ†æç»“æœ\n",
    "        analysis_results = self.analyze_results(channel_results, patient_results)\n",
    "        \n",
    "        # 7. å¯è§†åŒ–\n",
    "        self.create_visualizations(analysis_results)\n",
    "        \n",
    "        # 8. è¯¦ç»†ç‰¹å¾å¯è§†åŒ– (å¯é€‰)\n",
    "        if create_feature_viz:\n",
    "            self.create_detailed_feature_visualization(features_to_visualize)\n",
    "            self.create_feature_summary_statistics()\n",
    "        \n",
    "        # 9. ä¿å­˜ç»“æœ\n",
    "        self.save_results(analysis_results, normalization)\n",
    "        \n",
    "        # 10. æ€»ç»“\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"âœ… Analysis Complete\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        n_channels = len(self.channels_data)\n",
    "        n_grey = sum(1 for ch in self.channels_data.values() if ch['label'] == 1)\n",
    "        n_white = sum(1 for ch in self.channels_data.values() if ch['label'] == 0)\n",
    "        n_patients = len(set(ch['patient_id'] for ch in self.channels_data.values()))\n",
    "        \n",
    "        print(f\"ğŸ“Š Dataset:\")\n",
    "        print(f\"   Patients: {n_patients}\")\n",
    "        print(f\"   Channels: {n_channels} ({n_grey} grey, {n_white} white)\")\n",
    "        print(f\"   Normalization: {normalization} (patient-level)\")\n",
    "        print(f\"   Outlier clipping: {'Yes' if outlier_clip else 'No'} (IQR Ã— {iqr_factor})\")\n",
    "        print(f\"   Feature visualization: {'Yes' if create_feature_viz else 'No'}\")\n",
    "        \n",
    "        # æœ€ä½³ç»“æœ\n",
    "        if analysis_results.get('channel', {}).get('best_clf'):\n",
    "            channel_best = analysis_results['channel']\n",
    "            print(f\"\\nğŸ† Best Channel-Level: {channel_best['best_clf']}\")\n",
    "            print(f\"   F1 Score: {channel_best['best_f1']:.3f}\")\n",
    "        \n",
    "        if analysis_results.get('patient', {}).get('best_clf'):\n",
    "            patient_best = analysis_results['patient']\n",
    "            print(f\"\\nğŸ† Best Patient-Level: {patient_best['best_clf']}\")\n",
    "            print(f\"   F1 Score: {patient_best['best_f1']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ Results: {self.output_folder}\")\n",
    "        if create_feature_viz:\n",
    "            print(f\"ğŸ“ Feature visualizations: {self.output_folder}/feature_distributions/\")\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "def create_feature_visualizations_only():\n",
    "    \"\"\"åªåˆ›å»ºç‰¹å¾å¯è§†åŒ–çš„ç¤ºä¾‹\"\"\"\n",
    "    classifier = ChannelClassifier(\n",
    "        processed_folder= r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_test\",\n",
    "        output_folder= r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\gwclassification_feature_viz_only_test\"\n",
    "    )\n",
    "    \n",
    "    # åŠ è½½æ•°æ®å’Œæå–ç‰¹å¾\n",
    "    classifier.load_patients()\n",
    "    classifier.extract_all_features()\n",
    "    classifier.normalize_by_patient('robust')\n",
    "    \n",
    "    # åªåˆ›å»ºç‰¹å¾å¯è§†åŒ–\n",
    "    classifier.create_detailed_feature_visualization()\n",
    "    \n",
    "    # åˆ›å»ºç»Ÿè®¡æ‘˜è¦\n",
    "    classifier.create_feature_summary_statistics()\n",
    "    \n",
    "    print(\"âœ… Feature visualizations created!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f06bf3b6db8cea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "create_feature_visualizations_only()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de0f20f56a5fca73",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "classifier = ChannelClassifier(\n",
    "    processed_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\data\\gwbaseline_alt\",\n",
    "    output_folder = r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\"\n",
    ")\n",
    "\n",
    "# è¿è¡Œåˆ†æ\n",
    "results = classifier.run_analysis(\n",
    "    normalization='standard',    # 'robust', 'standard', 'minmax'\n",
    "    outlier_clip=True,         # æ˜¯å¦è¿›è¡Œoutlier clipping\n",
    "    iqr_factor=1.5            # IQRå€æ•°\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(\"\\nğŸ¯ Quick Summary:\")\n",
    "    \n",
    "    # Channel-levelç»“æœ\n",
    "    if 'channel' in results and results['channel'].get('best_clf'):\n",
    "        channel_best = results['channel']\n",
    "        print(f\"Channel-Level Best: {channel_best['best_clf']} (F1={channel_best['best_f1']:.3f})\")\n",
    "    \n",
    "    # Patient-levelç»“æœ\n",
    "    if 'patient' in results and results['patient'].get('best_clf'):\n",
    "        patient_best = results['patient']\n",
    "        print(f\"Patient-Level Best: {patient_best['best_clf']} (F1={patient_best['best_f1']:.3f})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41b13b726bd5f41d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_psd_analysis_from_classifier(classifier, figsize=(20, 12)):\n",
    "    \"\"\"\n",
    "    ä»ChannelClassifierè¾“å‡ºç»˜åˆ¶PSDåˆ†æå›¾\n",
    "    \n",
    "    Args:\n",
    "        classifier: ChannelClassifierå¯¹è±¡ï¼ŒåŒ…å«channels_dataå’Œpatients_data\n",
    "        figsize: å›¾ç‰‡å¤§å°\n",
    "    \"\"\"\n",
    "    \n",
    "    # å®šä¹‰é¢‘æ®µå’Œé¢œè‰²\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4, '#90EE90'),      # æµ…ç»¿è‰²\n",
    "        'theta': (4, 8, '#FFA500'),        # æ©™è‰²  \n",
    "        'alpha': (8, 13, '#FFB6C1'),       # æµ…ç²‰è‰²\n",
    "        'beta': (13, 30, '#87CEEB'),       # å¤©è“è‰²\n",
    "        'low_gamma': (30, 60, '#DDA0DD'),  # æ¢…èŠ±è‰²\n",
    "        'high_gamma': (60, 100, '#F0E68C'), # å¡å…¶è‰²\n",
    "        'ripple': (100, 200, '#D3D3D3')    # æµ…ç°è‰²\n",
    "    }\n",
    "    \n",
    "    # ä»classifierè·å–æ•°æ®\n",
    "    channels_data = classifier.channels_data\n",
    "    patients_data = classifier.patients_data\n",
    "    \n",
    "    # è·å–å”¯ä¸€æ‚£è€…ID\n",
    "    patient_ids = list(set([ch['patient_id'] for ch in channels_data.values()]))\n",
    "    patient_ids = sorted(patient_ids)\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªæ‚£è€…éœ€è¦çš„å­å›¾æ•°é‡\n",
    "    n_patients = len(patient_ids)\n",
    "    n_cols = min(3, n_patients)  # æœ€å¤š3åˆ—\n",
    "    n_rows = (n_patients + n_cols - 1) // n_cols  # å‘ä¸Šå–æ•´\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_patients == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    plt.suptitle('Power Spectral Density Analysis by Patient', fontsize=16, y=0.95)\n",
    "    \n",
    "    for idx, patient_id in enumerate(patient_ids):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            ax = axes[col] if n_cols > 1 else axes[0]\n",
    "        else:\n",
    "            ax = axes[row, col]\n",
    "        \n",
    "        # è·å–è¯¥æ‚£è€…çš„æ‰€æœ‰channels\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        if not patient_channels:\n",
    "            ax.set_title(f'{patient_id}\\nNo data available')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        # åˆ†ç¦»ç°è´¨å’Œç™½è´¨channels\n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        # è·å–è¯¥æ‚£è€…çš„åŸå§‹æ•°æ®\n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            # åˆå¹¶æ‰€æœ‰recordingæ•°æ®\n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                # è®¡ç®—æ¯ä¸ªchannelçš„PSD\n",
    "                grey_psds = []\n",
    "                white_psds = []\n",
    "                freqs = None\n",
    "                \n",
    "                # ç»˜åˆ¶ä¸ªä½“channelçš„PSDï¼ˆæµ…è‰²ï¼‰\n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            ax.plot(f, psd, color='blue', alpha=0.1, linewidth=0.5)\n",
    "                            grey_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "                \n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            ax.plot(f, psd, color='red', alpha=0.1, linewidth=0.5)\n",
    "                            white_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "                \n",
    "                # è®¡ç®—å¹¶ç»˜åˆ¶å¹³å‡PSDï¼ˆç²—çº¿ï¼‰\n",
    "                if grey_psds and freqs is not None:\n",
    "                    grey_psds = np.vstack(grey_psds)\n",
    "                    grey_mean_psd = np.mean(grey_psds, axis=0)\n",
    "                    ax.plot(freqs, grey_mean_psd, color='blue', linewidth=3, \n",
    "                           label=f'Grey Matter (n={len(grey_psds)})', alpha=0.8)\n",
    "                \n",
    "                if white_psds and freqs is not None:\n",
    "                    white_psds = np.vstack(white_psds)\n",
    "                    white_mean_psd = np.mean(white_psds, axis=0)\n",
    "                    ax.plot(freqs, white_mean_psd, color='red', linewidth=3, \n",
    "                           label=f'White Matter (n={len(white_psds)})', alpha=0.8)\n",
    "        \n",
    "        # æ·»åŠ é¢‘æ®µèƒŒæ™¯\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        for band_name, (low_freq, high_freq, color) in freq_bands.items():\n",
    "            if high_freq <= 200:  # åªæ˜¾ç¤º200Hzä»¥ä¸‹çš„é¢‘æ®µ\n",
    "                ax.axvspan(low_freq, high_freq, alpha=0.2, color=color)\n",
    "                # åœ¨é¡¶éƒ¨æ·»åŠ é¢‘æ®µæ ‡ç­¾\n",
    "                mid_freq = (low_freq + high_freq) / 2\n",
    "                ax.text(mid_freq, y_max * 0.9, band_name, \n",
    "                       ha='center', va='center', fontsize=8, \n",
    "                       bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "        \n",
    "        # è®¾ç½®åæ ‡è½´\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('Power Spectral Density (log scale)')\n",
    "        ax.set_xlim(0, 150)  # é™åˆ¶åˆ°150Hz\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        n_grey = len(grey_channels)\n",
    "        n_white = len(white_channels)\n",
    "        ax.set_title(f'{patient_id}\\n(G:{n_grey}, W:{n_white})')\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for idx in range(n_patients, n_rows * n_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        if n_rows == 1:\n",
    "            axes[col].axis('off')\n",
    "        else:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_combined_patient_psd_from_classifier(classifier, figsize=(15, 8)):\n",
    "    \"\"\"\n",
    "    ä»ChannelClassifierç»˜åˆ¶æ‰€æœ‰æ‚£è€…åˆå¹¶çš„PSDå¯¹æ¯”å›¾\n",
    "    \n",
    "    Args:\n",
    "        classifier: ChannelClassifierå¯¹è±¡\n",
    "        figsize: å›¾ç‰‡å¤§å°\n",
    "    \"\"\"\n",
    "    \n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4, '#90EE90'),\n",
    "        'theta': (4, 8, '#FFA500'),\n",
    "        'alpha': (8, 13, '#FFB6C1'),\n",
    "        'beta': (13, 30, '#87CEEB'),\n",
    "        'low_gamma': (30, 60, '#DDA0DD'),\n",
    "        'high_gamma': (60, 100, '#F0E68C'),\n",
    "        'ripple': (100, 200, '#D3D3D3')\n",
    "    }\n",
    "    \n",
    "    # ä»classifierè·å–æ•°æ®\n",
    "    channels_data = classifier.channels_data\n",
    "    patients_data = classifier.patients_data\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    all_grey_psds = []\n",
    "    all_white_psds = []\n",
    "    freqs = None\n",
    "    \n",
    "    # è·å–å”¯ä¸€æ‚£è€…ID\n",
    "    patient_ids = list(set([ch['patient_id'] for ch in channels_data.values()]))\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        # è·å–è¯¥æ‚£è€…çš„channels\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        # è·å–åŸå§‹æ•°æ®\n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                # è®¡ç®—ç°è´¨channelsçš„PSD\n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_grey_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "                \n",
    "                # è®¡ç®—ç™½è´¨channelsçš„PSD\n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_white_psds.append(psd)\n",
    "                            if freqs is None:\n",
    "                                freqs = f\n",
    "    \n",
    "    # è®¡ç®—å¹¶ç»˜åˆ¶æ€»ä½“å¹³å‡PSD\n",
    "    if all_grey_psds and freqs is not None:\n",
    "        all_grey_psds = np.vstack(all_grey_psds)\n",
    "        grey_mean_psd = np.mean(all_grey_psds, axis=0)\n",
    "        grey_std_psd = np.std(all_grey_psds, axis=0)\n",
    "        \n",
    "        ax.plot(freqs, grey_mean_psd, color='blue', linewidth=3, \n",
    "               label=f'Grey Matter (n={len(all_grey_psds)})')\n",
    "        ax.fill_between(freqs, grey_mean_psd - grey_std_psd, \n",
    "                       grey_mean_psd + grey_std_psd, \n",
    "                       color='blue', alpha=0.2)\n",
    "    \n",
    "    if all_white_psds and freqs is not None:\n",
    "        all_white_psds = np.vstack(all_white_psds)\n",
    "        white_mean_psd = np.mean(all_white_psds, axis=0)\n",
    "        white_std_psd = np.std(all_white_psds, axis=0)\n",
    "        \n",
    "        ax.plot(freqs, white_mean_psd, color='red', linewidth=3, \n",
    "               label=f'White Matter (n={len(all_white_psds)})')\n",
    "        ax.fill_between(freqs, white_mean_psd - white_std_psd, \n",
    "                       white_mean_psd + white_std_psd, \n",
    "                       color='red', alpha=0.2)\n",
    "    \n",
    "    # æ·»åŠ é¢‘æ®µèƒŒæ™¯\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    for band_name, (low_freq, high_freq, color) in freq_bands.items():\n",
    "        if high_freq <= 150:\n",
    "            ax.axvspan(low_freq, high_freq, alpha=0.2, color=color)\n",
    "            mid_freq = (low_freq + high_freq) / 2\n",
    "            ax.text(mid_freq, y_max * 0.9, band_name, \n",
    "                   ha='center', va='center', fontsize=10, \n",
    "                   bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.7))\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Power Spectral Density (log scale)')\n",
    "    ax.set_xlim(0, 150)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_title('Average Power Spectral Density - All Patients Combined')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_psd_comparison_grid_from_classifier(classifier, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    ä»ChannelClassifierç»˜åˆ¶å®Œæ•´çš„PSDå¯¹æ¯”ç½‘æ ¼\n",
    "    \n",
    "    Args:\n",
    "        classifier: ChannelClassifierå¯¹è±¡\n",
    "        figsize: å›¾ç‰‡å¤§å°\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä»classifierè·å–æ•°æ®\n",
    "    channels_data = classifier.channels_data\n",
    "    patients_data = classifier.patients_data\n",
    "    \n",
    "    # è·å–å”¯ä¸€æ‚£è€…ID\n",
    "    patient_ids = list(set([ch['patient_id'] for ch in channels_data.values()]))\n",
    "    patient_ids = sorted(patient_ids)\n",
    "    n_patients = len(patient_ids)\n",
    "    \n",
    "    # è®¡ç®—ç½‘æ ¼å¸ƒå±€ï¼šæ‚£è€…å›¾ + 1ä¸ªæ€»ä½“å›¾\n",
    "    n_total_plots = n_patients + 1\n",
    "    n_cols = min(3, n_total_plots)\n",
    "    n_rows = (n_total_plots + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # åˆ›å»ºæ‚£è€…å­å›¾\n",
    "    for idx, patient_id in enumerate(patient_ids):\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        \n",
    "        # ç»˜åˆ¶å•ä¸ªæ‚£è€…çš„PSDï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                grey_psds = []\n",
    "                white_psds = []\n",
    "                \n",
    "                # è®¡ç®—å¹³å‡PSD\n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            grey_psds.append(psd)\n",
    "                \n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            white_psds.append(psd)\n",
    "                \n",
    "                # ç»˜åˆ¶å¹³å‡çº¿\n",
    "                if grey_psds:\n",
    "                    grey_mean_psd = np.mean(np.vstack(grey_psds), axis=0)\n",
    "                    ax.plot(f, grey_mean_psd, color='blue', linewidth=2, \n",
    "                           label=f'Grey (n={len(grey_psds)})')\n",
    "                \n",
    "                if white_psds:\n",
    "                    white_mean_psd = np.mean(np.vstack(white_psds), axis=0)\n",
    "                    ax.plot(f, white_mean_psd, color='red', linewidth=2, \n",
    "                           label=f'White (n={len(white_psds)})')\n",
    "        \n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlim(0, 150)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_title(f'{patient_id}')\n",
    "        \n",
    "        if idx >= (n_rows - 1) * n_cols:  # æœ€åä¸€è¡Œ\n",
    "            ax.set_xlabel('Frequency (Hz)')\n",
    "        if idx % n_cols == 0:  # ç¬¬ä¸€åˆ—\n",
    "            ax.set_ylabel('PSD (log)')\n",
    "    \n",
    "    # åˆ›å»ºæ€»ä½“å¯¹æ¯”å›¾\n",
    "    ax_combined = plt.subplot(n_rows, n_cols, n_patients + 1)\n",
    "    \n",
    "    # æ€»ä½“PSDè®¡ç®—\n",
    "    all_grey_psds = []\n",
    "    all_white_psds = []\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        patient_channels = {k: v for k, v in channels_data.items() \n",
    "                          if v['patient_id'] == patient_id}\n",
    "        \n",
    "        grey_channels = {k: v for k, v in patient_channels.items() if v['label'] == 1}\n",
    "        white_channels = {k: v for k, v in patient_channels.items() if v['label'] == 0}\n",
    "        \n",
    "        if patient_id in patients_data:\n",
    "            patient_data = patients_data[patient_id]\n",
    "            recordings = patient_data['recordings']\n",
    "            \n",
    "            all_data = []\n",
    "            fs = recordings[0]['sampling_rate'] if recordings else 512\n",
    "            \n",
    "            for recording in recordings:\n",
    "                data_segment = recording['neural_data_processed']\n",
    "                if isinstance(data_segment, list):\n",
    "                    all_data.extend(data_segment)\n",
    "                else:\n",
    "                    all_data.append(data_segment)\n",
    "            \n",
    "            if all_data:\n",
    "                combined_data = np.vstack(all_data)\n",
    "                \n",
    "                for ch_id, ch_info in grey_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_grey_psds.append(psd)\n",
    "                \n",
    "                for ch_id, ch_info in white_channels.items():\n",
    "                    electrode_idx = ch_info['electrode_idx']\n",
    "                    if electrode_idx < combined_data.shape[1]:\n",
    "                        signal = combined_data[:, electrode_idx]\n",
    "                        if len(signal) > 0 and not np.all(signal == 0):\n",
    "                            f, psd = welch(signal, fs=fs, nperseg=min(1024, len(signal)), \n",
    "                                         scaling='density')\n",
    "                            all_white_psds.append(psd)\n",
    "    \n",
    "    # ç»˜åˆ¶æ€»ä½“å¹³å‡\n",
    "    if all_grey_psds:\n",
    "        grey_mean_psd = np.mean(np.vstack(all_grey_psds), axis=0)\n",
    "        ax_combined.plot(f, grey_mean_psd, color='blue', linewidth=3, \n",
    "                        label=f'Grey Matter (n={len(all_grey_psds)})')\n",
    "    \n",
    "    if all_white_psds:\n",
    "        white_mean_psd = np.mean(np.vstack(all_white_psds), axis=0)\n",
    "        ax_combined.plot(f, white_mean_psd, color='red', linewidth=3, \n",
    "                        label=f'White Matter (n={len(all_white_psds)})')\n",
    "    \n",
    "    ax_combined.set_yscale('log')\n",
    "    ax_combined.set_xlim(0, 150)\n",
    "    ax_combined.grid(True, alpha=0.3)\n",
    "    ax_combined.legend()\n",
    "    ax_combined.set_title('All Patients Combined')\n",
    "    ax_combined.set_xlabel('Frequency (Hz)')\n",
    "    if (n_patients + 1 - 1) % n_cols == 0:\n",
    "        ax_combined.set_ylabel('PSD (log)')\n",
    "    \n",
    "    plt.suptitle('Power Spectral Density Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc4caf4339d278fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# æ–¹æ³•1: è¯¦ç»†çš„ä¸ªä½“æ‚£è€…PSDå›¾\n",
    "fig1 = plot_psd_analysis_from_classifier(classifier, figsize=(20, 12))\n",
    "fig1.savefig(r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\\psd_analysis_individual_patients.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# æ–¹æ³•2: æ‰€æœ‰æ‚£è€…åˆå¹¶çš„PSDå¯¹æ¯”å›¾\n",
    "fig2 = plot_combined_patient_psd_from_classifier(classifier, figsize=(15, 8))\n",
    "fig2.savefig(r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\\psd_analysis_combined_patients.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# æ–¹æ³•3: å®Œæ•´çš„ç½‘æ ¼å¸ƒå±€ï¼ˆæ¨èï¼‰\n",
    "fig3 = plot_psd_comparison_grid_from_classifier(classifier, figsize=(20, 15))\n",
    "fig3.savefig(r\"D:\\BlcRepo\\LabCode\\SeizureProp\\result\\multi_patient_results_channel_level\\psd_analysis_comparison_grid.png\", dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c161e4b120179054",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def plot_classification_results_comprehensive(channel_results, patient_results, \n",
    "                                            save_path=None, figsize=(16, 12)):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºåˆ†ç±»ç»“æœçš„ç»¼åˆå¯è§†åŒ–å›¾è¡¨\n",
    "    \n",
    "    Args:\n",
    "        channel_results: Channel-levelç»“æœå­—å…¸\n",
    "        patient_results: Patient-levelç»“æœï¼ˆåŒ…å«cv_resultså’Œall_predictionsï¼‰\n",
    "        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
    "        figsize: å›¾ç‰‡å°ºå¯¸\n",
    "    \n",
    "    Returns:\n",
    "        fig: matplotlib figureå¯¹è±¡\n",
    "    \"\"\"\n",
    "    \n",
    "    # è§£æpatient_results\n",
    "    if isinstance(patient_results, tuple) and len(patient_results) == 2:\n",
    "        cv_results, all_predictions = patient_results\n",
    "    else:\n",
    "        cv_results = patient_results\n",
    "        all_predictions = None\n",
    "    \n",
    "    # è®¾ç½®æ ·å¼\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # åˆ›å»º2x2å­å›¾å¸ƒå±€\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # å®šä¹‰é¢œè‰²\n",
    "    colors = {\n",
    "        'channel': '#3498db',  # è“è‰²\n",
    "        'accuracy': '#f39c12',  # æ©™è‰²\n",
    "        'roc': '#2ecc71',      # ç»¿è‰²\n",
    "        'patient': '#9b59b6'   # ç´«è‰²\n",
    "    }\n",
    "    \n",
    "    # è·å–åˆ†ç±»å™¨åç§°\n",
    "    classifiers = list(channel_results.keys()) if channel_results else []\n",
    "    \n",
    "    # 1. F1 Scoreå¯¹æ¯”ï¼ˆå·¦ä¸Šï¼‰\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    plot_f1_comparison(ax1, channel_results, cv_results, all_predictions, classifiers, colors)\n",
    "    \n",
    "    # 2. Accuracyå¯¹æ¯”ï¼ˆå³ä¸Šï¼‰\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    plot_accuracy_comparison(ax2, channel_results, classifiers, colors)\n",
    "    \n",
    "    # 3. ROC AUCå¯¹æ¯”ï¼ˆå·¦ä¸‹ï¼‰\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    plot_roc_comparison(ax3, channel_results, classifiers, colors)\n",
    "    \n",
    "    # 4. Patient-wiseç»“æœï¼ˆå³ä¸‹ï¼‰\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    plot_patient_wise_results(ax4, cv_results, colors)\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    \n",
    "    # ä¿å­˜å›¾ç‰‡\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"ğŸ“Š Results visualization saved to: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_f1_comparison(ax, channel_results, cv_results, all_predictions, classifiers, colors):\n",
    "    \"\"\"ç»˜åˆ¶F1 Scoreå¯¹æ¯”å›¾\"\"\"\n",
    "    \n",
    "    # æå–Channel-level F1 scores\n",
    "    channel_f1s = []\n",
    "    channel_stds = []\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        if clf in channel_results:\n",
    "            channel_f1s.append(channel_results[clf]['f1'])\n",
    "            channel_stds.append(0)  # Channel-levelæ²¡æœ‰std\n",
    "        else:\n",
    "            channel_f1s.append(0)\n",
    "            channel_stds.append(0)\n",
    "    \n",
    "    # æå–Patient-level F1 scores\n",
    "    patient_f1s = []\n",
    "    patient_stds = []\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        if clf in cv_results and len(cv_results[clf]) > 0:\n",
    "            f1_values = [fold['f1'] for fold in cv_results[clf]]\n",
    "            patient_f1s.append(np.mean(f1_values))\n",
    "            patient_stds.append(np.std(f1_values))\n",
    "        else:\n",
    "            patient_f1s.append(0)\n",
    "            patient_stds.append(0)\n",
    "    \n",
    "    # ç»˜åˆ¶æ¡å½¢å›¾\n",
    "    x = np.arange(len(classifiers))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, channel_f1s, width, \n",
    "                   label='Channel-Level', color=colors['channel'], alpha=0.8,\n",
    "                   yerr=channel_stds, capsize=5)\n",
    "    \n",
    "    bars2 = ax.bar(x + width/2, patient_f1s, width,\n",
    "                   label='Patient-Level', color=colors['patient'], alpha=0.8,\n",
    "                   yerr=patient_stds, capsize=5)\n",
    "    \n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('F1 Score by Classifier (with std)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(classifiers, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, max(max(channel_f1s), max(patient_f1s)) * 1.1)\n",
    "\n",
    "def plot_accuracy_comparison(ax, channel_results, classifiers, colors):\n",
    "    \"\"\"ç»˜åˆ¶Accuracyå¯¹æ¯”å›¾\"\"\"\n",
    "    \n",
    "    accuracies = []\n",
    "    for clf in classifiers:\n",
    "        if clf in channel_results:\n",
    "            accuracies.append(channel_results[clf]['accuracy'])\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "    \n",
    "    bars = ax.bar(classifiers, accuracies, color=colors['accuracy'], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy by Classifier')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        if acc > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "def plot_roc_comparison(ax, channel_results, classifiers, colors):\n",
    "    \"\"\"ç»˜åˆ¶ROC AUCå¯¹æ¯”å›¾\"\"\"\n",
    "    \n",
    "    roc_aucs = []\n",
    "    for clf in classifiers:\n",
    "        if clf in channel_results:\n",
    "            roc_aucs.append(channel_results[clf]['roc_auc'])\n",
    "        else:\n",
    "            roc_aucs.append(0.5)  # é»˜è®¤å€¼\n",
    "    \n",
    "    bars = ax.bar(classifiers, roc_aucs, color=colors['roc'], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Classifiers')\n",
    "    ax.set_ylabel('ROC AUC')\n",
    "    ax.set_title('ROC AUC by Classifier')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ åŸºå‡†çº¿\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_patient_wise_results(ax, cv_results, colors):\n",
    "    \"\"\"ç»˜åˆ¶Patient-wiseç»“æœ\"\"\"\n",
    "    \n",
    "    # æ‰¾åˆ°æœ€ä½³åˆ†ç±»å™¨ï¼ˆè¿™é‡Œä»¥Naive Bayesä¸ºä¾‹ï¼Œä½ å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰\n",
    "    best_clf = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for clf_name, folds in cv_results.items():\n",
    "        if len(folds) > 0:\n",
    "            avg_f1 = np.mean([fold['f1'] for fold in folds])\n",
    "            if avg_f1 > best_score:\n",
    "                best_score = avg_f1\n",
    "                best_clf = clf_name\n",
    "    \n",
    "    if best_clf and best_clf in cv_results:\n",
    "        # æå–æ¯ä¸ªæ‚£è€…ï¼ˆfoldï¼‰çš„ç»“æœ\n",
    "        folds = cv_results[best_clf]\n",
    "        patients = [fold['test_patient'] for fold in folds]\n",
    "        f1_scores = [fold['f1'] for fold in folds]\n",
    "        \n",
    "        bars = ax.bar(patients, f1_scores, color=colors['patient'], alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Patients')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.set_title(f'Patient-wise F1 Score ({best_clf})')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # æ·»åŠ å¹³å‡çº¿\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        ax.axhline(y=mean_f1, color='red', linestyle='--', alpha=0.7, \n",
    "                  label=f'Mean: {mean_f1:.3f}')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No patient-wise results available', \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Patient-wise F1 Score')\n",
    "\n",
    "def plot_classification_results_simple(analysis_results, save_path=None, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–ç‰ˆæœ¬çš„ç»“æœå¯è§†åŒ–ï¼ˆé€‚ç”¨äºanalyzer.analyze_resultsçš„è¾“å‡ºï¼‰\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: analyzer.analyze_results()çš„è¾“å‡º\n",
    "        save_path: ä¿å­˜è·¯å¾„\n",
    "        figsize: å›¾ç‰‡å°ºå¯¸\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_results = analysis_results.get('channel', {}).get('results', {})\n",
    "    patient_summary = analysis_results.get('patient', {}).get('results', {})\n",
    "    \n",
    "    # åˆ›å»º2x2å­å›¾\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    classifiers = list(channel_results.keys())\n",
    "    colors = ['#3498db', '#f39c12', '#2ecc71', '#9b59b6', '#e74c3c', '#1abc9c', '#f1c40f']\n",
    "    \n",
    "    # 1. F1 Scoreå¯¹æ¯”\n",
    "    if channel_results and patient_summary:\n",
    "        channel_f1s = [channel_results[clf]['f1'] for clf in classifiers]\n",
    "        patient_f1s = [patient_summary[clf]['overall_f1'] if clf in patient_summary else 0 \n",
    "                      for clf in classifiers]\n",
    "        patient_stds = [patient_summary[clf]['cv_f1_std'] if clf in patient_summary else 0 \n",
    "                       for clf in classifiers]\n",
    "        \n",
    "        x = np.arange(len(classifiers))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, channel_f1s, width, label='Channel-Level', \n",
    "               color='#3498db', alpha=0.8)\n",
    "        ax1.bar(x + width/2, patient_f1s, width, label='Patient-Level', \n",
    "               color='#9b59b6', alpha=0.8, yerr=patient_stds, capsize=5)\n",
    "        \n",
    "        ax1.set_xlabel('Classifiers')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('F1 Score by Classifier (with std)')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(classifiers, rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy\n",
    "    if channel_results:\n",
    "        accuracies = [channel_results[clf]['accuracy'] for clf in classifiers]\n",
    "        bars = ax2.bar(classifiers, accuracies, color='#f39c12', alpha=0.8)\n",
    "        ax2.set_xlabel('Classifiers')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy by Classifier')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # 3. ROC AUC\n",
    "    if channel_results:\n",
    "        roc_aucs = [channel_results[clf]['roc_auc'] for clf in classifiers]\n",
    "        ax3.bar(classifiers, roc_aucs, color='#2ecc71', alpha=0.8)\n",
    "        ax3.set_xlabel('Classifiers')\n",
    "        ax3.set_ylabel('ROC AUC')\n",
    "        ax3.set_title('ROC AUC by Classifier')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        ax3.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 4. æ¨¡å‹å¯¹æ¯”é›·è¾¾å›¾\n",
    "    if channel_results:\n",
    "        metrics = ['F1', 'Accuracy', 'Precision', 'Recall', 'ROC AUC']\n",
    "        \n",
    "        # é€‰æ‹©æœ€ä½³æ¨¡å‹æ˜¾ç¤º\n",
    "        best_clf = max(channel_results.keys(), \n",
    "                      key=lambda x: channel_results[x]['f1'])\n",
    "        \n",
    "        values = [\n",
    "            channel_results[best_clf]['f1'],\n",
    "            channel_results[best_clf]['accuracy'],\n",
    "            channel_results[best_clf]['precision'],\n",
    "            channel_results[best_clf]['recall'],\n",
    "            channel_results[best_clf]['roc_auc']\n",
    "        ]\n",
    "        \n",
    "        # ç®€å•çš„æ¡å½¢å›¾ä»£æ›¿é›·è¾¾å›¾\n",
    "        ax4.barh(metrics, values, color='#e74c3c', alpha=0.8)\n",
    "        ax4.set_xlabel('Score')\n",
    "        ax4.set_title(f'Best Model Performance ({best_clf})')\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"ğŸ“Š Results visualization saved to: {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_summary_table(analysis_results, save_path=None):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç»“æœæ±‡æ€»è¡¨\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: åˆ†æç»“æœå­—å…¸\n",
    "        save_path: ä¿å­˜è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: ç»“æœæ±‡æ€»è¡¨\n",
    "    \"\"\"\n",
    "    \n",
    "    channel_results = analysis_results.get('channel', {}).get('results', {})\n",
    "    patient_results = analysis_results.get('patient', {}).get('results', {})\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for clf_name in channel_results.keys():\n",
    "        row = {\n",
    "            'Classifier': clf_name,\n",
    "            'Channel_F1': channel_results[clf_name]['f1'],\n",
    "            'Channel_Accuracy': channel_results[clf_name]['accuracy'],\n",
    "            'Channel_Precision': channel_results[clf_name]['precision'],\n",
    "            'Channel_Recall': channel_results[clf_name]['recall'],\n",
    "            'Channel_ROC_AUC': channel_results[clf_name]['roc_auc'],\n",
    "            'Channel_Balanced_Acc': channel_results[clf_name]['balanced_acc']\n",
    "        }\n",
    "        \n",
    "        if clf_name in patient_results:\n",
    "            row.update({\n",
    "                'Patient_F1_Mean': patient_results[clf_name]['cv_f1_mean'],\n",
    "                'Patient_F1_Std': patient_results[clf_name]['cv_f1_std'],\n",
    "                'Patient_Overall_F1': patient_results[clf_name]['overall_f1'],\n",
    "                'Patient_Overall_Acc': patient_results[clf_name]['overall_acc']\n",
    "            })\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if save_path:\n",
    "        summary_df.to_csv(save_path, index=False)\n",
    "        print(f\"ğŸ“‹ Summary table saved to: {save_path}\")\n",
    "    \n",
    "    return summary_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc025c670df4ca6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "channel_results = classifier.channel_level_validation()\n",
    "patient_results = classifier.patient_level_validation()\n",
    "analysis_results = classifier.analyze_results(channel_results, patient_results)\n",
    "\n",
    "fig2 = plot_classification_results_simple(\n",
    "    analysis_results,\n",
    "    save_path='classification_results_simple.png'\n",
    ")\n",
    "fig = plot_classification_results_comprehensive(\n",
    "    channel_results, patient_results,\n",
    "    save_path='detailed_results.png'\n",
    ")\n",
    "summary_table = create_summary_table(\n",
    "    analysis_results,\n",
    "    save_path='results_summary.csv'\n",
    ")\n",
    "\n",
    "print(summary_table)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db2b49b35507f239",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2871b03135502f0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
