import torch.nn.functional as F
import torch.optim as optim
import itertools
import numpy as np
from torch.nn import Module
from torch.utils.data import DataLoader
import logging
from optuna.visualization import plot_optimization_history, plot_param_importances
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import math
import torch
import torch.nn as nn
import optuna
import os
from tqdm import tqdm
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any, Callable
import json
from datetime import datetime

logger = logging.getLogger(__name__)

class BaseModel(nn.Module):
    """Base class for all models with common functionality."""

    def __init__(self, input_dim: int, output_dim: int, device: str = 'cuda:0'):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.device = device

        # Validate device
        if device.startswith('cuda') and not torch.cuda.is_available():
            logger.warning("CUDA not available, falling back to CPU")
            self.device = 'cpu'

        self.criteria = nn.CrossEntropyLoss()

    def random_init(self):
        """Initialize weights using appropriate initialization methods."""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'lstm' in name:
                    nn.init.orthogonal_(param)
                elif 'conv' in name:
                    nn.init.kaiming_normal_(param)
                else:
                    nn.init.xavier_normal_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)

    def configure_optimizer(self, lr: float = 0.001, weight_decay: float = 1e-5):
        """Configure optimizer with parameters."""
        self.optimizer = optim.Adam(
            self.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )


class LSTM(BaseModel):
    def __init__(self, input_dim: int, output_dim: int,
                 lr = 0.001, weight_decay=1e-5,
                 hidden_dim: int = 50, dropout: float = 0.2,
                 num_layers: int = 1, device: str = 'cuda:0'):
        super().__init__(input_dim, output_dim, device)

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.dropout1 = nn.Dropout(dropout)
        self.fc1 = nn.Linear(hidden_dim, 8)
        self.dropout2 = nn.Dropout(dropout)
        self.flatten = nn.Flatten()
        self.fc2 = nn.Linear(8 * 23, output_dim)  # Note: 23 should be calculated based on input

        self.criteria = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)

        self.to(device)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Handle input shape
        if x.shape[-1] != self.input_dim:
            x = x.transpose(1, 2)

        x = x.to(self.device)

        # LSTM layer
        lstm_out, _ = self.lstm(x)
        lstm_out = F.relu(lstm_out)
        lstm_out = self.dropout1(lstm_out)

        # Fully connected layers
        output = F.relu(self.fc1(lstm_out))
        output = self.dropout2(output)
        output = self.flatten(output)
        output = F.softmax(self.fc2(output), dim=1)

        return output


class CNN1D(BaseModel):
    def __init__(self, input_dim: int, output_dim: int,
                 lr=0.001, weight_decay=1e-5,
                 hidden_dim: int = 128, dropout: float = 0.2,
                 kernel_size: int = 256, device: str = 'cuda:0'):
        super().__init__(input_dim, output_dim, device)

        self.conv_layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(
                    in_channels=input_dim if i == 0 else hidden_dim // (2 ** (i - 1)),
                    out_channels=hidden_dim // (2 ** i),
                    kernel_size=kernel_size // (2 ** i),
                    padding='same'
                ),
                nn.MaxPool1d(2),
                nn.Dropout(dropout)
            ) for i in range(3)  # 3 conv layers
        ])

        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear((hidden_dim // 4) * (kernel_size // 8), hidden_dim // 8)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_dim // 8, output_dim)

        # Global average pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1).to('cuda:0')

        self.criteria = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)

        self.to(device)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.to(self.device)

        # Convolutional layers
        for conv_layer in self.conv_layers:
            x = F.relu(conv_layer(x))

        # Fully connected layers
        x = self.global_pool(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.softmax(self.fc2(x), dim=1)

        return x


class Wavenet(nn.Module):
    def __init__(self, input_dim, output_dim, lr=0.001, hidden_dim=128, weight_decay=1e-5,
                 dropout=0.2, kernel_size=32, dilation=1):
        super(Wavenet, self).__init__()

        self.padding1 = (kernel_size - 1) * dilation
        self.padding2 = (kernel_size // 2 - 1) * dilation
        self.padding3 = (kernel_size // 4 - 1) * dilation

        # Convolutional layers
        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=kernel_size, padding=0, dilation=dilation).to(
            'cuda:0')
        self.maxpool1 = nn.MaxPool1d(2).to('cuda:0')
        self.dropout1 = nn.Dropout(dropout).to('cuda:0')

        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim // 2, kernel_size=kernel_size // 2, padding=0,
                               dilation=dilation).to('cuda:0')
        self.maxpool2 = nn.MaxPool1d(2).to('cuda:0')
        self.dropout2 = nn.Dropout(dropout).to('cuda:0')

        self.conv3 = nn.Conv1d(hidden_dim // 2, hidden_dim // 4, kernel_size=kernel_size // 4, padding=0,
                               dilation=dilation).to('cuda:0')
        self.maxpool3 = nn.MaxPool1d(2).to('cuda:0')
        self.dropout3 = nn.Dropout(dropout).to('cuda:0')

        # Global average pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1).to('cuda:0')

        # FC layers with fixed size due to global pooling
        self.fc1 = nn.Linear(hidden_dim // 4, hidden_dim // 8).to('cuda:0')
        self.dropout4 = nn.Dropout(dropout).to('cuda:0')
        self.fc2 = nn.Linear(hidden_dim // 8, output_dim).to('cuda:0')

        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)
        self.criteria = nn.CrossEntropyLoss()

    def forward(self, x):
        x = x.to('cuda:0')

        # Convolutional layers
        x = F.pad(x, (self.padding1, 0))
        x = F.relu(self.conv1(x))
        x = self.maxpool1(x)
        x = self.dropout1(x)

        x = F.pad(x, (self.padding2, 0))
        x = F.relu(self.conv2(x))
        x = self.maxpool2(x)
        x = self.dropout2(x)

        x = F.pad(x, (self.padding3, 0))
        x = F.relu(self.conv3(x))
        x = self.maxpool3(x)
        x = self.dropout3(x)

        # Global average pooling
        x = self.global_pool(x)
        x = x.squeeze(-1)

        # FC layers
        x = F.relu(self.fc1(x))
        x = self.dropout4(x)
        x = F.softmax(self.fc2(x), dim=1)

        return x

    def random_init(self):
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.uniform_(param.data, -0.1, 0.1)
            elif 'bias' in name:
                nn.init.zeros_(param.data)

    def predict(self, x):
        self.eval()
        x = x.to('cuda:0')
        output = self(x)
        return output


class ResNet(nn.Module):
    def __init__(
            self,
            input_dim=12,  # Number of input features
            base_filters=32,
            n_blocks=3,
            kernel_size=16,
            n_classes=2,
            dropout=0.2,
            lr=0.001,
            weight_decay=1e-5,
            norm_type='batch'
    ):
        super().__init__()

        self.norm_layer = nn.BatchNorm1d if norm_type == 'batch' else nn.LayerNorm

        # Initial feature embedding layer to process all features together
        self.feature_embedding = nn.Linear(input_dim, base_filters)

        # Initial causal conv - works on the time dimension after feature embedding
        self.initial = nn.Sequential(
            nn.Conv1d(base_filters, base_filters, kernel_size),
            self.norm_layer(base_filters),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Stack of dilated convolution blocks
        self.blocks = nn.ModuleList()
        for i in range(n_blocks):
            dilation = 2 ** (i % 8)
            self.blocks.append(nn.ModuleDict({
                'dilated_conv': nn.Conv1d(
                    base_filters,
                    base_filters * 2,
                    kernel_size,
                    dilation=dilation,
                ),
                'norm': self.norm_layer(base_filters),
                'res_conv': nn.Conv1d(base_filters, base_filters, 1),
                'dropout': nn.Dropout(dropout)
            }))

        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(base_filters, n_classes)

        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)
        self.criteria = nn.CrossEntropyLoss()

    def forward(self, x):
        """
        Input: (batch_size, features, time_steps)
        Output: (batch_size, n_classes)
        """
        batch_size, n_features, time_steps = x.shape

        # Reshape and transpose to apply feature embedding
        # From [batch, features, time] to [batch * time, features]
        x = x.permute(0, 2, 1).reshape(batch_size * time_steps, n_features)

        # Apply feature embedding
        x = self.feature_embedding(x)

        # Reshape back to [batch, time, base_filters]
        x = x.reshape(batch_size, time_steps, -1)

        # Permute to [batch, base_filters, time] for conv1d operations
        x = x.permute(0, 2, 1)

        # Apply causal padding for initial conv
        x = F.pad(x, (1, 0))
        x = self.initial(x)

        # Process blocks and accumulate skip connections directly
        skip_sum = 0
        for block in self.blocks:
            residual = x

            # Causal padding
            padding = block['dilated_conv'].dilation[0] * (block['dilated_conv'].kernel_size[0] - 1)
            x = F.pad(x, (padding, 0))

            # Dilated conv and gating
            x = block['dilated_conv'](x)
            filter_x, gate_x = torch.chunk(x, 2, dim=1)
            x = torch.tanh(filter_x) * torch.sigmoid(gate_x)

            # Residual connection
            x = block['res_conv'](x)
            x = block['norm'](x)
            x = block['dropout'](x)
            x = x + residual

            # Add to skip connections sum directly
            skip_sum = skip_sum + x

        # Final processing
        x = self.global_pool(skip_sum)
        x = x.squeeze(-1)
        x = self.fc(x)

        x = F.softmax(x, dim=1)

        return x

    def random_init(self):
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.uniform_(param.data, -0.1, 0.1)
            elif 'bias' in name:
                nn.init.zeros_(param.data)

    def predict(self, x):
        self.eval()
        x = x.to('cuda:0')
        output = self(x)
        return output


class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding as in 'Attention is All You Need'."""
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class MultiHeadedAttention(nn.Module):
    """Multi-head attention mechanism."""
    def __init__(self, h, d_model, dropout=0.1):
        super().__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        nbatches = query.size(0)
        # 1) Do all the linear projections in batch
        query, key, value = [
            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linears, (query, key, value))
        ]
        # 2) Apply attention on all the projected vectors in batch
        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)
        # 3) Concatenate and apply final linear
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)

    @staticmethod
    def attention(query, key, value, mask=None, dropout=None):
        d_k = query.size(-1)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        p_attn = F.softmax(scores, dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn


class PositionwiseFeedForward(nn.Module):
    """Position-wise feed-forward network."""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))


class SublayerConnection(nn.Module):
    """Residual connection followed by layer norm."""
    def __init__(self, size, dropout):
        super().__init__()
        self.norm = nn.LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))


class EncoderLayer(nn.Module):
    """Encoder layer with self-attention and feed-forward."""
    def __init__(self, size, self_attn, feed_forward, dropout):
        super().__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = nn.ModuleList([SublayerConnection(size, dropout) for _ in range(2)])
        self.size = size

    def forward(self, x, mask=None):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)


class Encoder(nn.Module):
    """Stack of N encoder layers."""
    def __init__(self, layer, N):
        super().__init__()
        self.layers = nn.ModuleList([layer for _ in range(N)])
        self.norm = nn.LayerNorm(layer.size)

    def forward(self, x, mask=None):
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)


class CustomTransformerSeizurePredictor(BaseModel):
    """Transformer for seizure channel prediction (encoder only, no decoder)."""
    def __init__(self, input_dim, output_dim=2, d_model=512, n_heads=8, n_layers=6, d_ff=2048, dropout=0.1, max_seq_len=1000, lr=0.0001, weight_decay=1e-5, device='cuda:0'):
        super().__init__(input_dim, output_dim, device)
        self.input_projection = nn.Linear(input_dim, d_model)
        self.pos_encoding = PositionalEncoding(d_model, dropout, max_seq_len)
        attn = MultiHeadedAttention(n_heads, d_model, dropout)
        ff = PositionwiseFeedForward(d_model, d_ff, dropout)
        encoder_layer = EncoderLayer(d_model, attn, ff, dropout)
        self.encoder = Encoder(encoder_layer, n_layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, output_dim)
        )
        self.criteria = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)
        self.to(device)

    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        x = x.to(self.device)
        # If input is (batch, channels, seq_len), transpose to (batch, seq_len, channels)
        if len(x.shape) == 3 and x.shape[1] < x.shape[2]:
            x = x.transpose(1, 2)
        x = self.input_projection(x)
        x = self.pos_encoding(x)
        x = self.encoder(x)
        x = x.transpose(1, 2)  # (batch, d_model, seq_len)
        x = self.global_pool(x)
        x = x.squeeze(-1)  # (batch, d_model)
        x = self.classifier(x)
        return F.softmax(x, dim=1)

    def random_init(self):
        for name, param in self.named_parameters():
            if param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)


class EarlyStopping:
    """Early stopping handler"""

    def __init__(self, patience: int = 7, min_delta: float = 0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.should_stop = False

    def __call__(self, val_loss: float) -> bool:
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return self.should_stop


def detect_model_type(model):
    """
    æ£€æµ‹æ¨¡å‹ç±»å‹ä»¥ç¡®å®šä½¿ç”¨å“ªç§æŸå¤±å‡½æ•°å’Œè¯„ä¼°æ–¹æ³•
    """
    model_name = model.__class__.__name__
    if 'Transformer' in model_name or 'CustomTransformer' in model_name:
        return 'transformer'
    else:
        return 'standard'


def universal_evaluate_model(model, dataloader, device='cuda', model_type=None):
    """
    é€šç”¨æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼Œå…¼å®¹æ‰€æœ‰æ¨¡å‹ç±»å‹

    Args:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        model_type: æ¨¡å‹ç±»å‹ ('transformer' æˆ– 'standard')

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        avg_accuracy: å¹³å‡å‡†ç¡®ç‡
    """
    if model_type is None:
        model_type = detect_model_type(model)

    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0

    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æŸå¤±å‡½æ•°
    if model_type == 'transformer':
        criterion = nn.KLDivLoss(reduction='batchmean')
    else:
        criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for batch_data in dataloader:
            try:
                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if isinstance(batch_data, dict):
                    # å­—å…¸æ ¼å¼ (å¦‚EnhancedResNet)
                    data = batch_data['data'].float().to(device)
                    labels = batch_data['label'].long().to(device)
                elif isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:
                    # å…ƒç»„æ ¼å¼ (æ ‡å‡†æ ¼å¼)
                    data, labels = batch_data
                    data = data.float().to(device)
                    labels = labels.to(device)
                else:
                    # ç›´æ¥æ˜¯æ•°æ®
                    data = batch_data.float().to(device)
                    labels = None

                # å‰å‘ä¼ æ’­
                outputs = model(data)

                if labels is not None:
                    if model_type == 'transformer':
                        # Transformerä½¿ç”¨KLæ•£åº¦æŸå¤±
                        if labels.dtype == torch.long:
                            # å¦‚æœæ ‡ç­¾æ˜¯longç±»å‹ï¼Œè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                            labels_one_hot = torch.zeros_like(outputs)
                            labels_one_hot.scatter_(1, labels.unsqueeze(1), 1.0)
                            labels = labels_one_hot.float()

                        # ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒ
                        outputs = torch.clamp(outputs, min=1e-8, max=1.0)
                        outputs = outputs / outputs.sum(dim=1, keepdim=True)
                        log_outputs = torch.log(outputs)

                        loss = criterion(log_outputs, labels)
                    else:
                        # æ ‡å‡†æ¨¡å‹ä½¿ç”¨äº¤å‰ç†µæŸå¤±
                        if labels.dtype == torch.float and labels.shape[1] > 1:
                            # æ¦‚ç‡åˆ†å¸ƒæ ‡ç­¾è½¬æ¢ä¸ºç±»åˆ«æ ‡ç­¾
                            labels = torch.argmax(labels, dim=1)
                        loss = criterion(outputs, labels)

                    total_loss += loss.item()

                    # è®¡ç®—å‡†ç¡®ç‡
                    if model_type == 'transformer' and labels.shape[1] > 1:
                        # Transformerçš„æ¦‚ç‡åˆ†å¸ƒæ ‡ç­¾
                        predicted = torch.argmax(outputs, dim=1)
                        true_labels = torch.argmax(labels, dim=1)
                    else:
                        # æ ‡å‡†æ ‡ç­¾
                        predicted = torch.argmax(outputs, dim=1)
                        true_labels = labels

                    total += labels.size(0)
                    correct += (predicted == true_labels).sum().item()

            except Exception as e:
                logger.warning(f"Error in evaluation batch: {str(e)}")
                continue

    if total > 0:
        avg_loss = total_loss / len(dataloader)
        avg_accuracy = correct / total
        return avg_loss, avg_accuracy
    else:
        return float('inf'), 0.0


def universal_train_model(
        model,
        train_loader,
        val_loader,
        save_location: Optional[str] = None,
        epochs: int = 50,
        device: str = 'cuda',
        patience: int = 7,
        scheduler_patience: int = 5,
        gradient_clip: Optional[float] = 1.0,
        checkpoint_freq: int = 10,
        trial: Optional[optuna.Trial] = None,
        model_type: Optional[str] = None
) -> Tuple[List[float], List[float], List[float]]:
    """
    é€šç”¨è®­ç»ƒå‡½æ•°ï¼Œå…¼å®¹æ‰€æœ‰æ¨¡å‹ç±»å‹

    Args:
        model: è¦è®­ç»ƒçš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        save_location: æ¨¡å‹ä¿å­˜ä½ç½®
        epochs: è®­ç»ƒè½®æ•°
        device: è®¾å¤‡
        patience: æ—©åœè€å¿ƒå€¼
        scheduler_patience: å­¦ä¹ ç‡è°ƒåº¦å™¨è€å¿ƒå€¼
        gradient_clip: æ¢¯åº¦è£å‰ª
        checkpoint_freq: æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
        trial: Optunaè¯•éªŒå¯¹è±¡ï¼ˆç”¨äºå‰ªæï¼‰
        model_type: æ¨¡å‹ç±»å‹

    Returns:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        val_accuracies: éªŒè¯å‡†ç¡®ç‡åˆ—è¡¨
    """
    # æ£€æµ‹æ¨¡å‹ç±»å‹
    if model_type is None:
        model_type = detect_model_type(model)

    # è®¾ç½®è®¾å¤‡
    if device.startswith('cuda') and not torch.cuda.is_available():
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
        device = 'cpu'
    device = torch.device(device)
    model = model.to(device)

    # è·å–ä¼˜åŒ–å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„ï¼‰
    if hasattr(model, 'optimizer') and model.optimizer is not None:
        optimizer = model.optimizer
    else:
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æŸå¤±å‡½æ•°
    if model_type == 'transformer':
        criterion = nn.KLDivLoss(reduction='batchmean')
    else:
        criterion = nn.CrossEntropyLoss()

    # åˆå§‹åŒ–è·Ÿè¸ªå˜é‡
    train_losses = []
    val_losses = []
    val_accuracies = []
    best_val_acc = 0.0

    # æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    early_stopping = EarlyStopping(patience=patience)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=scheduler_patience, verbose=True
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    if save_location:
        save_location = Path(save_location)
        save_location.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        num_batches = 0

        # æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆOptunaè¯•éªŒæ—¶ç¦ç”¨ï¼‰
        disable_pbar = trial is not None
        pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}',
                    disable=disable_pbar, leave=False)

        for batch_idx, batch_data in enumerate(pbar):
            try:
                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if isinstance(batch_data, dict):
                    # å­—å…¸æ ¼å¼
                    data = batch_data['data'].float().to(device)
                    labels = batch_data['label'].to(device)
                elif isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:
                    # å…ƒç»„æ ¼å¼
                    data, labels = batch_data
                    data = data.float().to(device)
                    labels = labels.to(device)
                else:
                    logger.warning(f"æœªçŸ¥çš„æ‰¹æ¬¡æ•°æ®æ ¼å¼: {type(batch_data)}")
                    continue

                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                outputs = model(data)

                # è®¡ç®—æŸå¤±
                if model_type == 'transformer':
                    # Transformerä½¿ç”¨KLæ•£åº¦æŸå¤±
                    if labels.dtype == torch.long:
                        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                        labels_one_hot = torch.zeros_like(outputs)
                        labels_one_hot.scatter_(1, labels.unsqueeze(1), 1.0)
                        labels = labels_one_hot.float()

                    # ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒ
                    outputs = torch.clamp(outputs, min=1e-8, max=1.0)
                    outputs = outputs / outputs.sum(dim=1, keepdim=True)
                    log_outputs = torch.log(outputs)

                    loss = criterion(log_outputs, labels)
                else:
                    # æ ‡å‡†æ¨¡å‹ä½¿ç”¨äº¤å‰ç†µæŸå¤±
                    if labels.dtype == torch.float and labels.shape[1] > 1:
                        # æ¦‚ç‡åˆ†å¸ƒæ ‡ç­¾è½¬æ¢ä¸ºç±»åˆ«æ ‡ç­¾
                        labels = torch.argmax(labels, dim=1)
                    loss = criterion(outputs, labels)

                # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
                if torch.isnan(loss) or torch.isinf(loss):
                    logger.warning(f"è®­ç»ƒç¬¬{epoch + 1}è½®ç¬¬{batch_idx}æ‰¹æ¬¡å‡ºç°å¼‚å¸¸æŸå¤±å€¼")
                    continue

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                if gradient_clip:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)

                optimizer.step()

                total_loss += loss.item()
                num_batches += 1

                # æ›´æ–°è¿›åº¦æ¡
                if not disable_pbar:
                    pbar.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'avg_loss': f'{total_loss / num_batches:.4f}'
                    })

            except Exception as e:
                logger.error(f"è®­ç»ƒç¬¬{epoch + 1}è½®ç¬¬{batch_idx}æ‰¹æ¬¡å‡ºé”™: {str(e)}")
                continue

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        if num_batches > 0:
            avg_train_loss = total_loss / num_batches
            train_losses.append(avg_train_loss)
        else:
            logger.warning(f"ç¬¬{epoch + 1}è½®æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ‰¹æ¬¡")
            continue

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = universal_evaluate_model(model, val_loader, device, model_type)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)

        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler.step(val_loss)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            if save_location:
                save_model_checkpoint(
                    model, optimizer, scheduler, epoch,
                    avg_train_loss, val_loss, val_acc,
                    save_location / 'best_model.pth'
                )

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if save_location and (epoch + 1) % checkpoint_freq == 0:
            model_name = model.__class__.__name__
            checkpoint_path = save_location / f'{model_name}_epoch{epoch + 1}.pth'
            save_model_checkpoint(
                model, optimizer, scheduler, epoch,
                avg_train_loss, val_loss, val_acc, checkpoint_path
            )

        # æ‰“å°æŒ‡æ ‡
        if trial is None:
            print(f'Epoch [{epoch + 1}/{epochs}]')
            print(f'Train Loss: {avg_train_loss:.4f}')
            print(f'Val Loss: {val_loss:.4f}')
            print(f'Val Accuracy: {val_acc:.4f}')
        else:
            # Optunaè¯•éªŒä¸­çš„ç®€åŒ–è¾“å‡º
            if (epoch + 1) % 5 == 0:
                print(f'Epoch {epoch + 1}: Val Acc: {val_acc:.4f}')

        # Optunaå‰ªæ
        if trial is not None:
            trial.report(val_acc, epoch)
            if trial.should_prune():
                print(f"Trialåœ¨ç¬¬{epoch + 1}è½®è¢«å‰ªæ")
                raise optuna.exceptions.TrialPruned()

        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print(f"ç¬¬{epoch + 1}è½®è§¦å‘æ—©åœ")
            break

    # æ¸…ç†GPUå†…å­˜
    if device.type == 'cuda':
        torch.cuda.empty_cache()

    return train_losses, val_losses, val_accuracies


def save_model_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, val_acc, save_path):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆå¸¦å®Œæ•´ configï¼‰"""
    # 1) ä¼˜å…ˆç”¨æ¨¡å‹è‡ªå¸¦é…ç½®
    if hasattr(model, "get_config") and callable(model.get_config):
        model_config = model.get_config()
    elif hasattr(model, "init_kwargs"):  # ä½ å¯ä»¥åœ¨æ¨¡å‹ __init__ æœ€ååŠ ä¸Š self.init_kwargs = locals()
        model_config = dict(model.init_kwargs)
        model_config.pop("self", None)  # å»æ‰ self
    else:
        model_config = {}

    # 2) ç¡®ä¿å†™å…¥ input_dim / output_dim
    state = model.state_dict()
    if "input_dim" not in model_config or model_config["input_dim"] is None:
        for k, v in state.items():
            if k.endswith("weight") and v.ndim == 2:  # Linear
                model_config["input_dim"] = int(v.shape[1])
                break
    if "output_dim" not in model_config or model_config["output_dim"] is None:
        for k, v in state.items():
            if k.endswith("weight") and v.ndim == 2:  # Linear
                model_config["output_dim"] = int(v.shape[0])
        # æŸäº›æ¨¡å‹ç”¨ classifier/conv æœ«å±‚
        if "output_dim" not in model_config:
            for k, v in state.items():
                if ("classifier" in k or "fc" in k) and k.endswith("weight"):
                    model_config["output_dim"] = int(v.shape[0])
                    break

    # 3) ç»„è£… checkpoint
    checkpoint = {
        "epoch": epoch + 1,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict() if optimizer else None,
        "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
        "train_loss": train_loss,
        "val_loss": val_loss,
        "val_accuracy": val_acc,
        "model_config": model_config,
        "model_class": model.__class__.__name__,
        "model_module": model.__class__.__module__,
        "timestamp": datetime.now().isoformat(),
    }

    torch.save(checkpoint, save_path)
    print(f"âœ… æ¨¡å‹ä¿å­˜åˆ° {save_path}")
    print(f"ğŸ“‹ model_config: {model_config}")



def save_trial_details_to_txt(
        trial_number: int,
        model_name: str,
        params: Dict[str, Any],
        train_losses: List[float],
        val_losses: List[float],
        val_accuracies: List[float],
        best_accuracy: float,
        training_time: Optional[float] = None,
        save_folder: str = "trial_logs",
        additional_info: Optional[Dict] = None
):
    """
    ä¿å­˜æ¯æ¬¡trialçš„è®­ç»ƒç»†èŠ‚åˆ°txtæ–‡ä»¶

    Args:
        trial_number: Trialç¼–å·
        model_name: æ¨¡å‹åç§°
        params: è¶…å‚æ•°å­—å…¸
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        val_accuracies: éªŒè¯å‡†ç¡®ç‡åˆ—è¡¨
        best_accuracy: æœ€ä½³å‡†ç¡®ç‡
        training_time: è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰
        save_folder: ä¿å­˜æ–‡ä»¶å¤¹
        additional_info: é¢å¤–ä¿¡æ¯å­—å…¸
    """

    # ç¡®ä¿ä¿å­˜æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(save_folder, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"trial_{trial_number:03d}_{model_name}_{timestamp}.txt"
    filepath = os.path.join(save_folder, filename)

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    final_train_loss = train_losses[-1] if train_losses else "N/A"
    final_val_loss = val_losses[-1] if val_losses else "N/A"
    final_val_acc = val_accuracies[-1] if val_accuracies else "N/A"
    epochs_trained = len(train_losses)

    # æ‰¾åˆ°æœ€ä½³epoch
    if val_accuracies:
        best_epoch = val_accuracies.index(max(val_accuracies)) + 1
        best_val_loss = val_losses[best_epoch - 1] if val_losses else "N/A"
    else:
        best_epoch = "N/A"
        best_val_loss = "N/A"

    # è®¡ç®—è®­ç»ƒæ”¹å–„æƒ…å†µ
    if len(train_losses) >= 2:
        train_improvement = train_losses[0] - train_losses[-1]
        train_improvement_pct = (train_improvement / train_losses[0]) * 100
    else:
        train_improvement = "N/A"
        train_improvement_pct = "N/A"

    # å†™å…¥txtæ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write(f"OPTUNA TRIAL #{trial_number} - {model_name}\n")
        f.write("=" * 70 + "\n")
        f.write(f"æ—¶é—´æˆ³: {timestamp}\n")
        f.write(f"æ¨¡å‹: {model_name}\n")
        f.write(f"Trialç¼–å·: {trial_number}\n")
        if training_time:
            f.write(f"è®­ç»ƒè€—æ—¶: {training_time:.2f} ç§’ ({training_time / 60:.1f} åˆ†é’Ÿ)\n")
        f.write("\n")

        # è¶…å‚æ•°éƒ¨åˆ†
        f.write("ğŸ”§ è¶…å‚æ•°é…ç½®:\n")
        f.write("-" * 30 + "\n")
        for key, value in params.items():
            if isinstance(value, float):
                if value < 0.001:
                    f.write(f"  {key:15}: {value:.2e}\n")
                else:
                    f.write(f"  {key:15}: {value:.4f}\n")
            else:
                f.write(f"  {key:15}: {value}\n")
        f.write("\n")

        # è®­ç»ƒç»“æœæ¦‚è§ˆ
        f.write("ğŸ“Š è®­ç»ƒç»“æœæ¦‚è§ˆ:\n")
        f.write("-" * 30 + "\n")
        f.write(f"  è®­ç»ƒè½®æ•°:     {epochs_trained}\n")
        f.write(f"  æœ€ä½³å‡†ç¡®ç‡:   {best_accuracy:.4f} (ç¬¬{best_epoch}è½®)\n")
        f.write(f"  æœ€ä½³æ—¶éªŒè¯æŸå¤±: {best_val_loss}\n")
        f.write(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss}\n")
        f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss}\n")
        f.write(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc}\n")
        if train_improvement != "N/A":
            f.write(f"  è®­ç»ƒæŸå¤±æ”¹å–„: {train_improvement:.4f} ({train_improvement_pct:.1f}%)\n")
        f.write("\n")

        # è¯¦ç»†è®­ç»ƒå†å²
        f.write("ğŸ“ˆ è¯¦ç»†è®­ç»ƒå†å²:\n")
        f.write("-" * 50 + "\n")
        f.write(f"{'Epoch':<6} {'Train Loss':<12} {'Val Loss':<12} {'Val Acc':<12} {'Notes'}\n")
        f.write("-" * 50 + "\n")

        for i in range(epochs_trained):
            epoch = i + 1
            train_loss = train_losses[i] if i < len(train_losses) else "N/A"
            val_loss = val_losses[i] if i < len(val_losses) else "N/A"
            val_acc = val_accuracies[i] if i < len(val_accuracies) else "N/A"

            # æ·»åŠ æ³¨é‡Š
            notes = ""
            if val_accuracies and i < len(val_accuracies):
                if val_accuracies[i] == max(val_accuracies):
                    notes += "ğŸ† BEST"
                elif i > 0 and val_accuracies[i] > val_accuracies[i - 1]:
                    notes += "ğŸ“ˆ UP"
                elif i > 0 and val_accuracies[i] < val_accuracies[i - 1]:
                    notes += "ğŸ“‰ DOWN"

            # æ ¼å¼åŒ–æ•°å€¼
            if isinstance(train_loss, float):
                train_str = f"{train_loss:.4f}"
            else:
                train_str = str(train_loss)

            if isinstance(val_loss, float):
                val_loss_str = f"{val_loss:.4f}"
            else:
                val_loss_str = str(val_loss)

            if isinstance(val_acc, float):
                val_acc_str = f"{val_acc:.4f}"
            else:
                val_acc_str = str(val_acc)

            f.write(f"{epoch:<6} {train_str:<12} {val_loss_str:<12} {val_acc_str:<12} {notes}\n")

        f.write("\n")

        # é¢å¤–ä¿¡æ¯
        if additional_info:
            f.write("â„¹ï¸  é¢å¤–ä¿¡æ¯:\n")
            f.write("-" * 20 + "\n")
            for key, value in additional_info.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")

        # åŸå§‹æ•°æ®ï¼ˆä¾¿äºåç»­åˆ†æï¼‰
        f.write("ğŸ“‹ åŸå§‹æ•°æ® (JSONæ ¼å¼):\n")
        f.write("-" * 30 + "\n")
        raw_data = {
            "trial_number": trial_number,
            "model_name": model_name,
            "timestamp": timestamp,
            "params": params,
            "train_losses": train_losses,
            "val_losses": val_losses,
            "val_accuracies": val_accuracies,
            "best_accuracy": best_accuracy,
            "epochs_trained": epochs_trained,
            "training_time": training_time
        }
        f.write(json.dumps(raw_data, indent=2, ensure_ascii=False))
        f.write("\n\n")

        f.write("=" * 70 + "\n")
        f.write("Trialè®°å½•ç»“æŸ\n")
        f.write("=" * 70 + "\n")

    print(f"ğŸ“ Trial #{trial_number} è¯¦ç»†è®°å½•å·²ä¿å­˜åˆ°: {filepath}")
    return filepath


def create_trial_summary(save_folder: str = "trial_logs"):
    """
    åˆ›å»ºæ‰€æœ‰trialsçš„æ±‡æ€»æ–‡ä»¶

    Args:
        save_folder: trialæ—¥å¿—æ–‡ä»¶å¤¹
    """
    if not os.path.exists(save_folder):
        print(f"æ–‡ä»¶å¤¹ {save_folder} ä¸å­˜åœ¨")
        return

    # æŸ¥æ‰¾æ‰€æœ‰trialæ–‡ä»¶
    trial_files = [f for f in os.listdir(save_folder) if f.startswith("trial_") and f.endswith(".txt")]

    if not trial_files:
        print("æ²¡æœ‰æ‰¾åˆ°trialæ–‡ä»¶")
        return

    # æ’åºæ–‡ä»¶
    trial_files.sort()

    # åˆ›å»ºæ±‡æ€»æ–‡ä»¶
    summary_path = os.path.join(save_folder, "trials_summary.txt")

    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("OPTUNA TRIALS æ±‡æ€»æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»trialæ•°: {len(trial_files)}\n")
        f.write("\n")

        f.write("ğŸ“Š æ‰€æœ‰Trialsæ¦‚è§ˆ:\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'Trial':<6} {'Model':<20} {'Best Acc':<10} {'Batch Size':<12} {'LR':<12} {'Status'}\n")
        f.write("-" * 80 + "\n")

        # è§£ææ¯ä¸ªtrialæ–‡ä»¶
        best_overall = {"trial": 0, "accuracy": 0, "model": ""}

        for file in trial_files:
            try:
                with open(os.path.join(save_folder, file), 'r', encoding='utf-8') as trial_file:
                    content = trial_file.read()

                    # æå–å…³é”®ä¿¡æ¯
                    lines = content.split('\n')
                    trial_num = "N/A"
                    model_name = "N/A"
                    best_acc = 0
                    batch_size = "N/A"
                    lr = "N/A"

                    for line in lines:
                        if "Trialç¼–å·:" in line:
                            trial_num = line.split(":")[-1].strip()
                        elif "æ¨¡å‹:" in line and "Trialç¼–å·" not in line:
                            model_name = line.split(":")[-1].strip()
                        elif "æœ€ä½³å‡†ç¡®ç‡:" in line:
                            try:
                                best_acc = float(line.split(":")[1].split("(")[0].strip())
                            except:
                                best_acc = 0
                        elif "batch_size" in line and ":" in line:
                            batch_size = line.split(":")[-1].strip()
                        elif "lr" in line and ":" in line:
                            try:
                                lr_val = float(line.split(":")[-1].strip())
                                lr = f"{lr_val:.2e}"
                            except:
                                lr = line.split(":")[-1].strip()

                    # è®°å½•æœ€ä½³ç»“æœ
                    if best_acc > best_overall["accuracy"]:
                        best_overall = {"trial": trial_num, "accuracy": best_acc, "model": model_name}

                    # çŠ¶æ€åˆ¤æ–­
                    status = "å®Œæˆ"
                    if best_acc == 0:
                        status = "å¤±è´¥"
                    elif best_acc > 0.9:
                        status = "ä¼˜ç§€"
                    elif best_acc > 0.8:
                        status = "è‰¯å¥½"

                    f.write(f"{trial_num:<6} {model_name:<20} {best_acc:<10.4f} {batch_size:<12} {lr:<12} {status}\n")

            except Exception as e:
                f.write(f"è§£æ {file} æ—¶å‡ºé”™: {str(e)}\n")

        f.write("\n")
        f.write("ğŸ† æœ€ä½³ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        f.write(f"Trial #{best_overall['trial']} - {best_overall['model']}\n")
        f.write(f"æœ€ä½³å‡†ç¡®ç‡: {best_overall['accuracy']:.4f}\n")
        f.write("\n")

        f.write("=" * 80 + "\n")
        f.write("æ±‡æ€»æŠ¥å‘Šç»“æŸ\n")
        f.write("=" * 80 + "\n")

    print(f"ğŸ“Š Trialsæ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_path}")
    return summary_path


def universal_hyperparameter_search(
        model_class,
        train_loader,
        val_loader,
        input_dim: int,
        output_dim: int = 2,
        n_trials: int = 20,
        device: str = 'cuda',
        model_folder: str = 'checkpoints',
        search_space: Optional[Dict] = None
) -> Tuple[Dict, Any]:
    """
    é€šç”¨è¶…å‚æ•°æœç´¢å‡½æ•°ï¼Œå…¼å®¹æ‰€æœ‰æ¨¡å‹ç±»å‹

    Args:
        model_class: æ¨¡å‹ç±»
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        input_dim: è¾“å…¥ç»´åº¦
        output_dim: è¾“å‡ºç»´åº¦
        n_trials: è¯•éªŒæ¬¡æ•°
        device: è®¾å¤‡
        model_folder: æ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹
        search_space: æœç´¢ç©ºé—´é…ç½®

    Returns:
        best_params: æœ€ä½³å‚æ•°
        study: Optunaç ”ç©¶å¯¹è±¡
    """

    def objective(trial):
        # æ ¹æ®æ¨¡å‹ç±»å‹å®šä¹‰æœç´¢ç©ºé—´
        model_name = model_class.__name__

        if 'Transformer' in model_name or 'CustomTransformer' in model_name:
            # Transformerç‰¹å®šå‚æ•°
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-3, log=True),
                'd_model': trial.suggest_categorical('d_model', [128, 256, 512, 1024]),
                'n_heads': trial.suggest_categorical('n_heads', [8, 16, 32]),
                'n_layers': trial.suggest_categorical('n_layers', [2, 3, 4, 6]),
                'd_ff': trial.suggest_categorical('d_ff', [512, 1024, 2048]),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True),
            }
        elif 'ResNet' in model_name:
            # ResNetç‰¹å®šå‚æ•°
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-2, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'base_filters': trial.suggest_categorical('base_filters', [16, 32, 64, 128]),
                'kernel_size': trial.suggest_categorical('kernel_size', [8, 16, 32]),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
                'n_blocks': trial.suggest_int('n_blocks', 2, 6),
            }
        elif 'CNN1D' in model_name:
            # CNN1Dç‰¹å®šå‚æ•°
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-2, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256, 512]),
                'kernel_size': trial.suggest_categorical('kernel_size', [8, 16, 32]),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            }
        elif 'Wavenet' in model_name:
            # Wavenetç‰¹å®šå‚æ•°
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-2, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'hidden_dim': trial.suggest_categorical('hidden_dim', [64, 128, 256, 512]),
                'kernel_size': trial.suggest_categorical('kernel_size', [8, 16, 32]),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            }
        elif 'LSTM' in model_name:
            # LSTMç‰¹å®šå‚æ•°
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-2, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'hidden_dim': trial.suggest_categorical('hidden_dim', [32, 50, 100, 200]),
                'num_layers': trial.suggest_int('num_layers', 1, 3),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            }
        elif 'S4' in model_name:
            # S4ç‰¹å®šå‚æ•°
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-2, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'd_model': trial.suggest_categorical('d_model', [32, 64, 128, 256]),
                'n_layers': trial.suggest_int('n_layers', 1, 4),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            }
        else:
            # é»˜è®¤æœç´¢ç©ºé—´
            params = {
                'lr': trial.suggest_float('lr', 1e-6, 1e-2, log=True),
                'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),
                'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            }

        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰æœç´¢ç©ºé—´ï¼Œåˆ™è¦†ç›–é»˜è®¤å€¼
        if search_space:
            for param_name, config in search_space.items():
                if config['type'] == 'loguniform':
                    params[param_name] = trial.suggest_float(param_name, *config['range'], log=True)
                elif config['type'] == 'uniform':
                    params[param_name] = trial.suggest_float(param_name, *config['range'])
                elif config['type'] == 'int':
                    params[param_name] = trial.suggest_int(param_name, *config['range'])
                elif config['type'] == 'categorical':
                    params[param_name] = trial.suggest_categorical(param_name, config['values'])

        try:
            # åˆ›å»ºæ¨¡å‹
            if 'Transformer' in model_name or 'CustomTransformer' in model_name:
                model = model_class(
                    input_dim=input_dim,
                    output_dim=output_dim,
                    d_model=params.get('d_model', 512),
                    n_heads=params.get('n_heads', 8),
                    n_layers=params.get('n_layers', 6),
                    d_ff=params.get('d_ff', 2048),
                    dropout=params.get('dropout', 0.1),
                    lr=params['lr'],
                    weight_decay=params.get('weight_decay', 1e-5),
                    device=device
                )
            elif 'ResNet' in model_name:
                model = model_class(
                    input_dim=input_dim,
                    n_classes=output_dim,
                    base_filters=params.get('base_filters', 32),
                    kernel_size=params.get('kernel_size', 16),
                    dropout=params.get('dropout', 0.2),
                    n_blocks=params.get('n_blocks', 3),
                    lr=params['lr'],
                    weight_decay=params.get('weight_decay', 1e-5)
                )
            elif 'CNN1D' in model_name:
                model = model_class(
                    input_dim=input_dim,
                    output_dim=output_dim,
                    hidden_dim=params.get('hidden_dim', 128),
                    kernel_size=params.get('kernel_size', 256),
                    dropout=params.get('dropout', 0.2),
                    lr=params['lr'],
                    weight_decay=params.get('weight_decay', 1e-5),
                    device=device
                )
            elif 'Wavenet' in model_name:
                model = model_class(
                    input_dim=input_dim,
                    output_dim=output_dim,
                    hidden_dim=params.get('hidden_dim', 128),
                    kernel_size=params.get('kernel_size', 32),
                    dropout=params.get('dropout', 0.2),
                    lr=params['lr'],
                    weight_decay=params.get('weight_decay', 1e-5)
                )
            elif 'LSTM' in model_name:
                model = model_class(
                    input_dim=input_dim,
                    output_dim=output_dim,
                    hidden_dim=params.get('hidden_dim', 50),
                    num_layers=params.get('num_layers', 1),
                    dropout=params.get('dropout', 0.2),
                    lr=params['lr'],
                    weight_decay=params.get('weight_decay', 1e-5),
                    device=device
                )
            elif 'S4' in model_name:
                model = model_class(
                    d_input=input_dim,
                    d_output=output_dim,
                    d_model=params.get('d_model', 64),
                    n_layers=params.get('n_layers', 1),
                    dropout=params.get('dropout', 0.2),
                    lr=params['lr'],
                    weight_decay=params.get('weight_decay', 1e-5)
                )
            else:
                # é€šç”¨æ¨¡å‹åˆ›å»º
                model = model_class(input_dim=input_dim, output_dim=output_dim, lr=params['lr'])

            # åˆ›å»ºä¸´æ—¶ä¿å­˜ç›®å½•
            temp_dir = os.path.join(model_folder, f"optuna_trial_{trial.number}")
            os.makedirs(temp_dir, exist_ok=True)

            # ä¿å­˜è¯•éªŒé…ç½®
            trial_config = {
                'trial_number': trial.number,
                'model_class': model_name,
                'parameters': params,
                'timestamp': datetime.now().isoformat()
            }

            config_path = os.path.join(temp_dir, 'trial_config.json')
            with open(config_path, 'w') as f:
                json.dump(trial_config, f, indent=2)

            # è®­ç»ƒæ¨¡å‹
            train_losses, val_losses, val_accuracies = universal_train_model(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                save_location=temp_dir,
                epochs=15,  # ä¸ºå¿«é€Ÿæœç´¢ä½¿ç”¨è¾ƒå°‘çš„è½®æ•°
                device=device,
                patience=5,
                trial=trial  # ä¼ é€’trialå¯¹è±¡ç”¨äºå‰ªæ
            )

            # è¿”å›æœ€ä½³éªŒè¯å‡†ç¡®ç‡
            if val_accuracies:
                best_acc = max(val_accuracies)

                # ä¿å­˜è¯•éªŒç»“æœ
                trial_config['results'] = {
                    'best_val_accuracy': best_acc,
                    'final_train_loss': train_losses[-1] if train_losses else None,
                    'final_val_loss': val_losses[-1] if val_losses else None,
                    'converged_epochs': len(val_accuracies)
                }

                with open(config_path, 'w') as f:
                    json.dump(trial_config, f, indent=2)

                save_trial_details_to_txt(
                    trial_number=trial.number,
                    model_name=model_name,
                    params=params,
                    train_losses=train_losses,
                    val_losses=val_losses,
                    val_accuracies=val_accuracies,
                    best_accuracy=best_acc,
                    save_folder=os.path.join(model_folder, "trial_logs"),
                    additional_info={
                        "converged_epochs": len(val_accuracies),
                        "total_trials": n_trials,
                        "device": device
                    }
                )

                print(f"Trial {trial.number} completed with accuracy: {best_acc:.4f}")
                return best_acc
            else:
                print(f"Trial {trial.number} failed: No validation accuracies recorded")
                return 0.0

        except optuna.exceptions.TrialPruned:
            raise
        except Exception as e:
            print(f"Trial {trial.number} failed: {str(e)}")
            return 0.0

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(model_folder, exist_ok=True)

    # åˆ›å»ºOptuna study
    study_name = f"{model_class.__name__}_study_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    study = optuna.create_study(
        study_name=study_name,
        direction="maximize",
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=5,
            interval_steps=1
        )
    )

    print(f"å¼€å§‹{model_class.__name__}è¶…å‚æ•°æœç´¢ï¼Œå…±{n_trials}æ¬¡è¯•éªŒ...")

    # è¿è¡Œä¼˜åŒ–
    study.optimize(objective, n_trials=n_trials)

    # ä¿å­˜studyç»“æœ
    study_results_path = os.path.join(model_folder, f"{study_name}_results.json")
    study_summary = {
        'study_name': study_name,
        'model_class': model_class.__name__,
        'n_trials': len(study.trials),
        'best_value': study.best_value,
        'best_params': study.best_params,
        'best_trial_number': study.best_trial.number,
        'timestamp': datetime.now().isoformat(),
        'all_trials': [
            {
                'number': trial.number,
                'value': trial.value,
                'params': trial.params,
                'state': trial.state.name
            }
            for trial in study.trials
        ]
    }

    with open(study_results_path, 'w') as f:
        json.dump(study_summary, f, indent=2)

    create_trial_summary(save_folder=os.path.join(model_folder, "trial_logs"))

    # æ‰“å°æœ€ä½³ç»“æœ
    print(f"\n" + "=" * 60)
    print(f"{model_class.__name__}è¶…å‚æ•°æœç´¢å®Œæˆ!")
    print(f"=" * 60)
    print(f"æœ€ä½³è¯•éªŒ #{study.best_trial.number}:")
    print(f"  éªŒè¯å‡†ç¡®ç‡: {study.best_value:.4f}")
    print(f"  æœ€ä½³å‚æ•°:")
    for key, value in study.best_params.items():
        print(f"    {key}: {value}")
    print(f"  è¯¦ç»†ç»“æœä¿å­˜åœ¨: {study_results_path}")
    print(f"=" * 60)

    return study.best_params, study


def create_model_with_best_params(model_class, best_params, input_dim, output_dim=2, device='cuda'):
    """
    ä½¿ç”¨æœ€ä½³å‚æ•°åˆ›å»ºæ¨¡å‹

    Args:
        model_class: æ¨¡å‹ç±»
        best_params: æœ€ä½³å‚æ•°å­—å…¸
        input_dim: è¾“å…¥ç»´åº¦
        output_dim: è¾“å‡ºç»´åº¦
        device: è®¾å¤‡

    Returns:
        model: åˆ›å»ºçš„æ¨¡å‹å®ä¾‹
    """
    model_name = model_class.__name__

    try:
        if 'Transformer' in model_name or 'CustomTransformer' in model_name:
            model = model_class(
                input_dim=input_dim,
                output_dim=output_dim,
                d_model=best_params.get('d_model', 512),
                n_heads=best_params.get('n_heads', 8),
                n_layers=best_params.get('n_layers', 6),
                d_ff=best_params.get('d_ff', 2048),
                dropout=best_params.get('dropout', 0.1),
                lr=best_params.get('lr', 0.001),
                weight_decay=best_params.get('weight_decay', 1e-5),
                device=device
            )
        elif 'ResNet' in model_name:
            model = model_class(
                input_dim=input_dim,
                n_classes=output_dim,
                base_filters=best_params.get('base_filters', 32),
                kernel_size=best_params.get('kernel_size', 16),
                dropout=best_params.get('dropout', 0.2),
                n_blocks=best_params.get('n_blocks', 3),
                lr=best_params.get('lr', 0.001),
                weight_decay=best_params.get('weight_decay', 1e-5)
            )
        elif 'CNN1D' in model_name:
            model = model_class(
                input_dim=input_dim,
                output_dim=output_dim,
                hidden_dim=best_params.get('hidden_dim', 128),
                kernel_size=best_params.get('kernel_size', 256),
                dropout=best_params.get('dropout', 0.2),
                lr=best_params.get('lr', 0.001),
                weight_decay=best_params.get('weight_decay', 1e-5),
                device=device
            )
        elif 'Wavenet' in model_name:
            model = model_class(
                input_dim=input_dim,
                output_dim=output_dim,
                hidden_dim=best_params.get('hidden_dim', 128),
                kernel_size=best_params.get('kernel_size', 32),
                dropout=best_params.get('dropout', 0.2),
                lr=best_params.get('lr', 0.001),
                weight_decay=best_params.get('weight_decay', 1e-5)
            )
        elif 'LSTM' in model_name:
            model = model_class(
                input_dim=input_dim,
                output_dim=output_dim,
                hidden_dim=best_params.get('hidden_dim', 50),
                num_layers=best_params.get('num_layers', 1),
                dropout=best_params.get('dropout', 0.2),
                lr=best_params.get('lr', 0.001),
                weight_decay=best_params.get('weight_decay', 1e-5),
                device=device
            )
        elif 'S4' in model_name:
            model = model_class(
                d_input=input_dim,
                d_output=output_dim,
                d_model=best_params.get('d_model', 64),
                n_layers=best_params.get('n_layers', 1),
                dropout=best_params.get('dropout', 0.2),
                lr=best_params.get('lr', 0.001),
                weight_decay=best_params.get('weight_decay', 1e-5)
            )
        else:
            # é€šç”¨æ¨¡å‹åˆ›å»º
            model = model_class(
                input_dim=input_dim,
                output_dim=output_dim,
                lr=best_params.get('lr', 0.001)
            )

        print(f"âœ… æˆåŠŸåˆ›å»º{model_name}æ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³å‚æ•°")
        return model

    except Exception as e:
        print(f"âŒ åˆ›å»º{model_name}æ¨¡å‹å¤±è´¥: {str(e)}")
        return None


def load_model_with_config(checkpoint_path, model_class):
    """
    load model from checkpoints with config

    Args:
        checkpoint_path: checkpoint path
        model_class: model class

    Returns:
        model: loaded model
        checkpoint: checkpoints result
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    if 'model_config' not in checkpoint:
        raise ValueError(f"checkpoint {checkpoint_path} is missing config info")

    # load model config
    config = checkpoint['model_config']

    # create model based on config
    model_name = model_class.__name__

    if 'Transformer' in model_name or 'CustomTransformer' in model_name:
        model = model_class(
            input_dim=config.get('input_dim'),
            output_dim=config.get('output_dim', 2),
            d_model=config.get('d_model', 512),
            n_heads=config.get('n_heads', 8),
            n_layers=config.get('n_layers', 6),
            d_ff=config.get('d_ff', 2048),
            dropout=config.get('dropout', 0.1),
            max_seq_len=config.get('max_seq_len', 1000)
        )
    elif 'ResNet' in model_name:
        model = model_class(
            input_dim=config.get('input_dim'),
            n_classes=config.get('output_dim', 2),
            base_filters=config.get('base_filters', 32),
            kernel_size=config.get('kernel_size', 16),
            n_blocks=config.get('n_blocks', 3),
            dropout=config.get('dropout', 0.2)
        )
    elif 'CNN1D' in model_name:
        model = model_class(
            input_dim=config.get('input_dim'),
            output_dim=config.get('output_dim', 2),
            hidden_dim=config.get('hidden_dim', 128),
            kernel_size=config.get('kernel_size', 256),
            dropout=config.get('dropout', 0.2)
        )
    elif 'Wavenet' in model_name:
        model = model_class(
            input_dim=config.get('input_dim'),
            output_dim=config.get('output_dim', 2),
            hidden_dim=config.get('hidden_dim', 128),
            kernel_size=config.get('kernel_size', 32),
            dropout=config.get('dropout', 0.2)
        )
    elif 'LSTM' in model_name:
        model = model_class(
            input_dim=config.get('input_dim'),
            output_dim=config.get('output_dim', 2),
            hidden_dim=config.get('hidden_dim', 50),
            num_layers=config.get('num_layers', 1),
            dropout=config.get('dropout', 0.2)
        )
    elif 'S4' in model_name:
        model = model_class(
            d_input=config.get('input_dim'),
            d_output=config.get('output_dim', 2),
            d_model=config.get('d_model', 64),
            n_layers=config.get('n_layers', 1),
            dropout=config.get('dropout', 0.2)
        )
    else:
        # general model
        model = model_class(
            input_dim=config.get('input_dim'),
            output_dim=config.get('output_dim', 2)
        )

    # load model state dict
    model.load_state_dict(checkpoint['model_state_dict'])

    print(f"âœ… Successfully Load{model_name}")
    print(f"ğŸ“‹ Model Config: {config}")
    if 'timestamp' in checkpoint:
        print(f"ğŸ• Save time: {checkpoint['timestamp']}")

    return model, checkpoint
