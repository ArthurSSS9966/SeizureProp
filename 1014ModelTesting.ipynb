{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils import load_seizure, split_data\n",
    "from datasetConstruct import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from models import CNN1D, train_using_optimizer, Wavenet, LSTM, evaluate_model\n",
    "\n",
    "data_folder = \"data\"\n",
    "MODEL_FOLDER = \"D:/Blcdata/seizure/Model\"\n",
    "PAT_NO = 66"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:37:11.925253400Z",
     "start_time": "2024-10-15T02:37:06.930589200Z"
    }
   },
   "id": "c6d6dfd5661a6da5",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:37:11.973205800Z",
     "start_time": "2024-10-15T02:37:11.929242100Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = os.path.join(data_folder, f\"P{PAT_NO}\")\n",
    "seizure = load_seizure(data_folder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Divide the seizure and nonseizure data in to 1s windows\n",
    "window_size = 1\n",
    "fs = seizure.samplingRate\n",
    "\n",
    "seizure_data = seizure.ictal\n",
    "nonseizure_data = seizure.interictal\n",
    "nonseizure_data_postictal = seizure.postictal\n",
    "\n",
    "seizure_data = split_data(seizure_data, fs)\n",
    "nonseizure_data = split_data(nonseizure_data, fs)\n",
    "nonseizure_data_postictal = split_data(nonseizure_data_postictal, fs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:37:12.040028200Z",
     "start_time": "2024-10-15T02:37:11.980107500Z"
    }
   },
   "id": "811df118d2d7c467",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine the nonseizure and postictal data\n",
    "nonseizure_data = np.concatenate((nonseizure_data, nonseizure_data_postictal), axis=0)\n",
    "\n",
    "# Create the labels\n",
    "seizure_labels = np.ones(len(seizure_data))\n",
    "nonseizure_labels = np.zeros(len(nonseizure_data))\n",
    "\n",
    "seizure_data = seizure_data.transpose(0, 2, 1)\n",
    "nonseizure_data = nonseizure_data.transpose(0, 2, 1)\n",
    "\n",
    "channels = seizure_data.shape[1]\n",
    "time_steps = seizure_data.shape[2]\n",
    "\n",
    "# Combine the dataset and labels, then shuffle them and create training and validation sets\n",
    "data = np.concatenate((seizure_data, nonseizure_data), axis=0)\n",
    "labels = np.concatenate((seizure_labels, nonseizure_labels), axis=0)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_indices = np.random.permutation(len(data))\n",
    "data = data[shuffled_indices]\n",
    "\n",
    "labels = labels[shuffled_indices]\n",
    "\n",
    "# Create the training and validation sets\n",
    "train_data = data[:int(0.8 * len(data))]\n",
    "train_labels = labels[:int(0.8 * len(labels))]\n",
    "val_data = data[int(0.8 * len(data)):]\n",
    "val_labels = labels[int(0.8 * len(labels)):]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:37:12.106769Z",
     "start_time": "2024-10-15T02:37:12.049919100Z"
    }
   },
   "id": "742e47e1059d9800",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "val_dataset = CustomDataset(val_data, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:37:12.133694100Z",
     "start_time": "2024-10-15T02:37:12.105771600Z"
    }
   },
   "id": "d325107625bcf8e6",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\anaconda3\\envs\\Code\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ..\\aten\\src\\ATen\\native\\Convolution.cpp:1004.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.0894\n",
      "Epoch [2/200], Loss: 1.1079\n",
      "Epoch [3/200], Loss: 1.1012\n",
      "Epoch [4/200], Loss: 1.1012\n",
      "Epoch [5/200], Loss: 1.1079\n",
      "Epoch [6/200], Loss: 1.0945\n",
      "Epoch [7/200], Loss: 1.1079\n",
      "Epoch [8/200], Loss: 1.0811\n",
      "Epoch [9/200], Loss: 1.0878\n",
      "Epoch [10/200], Loss: 1.0945\n",
      "Epoch [11/200], Loss: 1.0878\n",
      "Epoch [12/200], Loss: 1.0945\n",
      "Epoch [13/200], Loss: 1.0945\n",
      "Epoch [14/200], Loss: 1.0811\n",
      "Epoch [15/200], Loss: 1.1079\n",
      "Epoch [16/200], Loss: 1.1012\n",
      "Epoch [17/200], Loss: 1.0945\n",
      "Epoch [18/200], Loss: 1.0945\n",
      "Epoch [19/200], Loss: 1.0945\n",
      "Epoch [20/200], Loss: 1.0878\n",
      "Validation Loss: 0.0417, Validation Accuracy: 0.2703\n",
      "Epoch [21/200], Loss: 1.0811\n",
      "Epoch [22/200], Loss: 1.1079\n",
      "Epoch [23/200], Loss: 1.1012\n",
      "Epoch [24/200], Loss: 1.1079\n",
      "Epoch [25/200], Loss: 1.1012\n",
      "Epoch [26/200], Loss: 1.0945\n",
      "Epoch [27/200], Loss: 1.0945\n",
      "Epoch [28/200], Loss: 1.1012\n",
      "Epoch [29/200], Loss: 1.0945\n",
      "Epoch [30/200], Loss: 1.0945\n",
      "Epoch [31/200], Loss: 1.1012\n",
      "Epoch [32/200], Loss: 1.1012\n",
      "Epoch [33/200], Loss: 1.0878\n",
      "Epoch [34/200], Loss: 1.0945\n",
      "Epoch [35/200], Loss: 1.0878\n",
      "Epoch [36/200], Loss: 1.1079\n",
      "Epoch [37/200], Loss: 1.0878\n",
      "Epoch [38/200], Loss: 1.1012\n",
      "Epoch [39/200], Loss: 1.1012\n",
      "Epoch [40/200], Loss: 1.1079\n",
      "Validation Loss: 0.0431, Validation Accuracy: 0.2359\n",
      "Epoch [41/200], Loss: 1.0878\n",
      "Epoch [42/200], Loss: 1.1079\n",
      "Epoch [43/200], Loss: 1.1012\n",
      "Epoch [44/200], Loss: 1.1079\n",
      "Epoch [45/200], Loss: 1.1012\n",
      "Epoch [46/200], Loss: 1.1012\n",
      "Epoch [47/200], Loss: 1.1012\n",
      "Epoch [48/200], Loss: 1.0945\n",
      "Epoch [49/200], Loss: 1.1012\n",
      "Epoch [50/200], Loss: 1.1012\n",
      "Epoch [51/200], Loss: 1.1012\n",
      "Epoch [52/200], Loss: 1.1012\n",
      "Epoch [53/200], Loss: 1.0945\n",
      "Epoch [54/200], Loss: 1.0878\n",
      "Epoch [55/200], Loss: 1.1012\n",
      "Epoch [56/200], Loss: 1.0945\n",
      "Epoch [57/200], Loss: 1.1012\n",
      "Epoch [58/200], Loss: 1.1079\n",
      "Epoch [59/200], Loss: 1.0945\n",
      "Epoch [60/200], Loss: 1.1012\n",
      "Validation Loss: 0.0424, Validation Accuracy: 0.2531\n",
      "Epoch [61/200], Loss: 1.1079\n",
      "Epoch [62/200], Loss: 1.0945\n",
      "Epoch [63/200], Loss: 1.0945\n",
      "Epoch [64/200], Loss: 1.0878\n",
      "Epoch [65/200], Loss: 1.1079\n",
      "Epoch [66/200], Loss: 1.0945\n",
      "Epoch [67/200], Loss: 1.1012\n",
      "Epoch [68/200], Loss: 1.1079\n",
      "Epoch [69/200], Loss: 1.0945\n",
      "Epoch [70/200], Loss: 1.1012\n",
      "Epoch [71/200], Loss: 1.1012\n",
      "Epoch [72/200], Loss: 1.0878\n",
      "Epoch [73/200], Loss: 1.1012\n",
      "Epoch [74/200], Loss: 1.1079\n",
      "Epoch [75/200], Loss: 1.0945\n",
      "Epoch [76/200], Loss: 1.0945\n",
      "Epoch [77/200], Loss: 1.1012\n",
      "Epoch [78/200], Loss: 1.1079\n",
      "Epoch [79/200], Loss: 1.1012\n",
      "Epoch [80/200], Loss: 1.0945\n",
      "Validation Loss: 0.0424, Validation Accuracy: 0.2531\n",
      "Epoch [81/200], Loss: 1.0878\n",
      "Epoch [82/200], Loss: 1.0945\n",
      "Epoch [83/200], Loss: 1.1079\n",
      "Epoch [84/200], Loss: 1.1079\n",
      "Epoch [85/200], Loss: 1.0945\n",
      "Epoch [86/200], Loss: 1.0945\n",
      "Epoch [87/200], Loss: 1.1012\n",
      "Epoch [88/200], Loss: 1.0945\n",
      "Epoch [89/200], Loss: 1.0945\n",
      "Epoch [90/200], Loss: 1.0945\n",
      "Epoch [91/200], Loss: 1.1012\n",
      "Epoch [92/200], Loss: 1.0945\n",
      "Epoch [93/200], Loss: 1.1012\n",
      "Epoch [94/200], Loss: 1.0878\n",
      "Epoch [95/200], Loss: 1.0811\n",
      "Epoch [96/200], Loss: 1.1079\n",
      "Epoch [97/200], Loss: 1.0744\n",
      "Epoch [98/200], Loss: 1.1079\n",
      "Epoch [99/200], Loss: 1.1012\n",
      "Epoch [100/200], Loss: 1.1079\n",
      "Validation Loss: 0.0431, Validation Accuracy: 0.2359\n",
      "Epoch [101/200], Loss: 1.0945\n",
      "Epoch [102/200], Loss: 1.0945\n",
      "Epoch [103/200], Loss: 1.0878\n",
      "Epoch [104/200], Loss: 1.1079\n",
      "Epoch [105/200], Loss: 1.0931\n",
      "Epoch [106/200], Loss: 1.1012\n",
      "Epoch [107/200], Loss: 1.1012\n",
      "Epoch [108/200], Loss: 1.0931\n",
      "Epoch [109/200], Loss: 1.0811\n",
      "Epoch [110/200], Loss: 1.0945\n",
      "Epoch [111/200], Loss: 1.1012\n",
      "Epoch [112/200], Loss: 1.1079\n",
      "Epoch [113/200], Loss: 1.0945\n",
      "Epoch [114/200], Loss: 1.0878\n",
      "Epoch [115/200], Loss: 1.0878\n",
      "Epoch [116/200], Loss: 1.0945\n",
      "Epoch [117/200], Loss: 1.1079\n",
      "Epoch [118/200], Loss: 1.0945\n",
      "Epoch [119/200], Loss: 1.1079\n",
      "Epoch [120/200], Loss: 1.0878\n",
      "Validation Loss: 0.0424, Validation Accuracy: 0.2531\n",
      "Epoch [121/200], Loss: 1.0945\n",
      "Epoch [122/200], Loss: 1.0945\n",
      "Epoch [123/200], Loss: 1.1079\n",
      "Epoch [124/200], Loss: 1.1012\n",
      "Epoch [125/200], Loss: 1.1012\n",
      "Epoch [126/200], Loss: 1.0945\n",
      "Epoch [127/200], Loss: 1.0945\n",
      "Epoch [128/200], Loss: 1.1012\n",
      "Epoch [129/200], Loss: 1.1012\n",
      "Epoch [130/200], Loss: 1.0878\n",
      "Epoch [131/200], Loss: 1.0945\n",
      "Epoch [132/200], Loss: 1.1012\n",
      "Epoch [133/200], Loss: 1.0878\n",
      "Epoch [134/200], Loss: 1.0945\n",
      "Epoch [135/200], Loss: 1.1012\n",
      "Epoch [136/200], Loss: 1.0945\n",
      "Epoch [137/200], Loss: 1.1088\n",
      "Epoch [138/200], Loss: 1.0878\n",
      "Epoch [139/200], Loss: 1.1012\n",
      "Epoch [140/200], Loss: 1.0945\n",
      "Validation Loss: 0.0410, Validation Accuracy: 0.2875\n",
      "Epoch [141/200], Loss: 1.0945\n",
      "Epoch [142/200], Loss: 1.0811\n",
      "Epoch [143/200], Loss: 1.1012\n",
      "Epoch [144/200], Loss: 1.1079\n",
      "Epoch [145/200], Loss: 1.1012\n",
      "Epoch [146/200], Loss: 1.1012\n",
      "Epoch [147/200], Loss: 1.0945\n",
      "Epoch [148/200], Loss: 1.0945\n",
      "Epoch [149/200], Loss: 1.1079\n",
      "Epoch [150/200], Loss: 1.1012\n",
      "Epoch [151/200], Loss: 1.0945\n",
      "Epoch [152/200], Loss: 1.0945\n",
      "Epoch [153/200], Loss: 1.0878\n",
      "Epoch [154/200], Loss: 1.1012\n",
      "Epoch [155/200], Loss: 1.1012\n",
      "Epoch [156/200], Loss: 1.0878\n",
      "Epoch [157/200], Loss: 1.0811\n",
      "Epoch [158/200], Loss: 1.1012\n",
      "Epoch [159/200], Loss: 1.0945\n",
      "Epoch [160/200], Loss: 1.0945\n",
      "Validation Loss: 0.0417, Validation Accuracy: 0.2703\n",
      "Epoch [161/200], Loss: 1.1079\n",
      "Epoch [162/200], Loss: 1.1012\n",
      "Epoch [163/200], Loss: 1.0878\n",
      "Epoch [164/200], Loss: 1.1012\n",
      "Epoch [165/200], Loss: 1.0945\n",
      "Epoch [166/200], Loss: 1.1079\n",
      "Epoch [167/200], Loss: 1.1079\n",
      "Epoch [168/200], Loss: 1.1012\n",
      "Epoch [169/200], Loss: 1.1079\n",
      "Epoch [170/200], Loss: 1.0878\n",
      "Epoch [171/200], Loss: 1.1012\n",
      "Epoch [172/200], Loss: 1.1079\n",
      "Epoch [173/200], Loss: 1.1012\n",
      "Epoch [174/200], Loss: 1.0945\n",
      "Epoch [175/200], Loss: 1.1012\n",
      "Epoch [176/200], Loss: 1.0811\n",
      "Epoch [177/200], Loss: 1.0878\n",
      "Epoch [178/200], Loss: 1.0945\n",
      "Epoch [179/200], Loss: 1.0878\n",
      "Epoch [180/200], Loss: 1.0878\n",
      "Validation Loss: 0.0424, Validation Accuracy: 0.2531\n",
      "Epoch [181/200], Loss: 1.1012\n",
      "Epoch [182/200], Loss: 1.0811\n",
      "Epoch [183/200], Loss: 1.1012\n",
      "Epoch [184/200], Loss: 1.0998\n",
      "Epoch [185/200], Loss: 1.0864\n",
      "Epoch [186/200], Loss: 1.1012\n",
      "Epoch [187/200], Loss: 1.0878\n",
      "Epoch [188/200], Loss: 1.0990\n",
      "Epoch [189/200], Loss: 1.0878\n",
      "Epoch [190/200], Loss: 1.0945\n",
      "Epoch [191/200], Loss: 1.0945\n",
      "Epoch [192/200], Loss: 1.1079\n",
      "Epoch [193/200], Loss: 1.0945\n",
      "Epoch [194/200], Loss: 1.0945\n",
      "Epoch [195/200], Loss: 1.0811\n",
      "Epoch [196/200], Loss: 1.0945\n",
      "Epoch [197/200], Loss: 1.1079\n",
      "Epoch [198/200], Loss: 1.1012\n",
      "Epoch [199/200], Loss: 1.0811\n",
      "Epoch [200/200], Loss: 1.0878\n",
      "Validation Loss: 0.0410, Validation Accuracy: 0.2875\n",
      "Finished Training\n",
      "Epoch [1/200], Loss: 0.8694\n",
      "Epoch [2/200], Loss: 0.4907\n",
      "Epoch [3/200], Loss: 0.4691\n",
      "Epoch [4/200], Loss: 0.4663\n",
      "Epoch [5/200], Loss: 0.4547\n",
      "Epoch [6/200], Loss: 0.4665\n",
      "Epoch [7/200], Loss: 0.4513\n",
      "Epoch [8/200], Loss: 0.4574\n",
      "Epoch [9/200], Loss: 0.4252\n",
      "Epoch [10/200], Loss: 0.4259\n",
      "Epoch [11/200], Loss: 0.4117\n",
      "Epoch [12/200], Loss: 0.4145\n",
      "Epoch [13/200], Loss: 0.4205\n",
      "Epoch [14/200], Loss: 0.4384\n",
      "Epoch [15/200], Loss: 0.4250\n",
      "Epoch [16/200], Loss: 0.4116\n",
      "Epoch [17/200], Loss: 0.4116\n",
      "Epoch [18/200], Loss: 0.4119\n",
      "Epoch [19/200], Loss: 0.4118\n",
      "Epoch [20/200], Loss: 0.4192\n",
      "Validation Loss: 0.0182, Validation Accuracy: 0.8578\n",
      "Epoch [21/200], Loss: 0.4205\n",
      "Epoch [22/200], Loss: 0.4321\n",
      "Epoch [23/200], Loss: 0.4115\n",
      "Epoch [24/200], Loss: 0.4249\n",
      "Epoch [25/200], Loss: 0.4204\n",
      "Epoch [26/200], Loss: 0.4250\n",
      "Epoch [27/200], Loss: 0.4183\n",
      "Epoch [28/200], Loss: 0.4115\n",
      "Epoch [29/200], Loss: 0.4182\n",
      "Epoch [30/200], Loss: 0.4182\n",
      "Epoch [31/200], Loss: 0.4182\n",
      "Epoch [32/200], Loss: 0.4182\n",
      "Epoch [33/200], Loss: 0.4204\n",
      "Epoch [34/200], Loss: 0.4115\n",
      "Epoch [35/200], Loss: 0.4115\n",
      "Epoch [36/200], Loss: 0.4115\n",
      "Epoch [37/200], Loss: 0.4208\n",
      "Epoch [38/200], Loss: 0.4115\n",
      "Epoch [39/200], Loss: 0.4204\n",
      "Epoch [40/200], Loss: 0.4182\n",
      "Validation Loss: 0.0189, Validation Accuracy: 0.8406\n",
      "Epoch [41/200], Loss: 0.4249\n",
      "Epoch [42/200], Loss: 0.4115\n",
      "Epoch [43/200], Loss: 0.4137\n",
      "Epoch [44/200], Loss: 0.4249\n",
      "Epoch [45/200], Loss: 0.4125\n",
      "Epoch [46/200], Loss: 0.4115\n",
      "Epoch [47/200], Loss: 0.4124\n",
      "Epoch [48/200], Loss: 0.4115\n",
      "Epoch [49/200], Loss: 0.4115\n",
      "Epoch [50/200], Loss: 0.4115\n",
      "Epoch [51/200], Loss: 0.4182\n",
      "Epoch [52/200], Loss: 0.4182\n",
      "Epoch [53/200], Loss: 0.4182\n",
      "Epoch [54/200], Loss: 0.4226\n",
      "Epoch [55/200], Loss: 0.4115\n",
      "Epoch [56/200], Loss: 0.4182\n",
      "Epoch [57/200], Loss: 0.4320\n",
      "Epoch [58/200], Loss: 0.4115\n",
      "Epoch [59/200], Loss: 0.4250\n",
      "Epoch [60/200], Loss: 0.4250\n",
      "Validation Loss: 0.0196, Validation Accuracy: 0.8234\n",
      "Epoch [61/200], Loss: 0.4186\n",
      "Epoch [62/200], Loss: 0.4115\n",
      "Epoch [63/200], Loss: 0.4249\n",
      "Epoch [64/200], Loss: 0.4271\n",
      "Epoch [65/200], Loss: 0.4115\n",
      "Epoch [66/200], Loss: 0.4115\n",
      "Epoch [67/200], Loss: 0.4182\n",
      "Epoch [68/200], Loss: 0.4115\n",
      "Epoch [69/200], Loss: 0.4115\n",
      "Epoch [70/200], Loss: 0.4115\n",
      "Epoch [71/200], Loss: 0.4249\n",
      "Epoch [72/200], Loss: 0.4182\n",
      "Epoch [73/200], Loss: 0.4185\n",
      "Epoch [74/200], Loss: 0.4182\n",
      "Epoch [75/200], Loss: 0.4249\n",
      "Epoch [76/200], Loss: 0.4204\n",
      "Epoch [77/200], Loss: 0.4182\n",
      "Epoch [78/200], Loss: 0.4182\n",
      "Epoch [79/200], Loss: 0.4115\n",
      "Epoch [80/200], Loss: 0.4182\n",
      "Validation Loss: 0.0182, Validation Accuracy: 0.8578\n",
      "Epoch [81/200], Loss: 0.4182\n",
      "Epoch [82/200], Loss: 0.4249\n",
      "Epoch [83/200], Loss: 0.4115\n",
      "Epoch [84/200], Loss: 0.4115\n",
      "Epoch [85/200], Loss: 0.4249\n",
      "Epoch [86/200], Loss: 0.4182\n",
      "Epoch [87/200], Loss: 0.4182\n",
      "Epoch [88/200], Loss: 0.4182\n",
      "Epoch [89/200], Loss: 0.4258\n",
      "Epoch [90/200], Loss: 0.4182\n",
      "Epoch [91/200], Loss: 0.4182\n",
      "Epoch [92/200], Loss: 0.4249\n",
      "Epoch [93/200], Loss: 0.4249\n",
      "Epoch [94/200], Loss: 0.4182\n",
      "Epoch [95/200], Loss: 0.4182\n",
      "Epoch [96/200], Loss: 0.4249\n",
      "Epoch [97/200], Loss: 0.4182\n",
      "Epoch [98/200], Loss: 0.4115\n",
      "Epoch [99/200], Loss: 0.4115\n",
      "Epoch [100/200], Loss: 0.4137\n",
      "Validation Loss: 0.0175, Validation Accuracy: 0.8750\n",
      "Epoch [101/200], Loss: 0.4182\n",
      "Epoch [102/200], Loss: 0.4115\n",
      "Epoch [103/200], Loss: 0.4182\n",
      "Epoch [104/200], Loss: 0.4182\n",
      "Epoch [105/200], Loss: 0.4182\n",
      "Epoch [106/200], Loss: 0.4316\n",
      "Epoch [107/200], Loss: 0.4204\n",
      "Epoch [108/200], Loss: 0.4115\n",
      "Epoch [109/200], Loss: 0.4115\n",
      "Epoch [110/200], Loss: 0.4115\n",
      "Epoch [111/200], Loss: 0.4115\n",
      "Epoch [112/200], Loss: 0.4115\n",
      "Epoch [113/200], Loss: 0.4182\n",
      "Epoch [114/200], Loss: 0.4182\n",
      "Epoch [115/200], Loss: 0.4115\n",
      "Epoch [116/200], Loss: 0.4159\n",
      "Epoch [117/200], Loss: 0.4115\n",
      "Epoch [118/200], Loss: 0.4115\n",
      "Epoch [119/200], Loss: 0.4271\n",
      "Epoch [120/200], Loss: 0.4204\n",
      "Validation Loss: 0.0210, Validation Accuracy: 0.7891\n",
      "Epoch [121/200], Loss: 0.4182\n",
      "Epoch [122/200], Loss: 0.4182\n",
      "Epoch [123/200], Loss: 0.4182\n",
      "Epoch [124/200], Loss: 0.4204\n",
      "Epoch [125/200], Loss: 0.4249\n",
      "Epoch [126/200], Loss: 0.4115\n",
      "Epoch [127/200], Loss: 0.4182\n",
      "Epoch [128/200], Loss: 0.4115\n",
      "Epoch [129/200], Loss: 0.4115\n",
      "Epoch [130/200], Loss: 0.4115\n",
      "Epoch [131/200], Loss: 0.4182\n",
      "Epoch [132/200], Loss: 0.4115\n",
      "Epoch [133/200], Loss: 0.4316\n",
      "Epoch [134/200], Loss: 0.4204\n",
      "Epoch [135/200], Loss: 0.4115\n",
      "Epoch [136/200], Loss: 0.4249\n",
      "Epoch [137/200], Loss: 0.4271\n",
      "Epoch [138/200], Loss: 0.4271\n",
      "Epoch [139/200], Loss: 0.4115\n",
      "Epoch [140/200], Loss: 0.4182\n",
      "Validation Loss: 0.0189, Validation Accuracy: 0.8406\n",
      "Epoch [141/200], Loss: 0.4115\n",
      "Epoch [142/200], Loss: 0.4115\n",
      "Epoch [143/200], Loss: 0.4115\n",
      "Epoch [144/200], Loss: 0.4115\n",
      "Epoch [145/200], Loss: 0.4115\n",
      "Epoch [146/200], Loss: 0.4115\n",
      "Epoch [147/200], Loss: 0.4249\n",
      "Epoch [148/200], Loss: 0.4115\n",
      "Epoch [149/200], Loss: 0.4249\n",
      "Epoch [150/200], Loss: 0.4249\n",
      "Epoch [151/200], Loss: 0.4115\n",
      "Epoch [152/200], Loss: 0.4115\n",
      "Epoch [153/200], Loss: 0.4182\n",
      "Epoch [154/200], Loss: 0.4182\n",
      "Epoch [155/200], Loss: 0.4115\n",
      "Epoch [156/200], Loss: 0.4182\n",
      "Epoch [157/200], Loss: 0.4115\n",
      "Epoch [158/200], Loss: 0.4115\n",
      "Epoch [159/200], Loss: 0.4249\n",
      "Epoch [160/200], Loss: 0.4271\n",
      "Validation Loss: 0.0182, Validation Accuracy: 0.8578\n",
      "Epoch [161/200], Loss: 0.4182\n",
      "Epoch [162/200], Loss: 0.4115\n",
      "Epoch [163/200], Loss: 0.4122\n",
      "Epoch [164/200], Loss: 0.4191\n",
      "Epoch [165/200], Loss: 0.4182\n",
      "Epoch [166/200], Loss: 0.4115\n",
      "Epoch [167/200], Loss: 0.4115\n",
      "Epoch [168/200], Loss: 0.4182\n",
      "Epoch [169/200], Loss: 0.4115\n",
      "Epoch [170/200], Loss: 0.4182\n",
      "Epoch [171/200], Loss: 0.4249\n",
      "Epoch [172/200], Loss: 0.4182\n",
      "Epoch [173/200], Loss: 0.4249\n",
      "Epoch [174/200], Loss: 0.4316\n",
      "Epoch [175/200], Loss: 0.4182\n",
      "Epoch [176/200], Loss: 0.4204\n",
      "Epoch [177/200], Loss: 0.4115\n",
      "Epoch [178/200], Loss: 0.4204\n",
      "Epoch [179/200], Loss: 0.4317\n",
      "Epoch [180/200], Loss: 0.4182\n",
      "Validation Loss: 0.0189, Validation Accuracy: 0.8406\n",
      "Epoch [181/200], Loss: 0.4115\n",
      "Epoch [182/200], Loss: 0.4115\n",
      "Epoch [183/200], Loss: 0.4271\n",
      "Epoch [184/200], Loss: 0.4182\n",
      "Epoch [185/200], Loss: 0.4182\n",
      "Epoch [186/200], Loss: 0.4115\n",
      "Epoch [187/200], Loss: 0.4271\n",
      "Epoch [188/200], Loss: 0.4115\n",
      "Epoch [189/200], Loss: 0.4115\n",
      "Epoch [190/200], Loss: 0.4115\n",
      "Epoch [191/200], Loss: 0.4137\n",
      "Epoch [192/200], Loss: 0.4115\n",
      "Epoch [193/200], Loss: 0.4182\n",
      "Epoch [194/200], Loss: 0.4137\n",
      "Epoch [195/200], Loss: 0.4115\n",
      "Epoch [196/200], Loss: 0.4182\n",
      "Epoch [197/200], Loss: 0.4316\n",
      "Epoch [198/200], Loss: 0.4182\n",
      "Epoch [199/200], Loss: 0.4182\n",
      "Epoch [200/200], Loss: 0.4182\n",
      "Validation Loss: 0.0189, Validation Accuracy: 0.8406\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "model1 = CNN1D(input_dim=channels, kernel_size=time_steps, output_dim=2)\n",
    "model2 = Wavenet(input_dim=channels, output_dim=2, kernel_size=time_steps)\n",
    "model3 = LSTM(input_dim=channels, output_dim=2)\n",
    "\n",
    "# Train the model\n",
    "CNNtrain_loss, CNNval_los, CNNval_accuracy = train_using_optimizer(model1, train_loader, val_loader, MODEL_FOLDER)\n",
    "Wavetrain_loss, Waveval_los, Waveval_accuracy = train_using_optimizer(model2, train_loader, val_loader, MODEL_FOLDER)\n",
    "\n",
    "# Evaluate the model\n",
    "loss_CNN, acuracy_CNN = evaluate_model(model1, val_loader,'cuda:0')\n",
    "# loss_LSTM, acuracy_LSTM = evaluate_model(model3, val_loader,'cuda:0')\n",
    "loss_Wavenet, acuracy_Wavenet = evaluate_model(model2, val_loader,'cuda:0')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:39:35.327559900Z",
     "start_time": "2024-10-15T02:37:12.135689400Z"
    }
   },
   "id": "fbed17c81da1a1f3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T02:39:35.329588500Z",
     "start_time": "2024-10-15T02:39:35.315590400Z"
    }
   },
   "id": "2bac56b86f2719a9",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
