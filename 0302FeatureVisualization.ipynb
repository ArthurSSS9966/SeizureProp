{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# At the start of your notebook\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "# After heavy computations\n",
    "clear_output(wait=True)\n",
    "gc.collect()\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "RESULT_FOLDER = \"result\"\n",
    "MODEL_FOLDER = \"model\"\n",
    "model_names = ['Wavenet']  # 'CNN1D', 'Wavenet', 'S4', 'Resnet'\n",
    "# Do batch analysis to find the best hyperparameters\n",
    "seizures = [1, 2, 3, 5, 7]\n",
    "thresholds = [0.8]\n",
    "smooth_windows = [80]\n",
    "patientID = 'P65'  # P65\n",
    "seizureID = f'{patientID}SZ'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the data from one patient:\n",
    "\n",
    "p66_data = pickle.load(open(f'data/{patientID}/seizure_All_combined.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e49261ab5184e787",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_grey_matter_channels(matter: pd.DataFrame):\n",
    "    \"\"\"Extract grey matter channels from Matter file\"\"\"\n",
    "    # Get grey matter channels\n",
    "    selected_matter = matter[matter['MatterType'].isin(['G', 'A'])]\n",
    "    grey_matter_channels = selected_matter['ChannelNumber'].values\n",
    "    \n",
    "    return grey_matter_channels\n",
    "\n",
    "p66_data.matter = pd.read_csv(f'data/{patientID}/matter.csv')\n",
    "all_channels = np.arange(0, p66_data.channelNumber)\n",
    "grey_channel = extract_grey_matter_channels(p66_data.matter) - 1\n",
    "white_channel = np.setdiff1d(all_channels, grey_channel)\n",
    "\n",
    "seizure_data_grey = p66_data.ictal[:,:,grey_channel]\n",
    "seizure_data_white = p66_data.ictal[:,:,white_channel]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa00cf25b39ca515",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasetConstruct import EDFData\n",
    "p66_raw = pickle.load(open(f'data/{patientID}/seizure_SZ1.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da74fdd12182140",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "raw_grey = p66_raw.ictal[:, grey_channel]\n",
    "raw_white = p66_raw.ictal[:, white_channel]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb127528015710b4",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.signal import butter, sosfilt, decimate\n",
    "from scipy.signal import iirnotch, filtfilt\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from meegkit import dss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class UnifiedPreprocessingDebugger:\n",
    "    \"\"\"\n",
    "    Debug tool that processes complete data then separates grey/white matter for evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, complete_data, grey_channels, white_channels, sampling_rate, \n",
    "                 gridmap=None, plot_folder='unified_debug_results'):\n",
    "        \"\"\"\n",
    "        Initialize the debugger with complete data and channel indices\n",
    "        \n",
    "        Args:\n",
    "            complete_data: Complete EEG data [time, all_channels]\n",
    "            grey_channels: Indices of grey matter channels\n",
    "            white_channels: Indices of white matter channels\n",
    "            sampling_rate: Original sampling rate\n",
    "            gridmap: Gridmap information (optional)\n",
    "            plot_folder: Folder to save debug plots\n",
    "        \"\"\"\n",
    "        self.complete_data_original = complete_data.copy()\n",
    "        self.grey_channels = np.array(grey_channels)\n",
    "        self.white_channels = np.array(white_channels)\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.gridmap = gridmap\n",
    "        self.plot_folder = plot_folder\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(plot_folder, exist_ok=True)\n",
    "        \n",
    "        # Initialize data containers for each step\n",
    "        self.processing_steps = {}\n",
    "        self.classification_results = {}\n",
    "        \n",
    "        print(f\"Initialized unified debugger with:\")\n",
    "        print(f\"Complete data shape: {complete_data.shape}\")\n",
    "        print(f\"Grey matter channels: {len(grey_channels)} channels\")\n",
    "        print(f\"White matter channels: {len(white_channels)} channels\")\n",
    "        print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "    \n",
    "    def butter_bandpass(self, lowcut, highcut, fs, order=5):\n",
    "        \"\"\"Butterworth bandpass filter\"\"\"\n",
    "        return butter(order, [lowcut, highcut], fs=fs, btype='band', analog=False, output='sos')\n",
    "    \n",
    "    def butter_bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n",
    "        \"\"\"Apply butterworth bandpass filter\"\"\"\n",
    "        sos = self.butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        return sosfilt(sos, data, axis=0)\n",
    "    \n",
    "    def apply_bipolar_reference(self, data, gridmap):\n",
    "        \"\"\"Apply bipolar referencing based on gridmap\"\"\"\n",
    "        if gridmap is None:\n",
    "            print(\"Warning: No gridmap provided, skipping bipolar referencing\")\n",
    "            return data\n",
    "            \n",
    "        bipolar_data = np.zeros_like(data)\n",
    "        \n",
    "        for _, row in gridmap.iterrows():\n",
    "            channel_range = row['Channel']\n",
    "            start, end = map(int, channel_range.split(':'))\n",
    "            \n",
    "            # Apply bipolar referencing within electrode group\n",
    "            for j in range(start, min(end + 1, data.shape[1])):\n",
    "                if j + 1 <= end and j + 1 < data.shape[1]:\n",
    "                    bipolar_data[:, j] = data[:, j] - data[:, j + 1]\n",
    "                elif j < data.shape[1]:\n",
    "                    bipolar_data[:, j] = data[:, j]\n",
    "                    \n",
    "        return bipolar_data\n",
    "    \n",
    "    def remove_line_noise(self, data, gridmap, line_freqs=[60, 100], fs=2000):\n",
    "        \"\"\"Remove line noise using DSS\"\"\"\n",
    "        if gridmap is None:\n",
    "            print(\"Warning: No gridmap provided, applying line noise removal to all channels\")\n",
    "            cleaned_data = data.copy()\n",
    "            try:\n",
    "                for f0 in line_freqs:\n",
    "                    cleaned_data, _ = dss.dss_line_iter(cleaned_data, f0, fs)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: DSS line noise removal failed: {str(e)}\")\n",
    "            return cleaned_data\n",
    "        \n",
    "        cleaned_data = data.copy()\n",
    "        for _, row in gridmap.iterrows():\n",
    "            channel_range = row['Channel']\n",
    "            start, end = map(int, channel_range.split(':'))\n",
    "            \n",
    "            # Ensure indices are within bounds\n",
    "            start = max(0, start)\n",
    "            end = min(end, data.shape[1] - 1)\n",
    "            \n",
    "            if start < data.shape[1] and end >= start:\n",
    "                section = cleaned_data[:, start:end + 1]\n",
    "                try:\n",
    "                    for f0 in line_freqs:\n",
    "                        section, _ = dss.dss_line_iter(section, f0, fs)\n",
    "                    cleaned_data[:, start:end + 1] = section\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to remove line noise for channels {start}:{end}: {str(e)}\")\n",
    "                    \n",
    "        return cleaned_data\n",
    "    \n",
    "    def apply_whitening(self, data, lags=1):\n",
    "        \"\"\"Apply whitening using auto-regressive model\"\"\"\n",
    "        data = np.asarray(data)\n",
    "        whitened = np.zeros_like(data)\n",
    "        \n",
    "        for i in range(data.shape[1]):\n",
    "            signal = data[:, i]\n",
    "            try:\n",
    "                if len(signal) > lags + 10:  # Ensure enough data points\n",
    "                    model = AutoReg(signal, lags=lags).fit()\n",
    "                    predictions = model.predict(start=lags, end=len(signal) - 1)\n",
    "                    \n",
    "                    whitened[lags:, i] = signal[lags:] - predictions\n",
    "                    whitened[:lags, i] = signal[:lags] - np.mean(signal)\n",
    "                else:\n",
    "                    whitened[:, i] = signal\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Whitening failed for channel {i}: {str(e)}\")\n",
    "                whitened[:, i] = signal\n",
    "                \n",
    "        return whitened\n",
    "    \n",
    "    def normalize_signal(self, signal, reference):\n",
    "        \"\"\"Normalize signal using RobustScaler fitted on reference\"\"\"\n",
    "        normalized = np.zeros_like(signal)\n",
    "        \n",
    "        for i in range(signal.shape[1]):\n",
    "            try:\n",
    "                scaler = RobustScaler()\n",
    "                scaler.fit(reference[:, i].reshape(-1, 1))\n",
    "                normalized[:, i] = scaler.transform(signal[:, i].reshape(-1, 1)).squeeze()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Normalization failed for channel {i}: {str(e)}\")\n",
    "                normalized[:, i] = signal[:, i]\n",
    "                \n",
    "        return normalized\n",
    "    \n",
    "    def extract_channel_features(self, data, channels, n_samples=20):\n",
    "        \"\"\"Extract features from specific channels\"\"\"\n",
    "        if len(channels) == 0:\n",
    "            return np.array([]), np.array([])\n",
    "            \n",
    "        # Select channels\n",
    "        channel_data = data[:, channels]\n",
    "        \n",
    "        # Create multiple samples using time windows\n",
    "        window_size = max(500, channel_data.shape[0] // n_samples)\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            start_idx = i * window_size\n",
    "            end_idx = min(start_idx + window_size, channel_data.shape[0])\n",
    "            \n",
    "            if end_idx - start_idx < 100:  # Skip if window too small\n",
    "                continue\n",
    "                \n",
    "            window_data = channel_data[start_idx:end_idx, :]\n",
    "            \n",
    "            # Extract features for this window\n",
    "            window_features = []\n",
    "            \n",
    "            # Per-channel features\n",
    "            for ch in range(window_data.shape[1]):\n",
    "                ch_data = window_data[:, ch]\n",
    "                ch_features = [\n",
    "                    np.mean(ch_data),\n",
    "                    np.std(ch_data),\n",
    "                    np.var(ch_data),\n",
    "                    np.median(ch_data),\n",
    "                    np.percentile(ch_data, 25),\n",
    "                    np.percentile(ch_data, 75),\n",
    "                    np.max(ch_data) - np.min(ch_data),  # Range\n",
    "                    np.sqrt(np.mean(ch_data**2))  # RMS\n",
    "                ]\n",
    "                window_features.extend(ch_features)\n",
    "            \n",
    "            # Global features across all channels in this window\n",
    "            all_data = window_data.flatten()\n",
    "            global_features = [\n",
    "                np.mean(all_data),\n",
    "                np.std(all_data),\n",
    "                np.var(all_data),\n",
    "                np.median(all_data),\n",
    "                np.max(all_data) - np.min(all_data)\n",
    "            ]\n",
    "            window_features.extend(global_features)\n",
    "            \n",
    "            features_list.append(window_features)\n",
    "        \n",
    "        return np.array(features_list)\n",
    "    \n",
    "    def evaluate_classification(self, complete_data, step_name):\n",
    "        \"\"\"Evaluate classification performance using the complete processed data\"\"\"\n",
    "        print(f\"\\nEvaluating classification for: {step_name}\")\n",
    "        \n",
    "        # Check data quality\n",
    "        if np.any(np.isnan(complete_data)) or np.any(np.isinf(complete_data)):\n",
    "            print(f\"Warning: Invalid values found in {step_name}\")\n",
    "            return {'accuracy': 0, 'auc': 0, 'error': 'Invalid values'}\n",
    "        \n",
    "        try:\n",
    "            # Extract features for grey and white matter\n",
    "            grey_features = self.extract_channel_features(complete_data, self.grey_channels)\n",
    "            white_features = self.extract_channel_features(complete_data, self.white_channels)\n",
    "            \n",
    "            if len(grey_features) == 0 or len(white_features) == 0:\n",
    "                print(f\"No features extracted for {step_name}\")\n",
    "                return {'accuracy': 0, 'auc': 0, 'error': 'No features'}\n",
    "            \n",
    "            # Balance the dataset\n",
    "            min_samples = min(len(grey_features), len(white_features))\n",
    "            if min_samples < 4:\n",
    "                print(f\"Not enough samples for {step_name}: {min_samples}\")\n",
    "                return {'accuracy': 0, 'auc': 0, 'error': f'Insufficient samples: {min_samples}'}\n",
    "            \n",
    "            # Use equal number of samples from each class\n",
    "            grey_features = grey_features[:min_samples]\n",
    "            white_features = white_features[:min_samples]\n",
    "            \n",
    "            # Ensure same number of features\n",
    "            min_features = min(grey_features.shape[1], white_features.shape[1])\n",
    "            grey_features = grey_features[:, :min_features]\n",
    "            white_features = white_features[:, :min_features]\n",
    "            \n",
    "            # Combine data\n",
    "            X = np.vstack([grey_features, white_features])\n",
    "            y = np.array([1] * len(grey_features) + [0] * len(white_features))\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            # Cross-validation\n",
    "            svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "            cv_scores = cross_val_score(svm, X_scaled, y, cv=5, scoring='accuracy')\n",
    "            \n",
    "            # Also compute AUC\n",
    "            svm.fit(X_scaled, y)\n",
    "            y_prob = svm.predict_proba(X_scaled)[:, 1]\n",
    "            auc_score = roc_auc_score(y, y_prob)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': cv_scores.mean(),\n",
    "                'accuracy_std': cv_scores.std(),\n",
    "                'auc': auc_score,\n",
    "                'n_features': X.shape[1],\n",
    "                'n_samples': len(X),\n",
    "                'grey_samples': len(grey_features),\n",
    "                'white_samples': len(white_features)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Classification failed for {step_name}: {str(e)}\")\n",
    "            return {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "    \n",
    "    def run_step_by_step_analysis(self):\n",
    "        \"\"\"Run step-by-step preprocessing analysis\"\"\"\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"UNIFIED STEP-BY-STEP PREPROCESSING ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Start with complete data\n",
    "        current_data = self.complete_data_original.copy()\n",
    "        current_fs = self.sampling_rate\n",
    "        \n",
    "        # Step 0: Original raw data\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 0: Original Raw Data\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        self.processing_steps['0_original'] = {\n",
    "            'data': current_data.copy(),\n",
    "            'fs': current_fs\n",
    "        }\n",
    "        \n",
    "        result = self.evaluate_classification(current_data, \"Original Raw Data\")\n",
    "        self.classification_results['0_original'] = result\n",
    "        print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "        print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "        \n",
    "        # Step 1: Scale to microvolts\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 1: Scale to Microvolts (×1e6)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        current_data = current_data * 1e6\n",
    "        \n",
    "        self.processing_steps['1_scaled'] = {\n",
    "            'data': current_data.copy(),\n",
    "            'fs': current_fs\n",
    "        }\n",
    "        \n",
    "        result = self.evaluate_classification(current_data, \"After Scaling\")\n",
    "        self.classification_results['1_scaled'] = result\n",
    "        print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "        print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "        \n",
    "        # Step 2: Bipolar referencing\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 2: Bipolar Referencing\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            current_data = self.apply_bipolar_reference(current_data, self.gridmap)\n",
    "            \n",
    "            self.processing_steps['2_bipolar'] = {\n",
    "                'data': current_data.copy(),\n",
    "                'fs': current_fs\n",
    "            }\n",
    "            \n",
    "            result = self.evaluate_classification(current_data, \"After Bipolar Referencing\")\n",
    "            self.classification_results['2_bipolar'] = result\n",
    "            print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "            print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Bipolar referencing failed: {str(e)}\")\n",
    "            self.classification_results['2_bipolar'] = {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "        \n",
    "        # Step 3: Line noise removal\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 3: Line Noise Removal (60Hz, 100Hz)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            current_data = self.remove_line_noise(current_data, self.gridmap, [60, 100], current_fs)\n",
    "            \n",
    "            self.processing_steps['3_line_noise'] = {\n",
    "                'data': current_data.copy(),\n",
    "                'fs': current_fs\n",
    "            }\n",
    "            \n",
    "            result = self.evaluate_classification(current_data, \"After Line Noise Removal\")\n",
    "            self.classification_results['3_line_noise'] = result\n",
    "            print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "            print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Line noise removal failed: {str(e)}\")\n",
    "            self.classification_results['3_line_noise'] = {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "            # Use previous step data\n",
    "            self.processing_steps['3_line_noise'] = self.processing_steps['2_bipolar']\n",
    "        \n",
    "        # Step 4: Bandpass filtering\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 4: Bandpass Filter (1-127 Hz)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            for i in range(current_data.shape[1]):\n",
    "                current_data[:, i] = self.butter_bandpass_filter(\n",
    "                    current_data[:, i], lowcut=1, highcut=127, fs=current_fs\n",
    "                )\n",
    "            \n",
    "            self.processing_steps['4_bandpass'] = {\n",
    "                'data': current_data.copy(),\n",
    "                'fs': current_fs\n",
    "            }\n",
    "            \n",
    "            result = self.evaluate_classification(current_data, \"After Bandpass Filtering\")\n",
    "            self.classification_results['4_bandpass'] = result\n",
    "            print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "            print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Bandpass filtering failed: {str(e)}\")\n",
    "            self.classification_results['4_bandpass'] = {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "            self.processing_steps['4_bandpass'] = self.processing_steps['3_line_noise']\n",
    "        \n",
    "        # Step 5: Downsampling\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 5: Downsampling to 512 Hz\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            factor = current_fs // 512\n",
    "            if factor > 1:\n",
    "                current_data = decimate(current_data, factor, axis=0)\n",
    "                current_fs = 512\n",
    "                print(f\"Downsampled by factor {factor}, new fs: {current_fs}\")\n",
    "            else:\n",
    "                print(\"No downsampling needed (already at target rate)\")\n",
    "                \n",
    "            self.processing_steps['5_downsample'] = {\n",
    "                'data': current_data.copy(),\n",
    "                'fs': current_fs\n",
    "            }\n",
    "            \n",
    "            result = self.evaluate_classification(current_data, \"After Downsampling\")\n",
    "            self.classification_results['5_downsample'] = result\n",
    "            print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "            print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Downsampling failed: {str(e)}\")\n",
    "            self.classification_results['5_downsample'] = {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "            self.processing_steps['5_downsample'] = self.processing_steps['4_bandpass']\n",
    "        \n",
    "        # Step 6: Whitening\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 6: AR Whitening\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            current_data = self.apply_whitening(current_data)\n",
    "            \n",
    "            self.processing_steps['6_whitening'] = {\n",
    "                'data': current_data.copy(),\n",
    "                'fs': current_fs\n",
    "            }\n",
    "            \n",
    "            result = self.evaluate_classification(current_data, \"After Whitening\")\n",
    "            self.classification_results['6_whitening'] = result\n",
    "            print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "            print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Whitening failed: {str(e)}\")\n",
    "            self.classification_results['6_whitening'] = {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "            self.processing_steps['6_whitening'] = self.processing_steps['5_downsample']\n",
    "        \n",
    "        # Step 7: Normalization\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Step 7: RobustScaler Normalization\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Use the interictal data as reference for normalization\n",
    "            current_data = self.normalize_signal(current_data, current_data)\n",
    "            \n",
    "            self.processing_steps['7_normalized'] = {\n",
    "                'data': current_data.copy(),\n",
    "                'fs': current_fs\n",
    "            }\n",
    "            \n",
    "            result = self.evaluate_classification(current_data, \"After Normalization\")\n",
    "            self.classification_results['7_normalized'] = result\n",
    "            print(f\"Classification accuracy: {result.get('accuracy', 0):.3f}\")\n",
    "            print(f\"AUC: {result.get('auc', 0):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Normalization failed: {str(e)}\")\n",
    "            self.classification_results['7_normalized'] = {'accuracy': 0, 'auc': 0, 'error': str(e)}\n",
    "            self.processing_steps['7_normalized'] = self.processing_steps['6_whitening']\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot comprehensive results\"\"\"\n",
    "        \n",
    "        # Extract results\n",
    "        steps = []\n",
    "        accuracies = []\n",
    "        aucs = []\n",
    "        \n",
    "        for step_name, result in self.classification_results.items():\n",
    "            steps.append(step_name.replace('_', ' ').title())\n",
    "            accuracies.append(result.get('accuracy', 0))\n",
    "            aucs.append(result.get('auc', 0))\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        bars1 = ax1.bar(range(len(steps)), accuracies, alpha=0.7, color='skyblue')\n",
    "        ax1.set_xlabel('Preprocessing Steps')\n",
    "        ax1.set_ylabel('Classification Accuracy')\n",
    "        ax1.set_title('Classification Accuracy vs Preprocessing Steps')\n",
    "        ax1.set_xticks(range(len(steps)))\n",
    "        ax1.set_xticklabels(steps, rotation=45, ha='right')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Color bars based on performance\n",
    "        for bar, acc in zip(bars1, accuracies):\n",
    "            if acc > 0.8:\n",
    "                bar.set_color('green')\n",
    "            elif acc > 0.6:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('red')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            ax1.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot AUC\n",
    "        bars2 = ax2.bar(range(len(steps)), aucs, alpha=0.7, color='lightcoral')\n",
    "        ax2.set_xlabel('Preprocessing Steps')\n",
    "        ax2.set_ylabel('AUC Score')\n",
    "        ax2.set_title('AUC Score vs Preprocessing Steps')\n",
    "        ax2.set_xticks(range(len(steps)))\n",
    "        ax2.set_xticklabels(steps, rotation=45, ha='right')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, auc in enumerate(aucs):\n",
    "            ax2.text(i, auc + 0.01, f'{auc:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.plot_folder, 'unified_preprocessing_analysis.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Signal characteristics plot\n",
    "        self.plot_signal_characteristics()\n",
    "    \n",
    "    def plot_signal_characteristics(self):\n",
    "        \"\"\"Plot signal characteristics evolution\"\"\"\n",
    "        \n",
    "        # Select a few steps to compare\n",
    "        key_steps = ['0_original', '1_scaled', '4_bandpass', '6_whitening', '7_normalized']\n",
    "        available_steps = [step for step in key_steps if step in self.processing_steps]\n",
    "        \n",
    "        fig, axes = plt.subplots(len(available_steps), 2, figsize=(16, 4*len(available_steps)))\n",
    "        if len(available_steps) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, step_key in enumerate(available_steps):\n",
    "            step_data = self.processing_steps[step_key]['data']\n",
    "            \n",
    "            # Sample a single channel for grey and white matter\n",
    "            grey_ch = self.grey_channels[0] if len(self.grey_channels) > 0 else 0\n",
    "            white_ch = self.white_channels[0] if len(self.white_channels) > 0 else min(1, step_data.shape[1]-1)\n",
    "            \n",
    "            sample_length = min(2000, step_data.shape[0])\n",
    "            \n",
    "            # Plot grey matter channel\n",
    "            axes[i, 0].plot(step_data[:sample_length, grey_ch], alpha=0.8)\n",
    "            axes[i, 0].set_title(f'{step_key.replace(\"_\", \" \").title()} - Grey Matter Channel')\n",
    "            axes[i, 0].set_ylabel('Amplitude')\n",
    "            axes[i, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot white matter channel\n",
    "            axes[i, 1].plot(step_data[:sample_length, white_ch], alpha=0.8, color='red')\n",
    "            axes[i, 1].set_title(f'{step_key.replace(\"_\", \" \").title()} - White Matter Channel')\n",
    "            axes[i, 1].set_ylabel('Amplitude')\n",
    "            axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[-1, 0].set_xlabel('Time Samples')\n",
    "        axes[-1, 1].set_xlabel('Time Samples')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.plot_folder, 'signal_evolution.png'), dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive report\"\"\"\n",
    "        \n",
    "        report_path = os.path.join(self.plot_folder, 'unified_preprocessing_report.txt')\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"UNIFIED PREPROCESSING DEBUG REPORT\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"DATA SUMMARY:\\n\")\n",
    "            f.write(f\"Complete data shape: {self.complete_data_original.shape}\\n\")\n",
    "            f.write(f\"Grey matter channels: {len(self.grey_channels)}\\n\")\n",
    "            f.write(f\"White matter channels: {len(self.white_channels)}\\n\")\n",
    "            f.write(f\"Original sampling rate: {self.sampling_rate} Hz\\n\\n\")\n",
    "            \n",
    "            f.write(\"CLASSIFICATION PERFORMANCE BY STEP:\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            \n",
    "            best_accuracy = 0\n",
    "            best_step = \"\"\n",
    "            worst_accuracy = 1\n",
    "            worst_step = \"\"\n",
    "            \n",
    "            for step_name, result in self.classification_results.items():\n",
    "                accuracy = result.get('accuracy', 0)\n",
    "                auc = result.get('auc', 0)\n",
    "                f.write(f\"{step_name.replace('_', ' ').title()}: Accuracy={accuracy:.3f}, AUC={auc:.3f}\\n\")\n",
    "                \n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_step = step_name\n",
    "                \n",
    "                if accuracy < worst_accuracy and accuracy > 0:\n",
    "                    worst_accuracy = accuracy\n",
    "                    worst_step = step_name\n",
    "            \n",
    "            f.write(f\"\\nBEST PERFORMANCE: {best_step.replace('_', ' ').title()} (Accuracy: {best_accuracy:.3f})\\n\")\n",
    "            f.write(f\"WORST PERFORMANCE: {worst_step.replace('_', ' ').title()} (Accuracy: {worst_accuracy:.3f})\\n\")\n",
    "            \n",
    "            # Performance analysis\n",
    "            f.write(f\"\\nPERFORMANCE CHANGES:\\n\")\n",
    "            f.write(\"-\"*25 + \"\\n\")\n",
    "            \n",
    "            original_acc = self.classification_results['0_original'].get('accuracy', 0)\n",
    "            final_acc = list(self.classification_results.values())[-1].get('accuracy', 0)\n",
    "            \n",
    "            f.write(f\"Original accuracy: {original_acc:.3f}\\n\")\n",
    "            f.write(f\"Final accuracy: {final_acc:.3f}\\n\")\n",
    "            f.write(f\"Net change: {final_acc - original_acc:+.3f}\\n\\n\")\n",
    "            \n",
    "            # Step-by-step changes\n",
    "            prev_acc = original_acc\n",
    "            for step_name, result in list(self.classification_results.items())[1:]:\n",
    "                current_acc = result.get('accuracy', 0)\n",
    "                change = current_acc - prev_acc\n",
    "                \n",
    "                if abs(change) > 0.05:  # Significant change\n",
    "                    direction = \"↗ IMPROVED\" if change > 0 else \"↘ DEGRADED\"\n",
    "                    f.write(f\"{step_name.replace('_', ' ').title()}: {direction} by {abs(change):.3f}\\n\")\n",
    "                \n",
    "                prev_acc = current_acc\n",
    "            \n",
    "            f.write(f\"\\nRECOMMENDATIONS:\\n\")\n",
    "            f.write(\"-\"*18 + \"\\n\")\n",
    "            \n",
    "            if best_step == '0_original':\n",
    "                f.write(\"   Raw data performs best! Consider minimal preprocessing.\\n\")\n",
    "                f.write(\"   All preprocessing steps appear to degrade performance.\\n\")\n",
    "            else:\n",
    "                f.write(f\"Best performance at: {best_step.replace('_', ' ').title()}\\n\")\n",
    "                f.write(\"   Consider stopping preprocessing at this step.\\n\")\n",
    "            \n",
    "            # Identify most harmful steps\n",
    "            harmful_steps = []\n",
    "            prev_acc = original_acc\n",
    "            \n",
    "            for step_name, result in list(self.classification_results.items())[1:]:\n",
    "                current_acc = result.get('accuracy', 0)\n",
    "                if current_acc < prev_acc - 0.05:  # Significant drop\n",
    "                    harmful_steps.append((step_name, prev_acc - current_acc))\n",
    "                prev_acc = current_acc\n",
    "            \n",
    "            if harmful_steps:\n",
    "                f.write(f\"\\n🚨 MOST HARMFUL STEPS:\\n\")\n",
    "                for step, drop in sorted(harmful_steps, key=lambda x: x[1], reverse=True):\n",
    "                    f.write(f\"   - {step.replace('_', ' ').title()}: -{drop:.3f}\\n\")\n",
    "        \n",
    "        print(f\"\\nComprehensive report saved to: {report_path}\")\n",
    "\n",
    "# Main function to run the unified analysis\n",
    "def run_unified_preprocessing_debug(complete_data, grey_channels, white_channels, \n",
    "                                   sampling_rate, gridmap=None, \n",
    "                                   output_folder='unified_debug_results'):\n",
    "    \"\"\"\n",
    "    Main function to run the unified preprocessing debug analysis\n",
    "    \n",
    "    Args:\n",
    "        complete_data: Complete EEG data [time, all_channels]\n",
    "        grey_channels: List/array of grey matter channel indices\n",
    "        white_channels: List/array of white matter channel indices\n",
    "        sampling_rate: Original sampling rate\n",
    "        gridmap: Gridmap information (optional)\n",
    "        output_folder: Folder to save debug results\n",
    "    \n",
    "    Returns:\n",
    "        UnifiedPreprocessingDebugger object with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting Unified Preprocessing Debug Analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize debugger\n",
    "    debugger = UnifiedPreprocessingDebugger(\n",
    "        complete_data, grey_channels, white_channels, \n",
    "        sampling_rate, gridmap, output_folder\n",
    "    )\n",
    "    \n",
    "    # Run step-by-step analysis\n",
    "    debugger.run_step_by_step_analysis()\n",
    "    \n",
    "    # Generate plots\n",
    "    debugger.plot_results()\n",
    "    \n",
    "    # Generate report\n",
    "    debugger.generate_report()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"Unified debug analysis complete!\")\n",
    "    print(f\"Results saved to: {output_folder}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return debugger"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e89a6e3610b739d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "debugger = run_unified_preprocessing_debug(p66_raw.interictal, grey_channel, white_channel, \n",
    "                                           sampling_rate=p66_raw.samplingRate, \n",
    "                                           gridmap=p66_raw.gridmap, \n",
    "                                           output_folder='unified_debug_results')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2b4b95f5dba4800",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "channel_names = p66_data.matter['ElectrodeName'].values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda9017ae9535797",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine the segment data for seizure_data_grey and seizure_data_white\n",
    "seizure_data_grey_new = np.concatenate(seizure_data_grey, axis=0)\n",
    "seizure_data_white_new = np.concatenate(seizure_data_white, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47aebafcb78e1824",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def improved_extract_and_classify_features_strict(grey_matter_data, white_matter_data, \n",
    "                                         plot_folder='result/improved_wg_classification_strict', \n",
    "                                         fs=250, \n",
    "                                         use_windowing=True, \n",
    "                                         n_windows_per_channel=None,  # Will be calculated based on EEG extractor style\n",
    "                                         window_overlap=None,         # Will use EEG extractor step size\n",
    "                                         min_window_length=None,      # Will use EEG extractor window size\n",
    "                                         validation_type='strict',\n",
    "                                         max_samples_per_channel=20):\n",
    "    \"\"\"\n",
    "    Improved version with STRICT channel-wise validation to prevent data leakage\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grey_matter_data : np.ndarray\n",
    "        Grey matter data with shape [time, channels]\n",
    "    white_matter_data : np.ndarray\n",
    "        White matter data with shape [time, channels]\n",
    "    plot_folder : str\n",
    "        Folder path to save all plots\n",
    "    fs : int\n",
    "        Sampling frequency in Hz\n",
    "    use_windowing : bool\n",
    "        Whether to use time windowing to increase sample size\n",
    "    n_windows_per_channel : int, optional\n",
    "        Maximum number of windows per channel (will be calculated automatically if None)\n",
    "    window_overlap : float, optional\n",
    "        Overlap between windows (0-1) - if None, uses EEG extractor step size\n",
    "    min_window_length : int, optional\n",
    "        Minimum window length in samples - if None, uses EEG extractor window size\n",
    "    validation_type : str\n",
    "        'strict' = channel-wise split, 'normal' = random split\n",
    "    max_samples_per_channel : int\n",
    "        Maximum number of samples per channel to prevent overfitting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing classification results and analysis\n",
    "    \"\"\"\n",
    "    # Create the plot folder if it doesn't exist\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    \n",
    "    # Get dimensions\n",
    "    grey_time, grey_channels = grey_matter_data.shape\n",
    "    white_time, white_channels = white_matter_data.shape\n",
    "    \n",
    "    print(f\"VALIDATION MODE: {validation_type}\")\n",
    "    print(f\"Grey matter data shape: [{grey_time}, {grey_channels}]\")\n",
    "    print(f\"White matter data shape: [{white_time}, {white_channels}]\")\n",
    "    print(f\"Windowing approach: {use_windowing}\")\n",
    "    print(f\"Max samples per channel: {max_samples_per_channel}\")\n",
    "    \n",
    "    # Calculate windowing parameters if not provided\n",
    "    if use_windowing:\n",
    "        # Set default EEG extractor style parameters if not provided\n",
    "        if min_window_length is None:\n",
    "            window_duration = 0.05  # 50ms\n",
    "            min_window_length = int(window_duration * fs)\n",
    "        \n",
    "        if window_overlap is None:\n",
    "            window_step = 0.025     # 25ms step\n",
    "            step_samples = int(window_step * fs)\n",
    "        else:\n",
    "            # Calculate step from overlap\n",
    "            step_samples = int(min_window_length * (1 - window_overlap))\n",
    "        \n",
    "        # Calculate expected number of windows\n",
    "        if n_windows_per_channel is None:\n",
    "            max_windows_grey = (grey_time - min_window_length) // step_samples + 1\n",
    "            max_windows_white = (white_time - min_window_length) // step_samples + 1\n",
    "            n_windows_per_channel = min(max_windows_grey, max_windows_white)\n",
    "            \n",
    "        print(f\"Window parameters:\")\n",
    "        print(f\"  Window size: {min_window_length} samples ({min_window_length/fs:.3f}s)\")\n",
    "        print(f\"  Step size: {step_samples} samples ({step_samples/fs:.3f}s)\")\n",
    "        print(f\"  Expected windows per channel: {n_windows_per_channel}\")\n",
    "    else:\n",
    "        min_window_length = None\n",
    "        step_samples = None\n",
    "        n_windows_per_channel = 1\n",
    "    \n",
    "    # Define frequency bands (keeping EEGTimeSeriesFeatureExtractor bands)\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),        \n",
    "        'alpha': (8, 13),       \n",
    "        'beta': (13, 30),       \n",
    "        'gamma': (30, 100),     \n",
    "        'high_gamma': (100, min(200, fs//2))\n",
    "    }\n",
    "    \n",
    "    def extract_half_wave_features(signal, amplitude_threshold=0.1):\n",
    "        \"\"\"Extract half-wave features\"\"\"\n",
    "        # Find zero crossings\n",
    "        zero_crossings = np.where(np.diff(np.signbit(signal)))[0]\n",
    "        \n",
    "        if len(zero_crossings) <= 1:\n",
    "            return {\n",
    "                'hw_count': 0,\n",
    "                'hw_mean_amp': 0,\n",
    "                'hw_mean_duration': 0\n",
    "            }\n",
    "        \n",
    "        half_wave_amps = []\n",
    "        half_wave_durations = []\n",
    "        \n",
    "        for i in range(len(zero_crossings) - 1):\n",
    "            start_idx = zero_crossings[i]\n",
    "            end_idx = zero_crossings[i + 1]\n",
    "            duration = end_idx - start_idx\n",
    "            segment = signal[start_idx:end_idx]\n",
    "            \n",
    "            if len(segment) > 0:\n",
    "                amplitude = np.max(np.abs(segment))\n",
    "                if amplitude >= amplitude_threshold:\n",
    "                    half_wave_amps.append(amplitude)\n",
    "                    half_wave_durations.append(duration)\n",
    "        \n",
    "        return {\n",
    "            'hw_count': len(half_wave_amps),\n",
    "            'hw_mean_amp': np.mean(half_wave_amps) if half_wave_amps else 0,\n",
    "            'hw_mean_duration': np.mean(half_wave_durations) / fs if half_wave_durations else 0\n",
    "        }\n",
    "    \n",
    "    def extract_comprehensive_features_from_window(signal):\n",
    "        \"\"\"Extract comprehensive features from a signal window\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Skip if signal is too short or all zeros\n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Time domain statistical features\n",
    "            features['mean'] = np.mean(signal)\n",
    "            features['std'] = np.std(signal)\n",
    "            features['median'] = np.median(signal)\n",
    "            features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "            \n",
    "            # Handle edge cases for skew and kurtosis\n",
    "            try:\n",
    "                features['skew'] = stats.skew(signal)\n",
    "                features['kurtosis'] = stats.kurtosis(signal)\n",
    "            except:\n",
    "                features['skew'] = 0\n",
    "                features['kurtosis'] = 0\n",
    "                \n",
    "            features['range'] = np.max(signal) - np.min(signal)\n",
    "            features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "            features['zero_crossings'] = np.sum(np.diff(np.signbit(signal).astype(int)) != 0)\n",
    "            \n",
    "            # Half-wave features\n",
    "            hw_features = extract_half_wave_features(signal)\n",
    "            features.update(hw_features)\n",
    "            \n",
    "            # Line length and area features\n",
    "            features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "            features['area'] = np.sum(np.abs(signal))\n",
    "            \n",
    "            # Frequency domain features using FFT\n",
    "            try:\n",
    "                # Apply Hamming window to reduce spectral leakage\n",
    "                windowed_signal = signal * np.hamming(len(signal))\n",
    "                \n",
    "                # Compute FFT\n",
    "                from scipy.fftpack import fft\n",
    "                fft_vals = fft(windowed_signal)\n",
    "                fft_abs = np.abs(fft_vals[:len(signal) // 2])\n",
    "                \n",
    "                # Normalize by window length\n",
    "                fft_abs = fft_abs / len(signal)\n",
    "                \n",
    "                # Calculate frequency bins\n",
    "                freq_bins = np.fft.fftfreq(len(signal), 1 / fs)[:len(signal) // 2]\n",
    "                \n",
    "                # Band powers\n",
    "                for band_name, (low_freq, high_freq) in freq_bands.items():\n",
    "                    band_mask = (freq_bins >= low_freq) & (freq_bins <= high_freq)\n",
    "                    if np.any(band_mask):\n",
    "                        band_power = np.sum(fft_abs[band_mask] ** 2)\n",
    "                        features[f'power_{band_name}'] = band_power\n",
    "                    else:\n",
    "                        features[f'power_{band_name}'] = 0\n",
    "                \n",
    "                # Total power\n",
    "                total_power = sum([features[f'power_{band}'] for band in freq_bands.keys()])\n",
    "                features['total_power'] = total_power\n",
    "                \n",
    "                # Spectral edge frequency (95%)\n",
    "                if len(fft_abs) > 0 and total_power > 0:\n",
    "                    cumulative_power = np.cumsum(fft_abs ** 2)\n",
    "                    edge_95_idx = np.argmax(cumulative_power >= 0.95 * np.sum(fft_abs ** 2))\n",
    "                    features['spectral_edge_freq'] = freq_bins[edge_95_idx] if edge_95_idx > 0 else freq_bins[-1]\n",
    "                else:\n",
    "                    features['spectral_edge_freq'] = 0\n",
    "                \n",
    "                # Spectral entropy\n",
    "                if total_power > 0:\n",
    "                    power_spectrum = fft_abs ** 2\n",
    "                    pxx_norm = power_spectrum / np.sum(power_spectrum)\n",
    "                    features['spectral_entropy'] = -np.sum(pxx_norm * np.log2(pxx_norm + 1e-10))\n",
    "                else:\n",
    "                    features['spectral_entropy'] = 0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Spectral analysis failed: {str(e)}\")\n",
    "                # Fill with zeros if spectral analysis fails\n",
    "                for band_name in freq_bands.keys():\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                features['total_power'] = 0\n",
    "                features['spectral_edge_freq'] = 0\n",
    "                features['spectral_entropy'] = 0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Feature extraction failed: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def create_channel_separated_samples(data, tissue_type):\n",
    "        \"\"\"\n",
    "        Create samples with channel tracking for strict validation\n",
    "        Uses the calculated windowing parameters\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        channel_info = []\n",
    "        n_time, n_channels = data.shape\n",
    "        \n",
    "        if use_windowing:\n",
    "            # Use the calculated parameters\n",
    "            window_samples = min_window_length\n",
    "            \n",
    "            print(f\"{tissue_type} - Window: {window_samples} samples, Step: {step_samples} samples\")\n",
    "            \n",
    "            for ch in range(n_channels):\n",
    "                channel_data = data[:, ch]\n",
    "                channel_samples = []\n",
    "                \n",
    "                # Create windows for this channel\n",
    "                for start_idx in range(0, len(channel_data) - window_samples + 1, step_samples):\n",
    "                    end_idx = start_idx + window_samples\n",
    "                    window_data = channel_data[start_idx:end_idx]\n",
    "                    \n",
    "                    features = extract_comprehensive_features_from_window(window_data)\n",
    "                    if features is not None:\n",
    "                        channel_samples.append(features)\n",
    "                    \n",
    "                    # Stop if we've reached the desired number of windows\n",
    "                    if len(channel_samples) >= n_windows_per_channel:\n",
    "                        break\n",
    "                \n",
    "                # Limit samples per channel to prevent overfitting\n",
    "                if max_samples_per_channel and len(channel_samples) > max_samples_per_channel:\n",
    "                    # Randomly sample to avoid bias\n",
    "                    indices = np.random.choice(len(channel_samples), max_samples_per_channel, replace=False)\n",
    "                    channel_samples = [channel_samples[i] for i in sorted(indices)]\n",
    "                \n",
    "                # Add all samples from this channel\n",
    "                for sample in channel_samples:\n",
    "                    samples.append(sample)\n",
    "                    channel_info.append(ch)  # Track which channel this sample came from\n",
    "                    \n",
    "        else:\n",
    "            # Per-channel approach (no windowing)\n",
    "            for ch in range(n_channels):\n",
    "                channel_data = data[:, ch]\n",
    "                features = extract_comprehensive_features_from_window(channel_data)\n",
    "                if features is not None:\n",
    "                    samples.append(features)\n",
    "                    channel_info.append(ch)\n",
    "        \n",
    "        return samples, channel_info\n",
    "    \n",
    "    # Extract features with channel tracking\n",
    "    print(\"\\nExtracting features with channel separation...\")\n",
    "    grey_samples, grey_channel_info = create_channel_separated_samples(\n",
    "        grey_matter_data, \"Grey Matter\"\n",
    "    )\n",
    "    white_samples, white_channel_info = create_channel_separated_samples(\n",
    "        white_matter_data, \"White Matter\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Grey matter samples created: {len(grey_samples)} from {len(set(grey_channel_info))} channels\")\n",
    "    print(f\"White matter samples created: {len(white_samples)} from {len(set(white_channel_info))} channels\")\n",
    "    \n",
    "    if len(grey_samples) == 0 or len(white_samples) == 0:\n",
    "        return {'error': 'No samples created'}\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    grey_df = pd.DataFrame(grey_samples)\n",
    "    white_df = pd.DataFrame(white_samples)\n",
    "    \n",
    "    # Add channel info for validation\n",
    "    grey_df['channel_id'] = grey_channel_info\n",
    "    white_df['channel_id'] = [ch + grey_channels for ch in white_channel_info]  # Offset white matter channel IDs\n",
    "    \n",
    "    # Clean data\n",
    "    def clean_dataframe(df, name):\n",
    "        original_shape = df.shape\n",
    "        \n",
    "        # Handle NaNs\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if df[numeric_cols].isna().any().any():\n",
    "            print(f\"Found NaN values in {name}, filling with column medians\")\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        \n",
    "        # Handle infinites\n",
    "        for col in numeric_cols:\n",
    "            if np.any(np.isinf(df[col])):\n",
    "                print(f\"Found infinite values in {name} column {col}\")\n",
    "                finite_mask = np.isfinite(df[col])\n",
    "                if np.any(finite_mask):\n",
    "                    df.loc[~finite_mask, col] = df.loc[finite_mask, col].median()\n",
    "                else:\n",
    "                    df[col] = 0\n",
    "        \n",
    "        print(f\"Cleaned {name}: {original_shape} -> {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    grey_df = clean_dataframe(grey_df, \"grey matter features\")\n",
    "    white_df = clean_dataframe(white_df, \"white matter features\")\n",
    "    \n",
    "    # Ensure both dataframes have the same feature columns\n",
    "    feature_cols = [col for col in grey_df.columns if col != 'channel_id']\n",
    "    common_features = list(set(feature_cols).intersection(set([col for col in white_df.columns if col != 'channel_id'])))\n",
    "    \n",
    "    if len(common_features) == 0:\n",
    "        return {'error': 'No common features'}\n",
    "    \n",
    "    print(f\"Using {len(common_features)} common features\")\n",
    "    \n",
    "    # Add labels\n",
    "    grey_df['Matter'] = 'Grey'\n",
    "    white_df['Matter'] = 'White'\n",
    "    \n",
    "    # Combine data\n",
    "    combined_df = pd.concat([grey_df, white_df], ignore_index=True)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = combined_df[common_features]\n",
    "    y = combined_df['Matter'].map({'Grey': 1, 'White': 0})\n",
    "    channel_ids = combined_df['channel_id'].values\n",
    "    \n",
    "    print(f\"Final dataset: {len(X)} samples × {len(common_features)} features\")\n",
    "    print(f\"Sample-to-feature ratio: {len(X) / len(common_features):.1f}:1\")\n",
    "    \n",
    "    # VALIDATION: Check for perfect separability (data leakage detection)\n",
    "    def check_perfect_separability(X, y, feature_names):\n",
    "        \"\"\"Check if any single feature perfectly separates the classes\"\"\"\n",
    "        perfect_features = []\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            feature_vals = X.iloc[:, i]\n",
    "            grey_vals = feature_vals[y == 1]\n",
    "            white_vals = feature_vals[y == 0]\n",
    "            \n",
    "            # Check if ranges don't overlap\n",
    "            grey_min, grey_max = grey_vals.min(), grey_vals.max()\n",
    "            white_min, white_max = white_vals.min(), white_vals.max()\n",
    "            \n",
    "            if grey_max < white_min or white_max < grey_min:\n",
    "                perfect_features.append(feature)\n",
    "                print(f\"⚠️  PERFECT SEPARATION found in feature '{feature}':\")\n",
    "                print(f\"   Grey range: [{grey_min:.3f}, {grey_max:.3f}]\")\n",
    "                print(f\"   White range: [{white_min:.3f}, {white_max:.3f}]\")\n",
    "        \n",
    "        return perfect_features\n",
    "    \n",
    "    # Check for perfect separability\n",
    "    perfect_features = check_perfect_separability(X, y, common_features)\n",
    "    if perfect_features:\n",
    "        print(f\"\\n🚨 WARNING: Found {len(perfect_features)} features with perfect separation!\")\n",
    "        print(\"This may indicate data leakage or preprocessing artifacts.\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # STRICT VALIDATION: Channel-wise train/test split\n",
    "    if validation_type == 'strict':\n",
    "        print(\"\\n🔒 STRICT VALIDATION: Channel-wise train/test split\")\n",
    "        \n",
    "        # Get unique channels for each class\n",
    "        grey_channels_used = combined_df[combined_df['Matter'] == 'Grey']['channel_id'].unique()\n",
    "        white_channels_used = combined_df[combined_df['Matter'] == 'White']['channel_id'].unique()\n",
    "        \n",
    "        print(f\"Grey matter: {len(grey_channels_used)} unique channels\")\n",
    "        print(f\"White matter: {len(white_channels_used)} unique channels\")\n",
    "        \n",
    "        # Split channels (not samples) into train/test\n",
    "        grey_train_channels, grey_test_channels = train_test_split(\n",
    "            grey_channels_used, test_size=0.3, random_state=42\n",
    "        )\n",
    "        white_train_channels, white_test_channels = train_test_split(\n",
    "            white_channels_used, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create train/test masks based on channels\n",
    "        train_mask = (\n",
    "            (combined_df['Matter'] == 'Grey') & (combined_df['channel_id'].isin(grey_train_channels)) |\n",
    "            (combined_df['Matter'] == 'White') & (combined_df['channel_id'].isin(white_train_channels))\n",
    "        )\n",
    "        test_mask = ~train_mask\n",
    "        \n",
    "        X_train, X_test = X_scaled[train_mask], X_scaled[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        \n",
    "        print(f\"Channel-wise split: Train={len(X_train)}, Test={len(X_test)}\")\n",
    "        \n",
    "    else:\n",
    "        # Standard random train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "    \n",
    "    # Apply PCA analysis\n",
    "    variance_threshold = 0.95\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, min(21, len(explained_variance) + 1)), \n",
    "            explained_variance[:20], alpha=0.7, label='Individual')\n",
    "    plt.step(range(1, min(21, len(cumulative_variance) + 1)), \n",
    "             cumulative_variance[:20], where='mid', label='Cumulative')\n",
    "    plt.axhline(y=variance_threshold, color='r', linestyle='--', \n",
    "                label=f'{variance_threshold*100}% Variance')\n",
    "    \n",
    "    # Find number of components for threshold\n",
    "    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
    "    if n_components < len(cumulative_variance):\n",
    "        plt.axvline(x=n_components, color='r', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title(f'PCA Explained Variance ({validation_type.upper()} Validation)')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(1, 20)\n",
    "    plt.savefig(os.path.join(plot_folder, f'pca_explained_variance_{validation_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Number of components for {variance_threshold*100}% variance: {n_components}\")\n",
    "    \n",
    "    # Apply PCA with determined number of components\n",
    "    n_components = min(n_components, len(common_features), 50)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Visualize PCA results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', alpha=0.6, s=20)\n",
    "    plt.colorbar(scatter, label='Grey Matter (1) vs White Matter (0)')\n",
    "    plt.xlabel(f'PC1 ({explained_variance[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({explained_variance[1]:.1%} variance)')\n",
    "    plt.title(f'PCA of Grey vs White Matter Features ({validation_type.upper()} Validation)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, f'pca_2d_scatter_{validation_type}.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Classification with multiple algorithms\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM (RBF kernel)': SVC(probability=True, random_state=42, C=1.0),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "        'MLP Neural Network': MLPClassifier(max_iter=1000, random_state=42, \n",
    "                                           hidden_layer_sizes=(50,), alpha=0.01),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'LDA': LDA()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # For strict validation, use channel-wise cross-validation\n",
    "    if validation_type == 'strict':\n",
    "        print(\"\\n🔒 Using channel-wise cross-validation...\")\n",
    "        from sklearn.model_selection import GroupKFold\n",
    "        groups = channel_ids\n",
    "        cv = GroupKFold(n_splits=5)\n",
    "    else:\n",
    "        # Standard stratified CV\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        groups = None\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            if validation_type == 'strict':\n",
    "                cv_scores = cross_val_score(clf, X_scaled, y, cv=cv, groups=groups, scoring='accuracy')\n",
    "            else:\n",
    "                cv_scores = cross_val_score(clf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "            \n",
    "            # Train and test\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # ROC curve if possible\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'cv_accuracy': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "            }\n",
    "            \n",
    "            # Feature importance if available\n",
    "            if hasattr(clf, \"feature_importances_\"):\n",
    "                results[name]['importance'] = clf.feature_importances_\n",
    "            elif hasattr(clf, \"coef_\"):\n",
    "                results[name]['importance'] = np.abs(clf.coef_[0]) if clf.coef_.ndim > 1 else np.abs(clf.coef_)\n",
    "            \n",
    "            print(f\"  CV Accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "            print(f\"  Test Accuracy: {test_accuracy:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    # Finalize ROC plot\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {validation_type.upper()} Validation')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, f'roc_curves_{validation_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create performance summary\n",
    "    performance_summary = []\n",
    "    for name, result in results.items():\n",
    "        if 'error' not in result:\n",
    "            performance_summary.append({\n",
    "                'Classifier': name,\n",
    "                'CV Accuracy': f\"{result['cv_accuracy']:.3f} ± {result['cv_std']:.3f}\",\n",
    "                'Test Accuracy': result['test_accuracy'],\n",
    "                'Precision (Grey)': result['classification_report'].get('1', {}).get('precision', 0),\n",
    "                'Recall (Grey)': result['classification_report'].get('1', {}).get('recall', 0),\n",
    "                'F1 Score (Grey)': result['classification_report'].get('1', {}).get('f1-score', 0)\n",
    "            })\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_summary)\n",
    "    performance_df.to_csv(os.path.join(plot_folder, f'performance_{validation_type}.csv'), index=False)\n",
    "    \n",
    "    # Visualize performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cv_accuracies = [float(row['CV Accuracy'].split(' ')[0]) for row in performance_summary]\n",
    "    \n",
    "    bars = plt.bar(range(len(performance_summary)), cv_accuracies)\n",
    "    \n",
    "    # Color bars based on performance\n",
    "    for bar, acc in zip(bars, cv_accuracies):\n",
    "        if acc > 0.9:\n",
    "            bar.set_color('darkgreen')\n",
    "        elif acc > 0.8:\n",
    "            bar.set_color('green')\n",
    "        elif acc > 0.7:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    plt.xticks(range(len(performance_summary)), \n",
    "               [row['Classifier'] for row in performance_summary], \n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel('Cross-Validation Accuracy')\n",
    "    plt.title(f'Classifier Performance - {validation_type.upper()} Validation')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, acc in enumerate(cv_accuracies):\n",
    "        plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, f'classifier_performance_{validation_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance analysis for best classifier\n",
    "    best_classifier = max(results.items(), key=lambda x: x[1]['cv_accuracy'] if 'error' not in x[1] else 0)\n",
    "    best_name, best_result = best_classifier\n",
    "    \n",
    "    print(f\"\\nBest classifier: {best_name} (CV Accuracy: {best_result['cv_accuracy']:.3f})\")\n",
    "    \n",
    "    if 'importance' in best_result and best_result['importance'] is not None:\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': common_features,\n",
    "            'Importance': best_result['importance']\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "        import seaborn as sns\n",
    "        sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "        plt.title(f'Top 15 Features - {best_name} ({validation_type.upper()} Validation)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_folder, f'feature_importance_{validation_type}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        importance_df.to_csv(os.path.join(plot_folder, f'feature_importance_{validation_type}.csv'), index=False)\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VALIDATION SUMMARY ({validation_type.upper()})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Perfect separation features: {len(perfect_features)}\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Sample-to-feature ratio: {len(X) / len(common_features):.1f}:1\")\n",
    "    \n",
    "    if performance_summary:\n",
    "        best_cv = max(performance_summary, key=lambda x: float(x['CV Accuracy'].split(' ')[0]))\n",
    "        print(f\"Best CV accuracy: {best_cv['Classifier']} - {best_cv['CV Accuracy']}\")\n",
    "        \n",
    "        # Check for suspiciously high accuracy\n",
    "        best_acc = float(best_cv['CV Accuracy'].split(' ')[0])\n",
    "        if best_acc > 0.95:\n",
    "            print(\"⚠️  WARNING: Accuracy > 95% may indicate data leakage!\")\n",
    "        elif best_acc > 0.90:\n",
    "            print(\"⚠️  CAUTION: Accuracy > 90% - verify results\")\n",
    "        else:\n",
    "            print(\"✅ Accuracy seems reasonable\")\n",
    "    \n",
    "    return {\n",
    "        'validation_type': validation_type,\n",
    "        'approach': 'windowed' if use_windowing else 'per_channel',\n",
    "        'n_samples': len(X),\n",
    "        'n_features': len(common_features),\n",
    "        'sample_feature_ratio': len(X) / len(common_features),\n",
    "        'perfect_features': perfect_features,\n",
    "        'grey_features': grey_df,\n",
    "        'white_features': white_df,\n",
    "        'pca': pca,\n",
    "        'pca_data': X_pca,\n",
    "        'classifier_results': results,\n",
    "        'performance_summary': performance_df,\n",
    "        'best_classifier': best_name,\n",
    "        'best_accuracy': best_result['cv_accuracy'] if 'error' not in best_result else 0,\n",
    "        'feature_names': common_features\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73d0e9202bd27810",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_2 = improved_extract_and_classify_features_strict(\n",
    "    seizure_data_grey_new, seizure_data_white_new, \n",
    "    plot_folder=f'result/{seizureID}/improved_wg_classification_strict',\n",
    "    fs=p66_data.samplingRate,\n",
    "    use_windowing=True,  # Use windowing for better sample size\n",
    "    n_windows_per_channel=100,\n",
    "    min_window_length=1000,\n",
    "    validation_type='strict',  # Strict channel-wise validation\n",
    "    max_samples_per_channel=100)  # Limit samples per channel to prevent overfitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d0d042ca270b3e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
    "                           balanced_accuracy_score, roc_auc_score, confusion_matrix,\n",
    "                           classification_report, roc_curve, auc)\n",
    "\n",
    "def improved_extract_and_classify_features_complete(grey_matter_data, white_matter_data, \n",
    "                                         plot_folder='result/complete_strict_validation', \n",
    "                                         fs=250, \n",
    "                                         use_windowing=True, \n",
    "                                         n_windows_per_channel=None,\n",
    "                                         window_overlap=None,\n",
    "                                         min_window_length=None,\n",
    "                                         validation_type='strict',\n",
    "                                         max_samples_per_channel=20,\n",
    "                                         balance_method='hybrid',\n",
    "                                         class_weight='balanced'):\n",
    "    \"\"\"\n",
    "    完整的grey/white matter分类系统，包含strict validation和channel-level分析\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grey_matter_data : np.ndarray\n",
    "        Grey matter data with shape [time, channels]\n",
    "    white_matter_data : np.ndarray\n",
    "        White matter data with shape [time, channels]\n",
    "    plot_folder : str\n",
    "        Folder path to save all plots\n",
    "    fs : int\n",
    "        Sampling frequency in Hz\n",
    "    use_windowing : bool\n",
    "        Whether to use time windowing to increase sample size\n",
    "    n_windows_per_channel : int, optional\n",
    "        Maximum number of windows per channel\n",
    "    window_overlap : float, optional\n",
    "        Overlap between windows (0-1)\n",
    "    min_window_length : int, optional\n",
    "        Minimum window length in samples\n",
    "    validation_type : str\n",
    "        'strict' = channel-wise split, 'normal' = random split\n",
    "    max_samples_per_channel : int\n",
    "        Maximum number of samples per channel\n",
    "    balance_method : str\n",
    "        'downsample', 'upsample', 'hybrid'\n",
    "    class_weight : str or dict\n",
    "        Class weighting for classifiers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Complete results including sample-level and channel-level analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    \n",
    "    # Get dimensions\n",
    "    grey_time, grey_channels = grey_matter_data.shape\n",
    "    white_time, white_channels = white_matter_data.shape\n",
    "    \n",
    "    print(f\"COMPLETE GREY/WHITE MATTER CLASSIFICATION SYSTEM\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"VALIDATION MODE: {validation_type}\")\n",
    "    print(f\"Grey matter data shape: [{grey_time}, {grey_channels}]\")\n",
    "    print(f\"White matter data shape: [{white_time}, {white_channels}]\")\n",
    "    print(f\"Windowing approach: {use_windowing}\")\n",
    "    print(f\"Balance method: {balance_method}\")\n",
    "    print(f\"Class weight: {class_weight}\")\n",
    "    print(f\"Max samples per channel: {max_samples_per_channel}\")\n",
    "    \n",
    "    # Calculate windowing parameters\n",
    "    if use_windowing:\n",
    "        if min_window_length is None:\n",
    "            window_duration = 0.5  # 50ms\n",
    "            min_window_length = int(window_duration * fs)\n",
    "        \n",
    "        if window_overlap is None:\n",
    "            window_step = 0.25     # 25ms step\n",
    "            step_samples = int(window_step * fs)\n",
    "        else:\n",
    "            step_samples = int(min_window_length * (1 - window_overlap))\n",
    "        \n",
    "        if n_windows_per_channel is None:\n",
    "            max_windows_grey = (grey_time - min_window_length) // step_samples + 1\n",
    "            max_windows_white = (white_time - min_window_length) // step_samples + 1\n",
    "            n_windows_per_channel = min(max_windows_grey, max_windows_white)\n",
    "            \n",
    "        print(f\"Window parameters:\")\n",
    "        print(f\"  Window size: {min_window_length} samples ({min_window_length/fs:.3f}s)\")\n",
    "        print(f\"  Step size: {step_samples} samples ({step_samples/fs:.3f}s)\")\n",
    "        print(f\"  Expected windows per channel: {n_windows_per_channel}\")\n",
    "    \n",
    "    # Define frequency bands\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),        \n",
    "        'alpha': (8, 13),       \n",
    "        'beta': (13, 30),       \n",
    "        'gamma': (30, 100),     \n",
    "        'high_gamma': (100, min(200, fs//2))\n",
    "    }\n",
    "    \n",
    "    def extract_half_wave_features(signal, amplitude_threshold=0.1):\n",
    "        \"\"\"Extract half-wave features\"\"\"\n",
    "        zero_crossings = np.where(np.diff(np.signbit(signal)))[0]\n",
    "        \n",
    "        if len(zero_crossings) <= 1:\n",
    "            return {\n",
    "                'hw_count': 0,\n",
    "                'hw_mean_amp': 0,\n",
    "                'hw_mean_duration': 0\n",
    "            }\n",
    "        \n",
    "        half_wave_amps = []\n",
    "        half_wave_durations = []\n",
    "        \n",
    "        for i in range(len(zero_crossings) - 1):\n",
    "            start_idx = zero_crossings[i]\n",
    "            end_idx = zero_crossings[i + 1]\n",
    "            duration = end_idx - start_idx\n",
    "            segment = signal[start_idx:end_idx]\n",
    "            \n",
    "            if len(segment) > 0:\n",
    "                amplitude = np.max(np.abs(segment))\n",
    "                if amplitude >= amplitude_threshold:\n",
    "                    half_wave_amps.append(amplitude)\n",
    "                    half_wave_durations.append(duration)\n",
    "        \n",
    "        return {\n",
    "            'hw_count': len(half_wave_amps),\n",
    "            'hw_mean_amp': np.mean(half_wave_amps) if half_wave_amps else 0,\n",
    "            'hw_mean_duration': np.mean(half_wave_durations) / fs if half_wave_durations else 0\n",
    "        }\n",
    "    \n",
    "    def extract_comprehensive_features_from_window(signal):\n",
    "        \"\"\"Extract comprehensive features from a signal window\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Time domain statistical features\n",
    "            features['mean'] = np.mean(signal)\n",
    "            features['std'] = np.std(signal)\n",
    "            features['median'] = np.median(signal)\n",
    "            features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "            \n",
    "            try:\n",
    "                features['skew'] = stats.skew(signal)\n",
    "                features['kurtosis'] = stats.kurtosis(signal)\n",
    "            except:\n",
    "                features['skew'] = 0\n",
    "                features['kurtosis'] = 0\n",
    "                \n",
    "            features['range'] = np.max(signal) - np.min(signal)\n",
    "            features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "            features['zero_crossings'] = np.sum(np.diff(np.signbit(signal).astype(int)) != 0)\n",
    "            \n",
    "            # Half-wave features\n",
    "            hw_features = extract_half_wave_features(signal)\n",
    "            features.update(hw_features)\n",
    "            \n",
    "            # Line length and area features\n",
    "            features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "            features['area'] = np.sum(np.abs(signal))\n",
    "            \n",
    "            # Frequency domain features\n",
    "            try:\n",
    "                windowed_signal = signal * np.hamming(len(signal))\n",
    "                from scipy.fftpack import fft\n",
    "                fft_vals = fft(windowed_signal)\n",
    "                fft_abs = np.abs(fft_vals[:len(signal) // 2])\n",
    "                fft_abs = fft_abs / len(signal)\n",
    "                freq_bins = np.fft.fftfreq(len(signal), 1 / fs)[:len(signal) // 2]\n",
    "                \n",
    "                # Band powers\n",
    "                for band_name, (low_freq, high_freq) in freq_bands.items():\n",
    "                    band_mask = (freq_bins >= low_freq) & (freq_bins <= high_freq)\n",
    "                    if np.any(band_mask):\n",
    "                        band_power = np.sum(fft_abs[band_mask] ** 2)\n",
    "                        features[f'power_{band_name}'] = band_power\n",
    "                    else:\n",
    "                        features[f'power_{band_name}'] = 0\n",
    "                \n",
    "                # Total power\n",
    "                total_power = sum([features[f'power_{band}'] for band in freq_bands.keys()])\n",
    "                features['total_power'] = total_power\n",
    "                \n",
    "                # Spectral edge frequency (95%)\n",
    "                if len(fft_abs) > 0 and total_power > 0:\n",
    "                    cumulative_power = np.cumsum(fft_abs ** 2)\n",
    "                    edge_95_idx = np.argmax(cumulative_power >= 0.95 * np.sum(fft_abs ** 2))\n",
    "                    features['spectral_edge_freq'] = freq_bins[edge_95_idx] if edge_95_idx > 0 else freq_bins[-1]\n",
    "                else:\n",
    "                    features['spectral_edge_freq'] = 0\n",
    "                \n",
    "                # Spectral entropy\n",
    "                if total_power > 0:\n",
    "                    power_spectrum = fft_abs ** 2\n",
    "                    pxx_norm = power_spectrum / np.sum(power_spectrum)\n",
    "                    features['spectral_entropy'] = -np.sum(pxx_norm * np.log2(pxx_norm + 1e-10))\n",
    "                else:\n",
    "                    features['spectral_entropy'] = 0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Spectral analysis failed: {str(e)}\")\n",
    "                for band_name in freq_bands.keys():\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                features['total_power'] = 0\n",
    "                features['spectral_edge_freq'] = 0\n",
    "                features['spectral_entropy'] = 0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Feature extraction failed: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def add_contact_depth_features(samples, channel_info, n_total_channels):\n",
    "        \"\"\"Add contact depth features\"\"\"\n",
    "        enhanced_samples = []\n",
    "        \n",
    "        for sample, ch_id in zip(samples, channel_info):\n",
    "            enhanced_sample = sample.copy()\n",
    "            \n",
    "            if n_total_channels > 1:\n",
    "                relative_depth = ch_id / (n_total_channels - 1)\n",
    "            else:\n",
    "                relative_depth = 0.5\n",
    "            \n",
    "            enhanced_sample['contact_depth'] = relative_depth\n",
    "            enhanced_sample['contact_depth_squared'] = relative_depth ** 2\n",
    "            enhanced_sample['is_surface_contact'] = 1 if relative_depth < 0.2 else 0\n",
    "            enhanced_sample['is_middle_contact'] = 1 if 0.3 <= relative_depth <= 0.7 else 0\n",
    "            enhanced_sample['is_deep_contact'] = 1 if relative_depth > 0.8 else 0\n",
    "            \n",
    "            enhanced_samples.append(enhanced_sample)\n",
    "        \n",
    "        return enhanced_samples\n",
    "    \n",
    "    def create_channel_separated_samples(data, tissue_type):\n",
    "        \"\"\"Create samples with channel tracking\"\"\"\n",
    "        samples = []\n",
    "        channel_info = []\n",
    "        n_time, n_channels = data.shape\n",
    "        \n",
    "        if use_windowing:\n",
    "            window_samples = min_window_length\n",
    "            \n",
    "            print(f\"{tissue_type} - Window: {window_samples} samples, Step: {step_samples} samples\")\n",
    "            \n",
    "            for ch in range(n_channels):\n",
    "                channel_data = data[:, ch]\n",
    "                channel_samples = []\n",
    "                \n",
    "                for start_idx in range(0, len(channel_data) - window_samples + 1, step_samples):\n",
    "                    end_idx = start_idx + window_samples\n",
    "                    window_data = channel_data[start_idx:end_idx]\n",
    "                    \n",
    "                    features = extract_comprehensive_features_from_window(window_data)\n",
    "                    if features is not None:\n",
    "                        channel_samples.append(features)\n",
    "                    \n",
    "                    if len(channel_samples) >= n_windows_per_channel:\n",
    "                        break\n",
    "                \n",
    "                if max_samples_per_channel and len(channel_samples) > max_samples_per_channel:\n",
    "                    indices = np.random.choice(len(channel_samples), max_samples_per_channel, replace=False)\n",
    "                    channel_samples = [channel_samples[i] for i in sorted(indices)]\n",
    "                \n",
    "                for sample in channel_samples:\n",
    "                    samples.append(sample)\n",
    "                    channel_info.append(ch)\n",
    "                    \n",
    "        else:\n",
    "            for ch in range(n_channels):\n",
    "                channel_data = data[:, ch]\n",
    "                features = extract_comprehensive_features_from_window(channel_data)\n",
    "                if features is not None:\n",
    "                    samples.append(features)\n",
    "                    channel_info.append(ch)\n",
    "        \n",
    "        return samples, channel_info\n",
    "    \n",
    "    def balance_dataset(grey_df, white_df, grey_channel_info, white_channel_info, \n",
    "                       balance_method='hybrid', max_samples_total=2000):\n",
    "        \"\"\"Balance dataset to handle class imbalance\"\"\"\n",
    "        print(f\"Original data - Grey: {len(grey_df)}, White: {len(white_df)}\")\n",
    "        \n",
    "        if balance_method == 'downsample':\n",
    "            min_samples = min(len(grey_df), len(white_df))\n",
    "            target_samples = min(min_samples, max_samples_total // 2)\n",
    "            \n",
    "            if len(grey_df) > target_samples:\n",
    "                sample_indices = np.random.choice(len(grey_df), target_samples, replace=False)\n",
    "                grey_df = grey_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                grey_channel_info = [grey_channel_info[i] for i in sample_indices]\n",
    "            \n",
    "            if len(white_df) > target_samples:\n",
    "                sample_indices = np.random.choice(len(white_df), target_samples, replace=False)\n",
    "                white_df = white_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                white_channel_info = [white_channel_info[i] for i in sample_indices]\n",
    "                \n",
    "        elif balance_method == 'upsample':\n",
    "            max_samples = max(len(grey_df), len(white_df))\n",
    "            target_samples = min(max_samples, max_samples_total // 2)\n",
    "            \n",
    "            if len(grey_df) < target_samples:\n",
    "                n_upsample = target_samples - len(grey_df)\n",
    "                upsample_indices = np.random.choice(len(grey_df), n_upsample, replace=True)\n",
    "                grey_upsampled = grey_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                grey_df = pd.concat([grey_df, grey_upsampled], ignore_index=True)\n",
    "                grey_channel_info.extend([grey_channel_info[i] for i in upsample_indices])\n",
    "            \n",
    "            if len(white_df) < target_samples:\n",
    "                n_upsample = target_samples - len(white_df)\n",
    "                upsample_indices = np.random.choice(len(white_df), n_upsample, replace=True)\n",
    "                white_upsampled = white_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                white_df = pd.concat([white_df, white_upsampled], ignore_index=True)\n",
    "                white_channel_info.extend([white_channel_info[i] for i in upsample_indices])\n",
    "                \n",
    "        elif balance_method == 'hybrid':\n",
    "            total_samples = len(grey_df) + len(white_df)\n",
    "            target_per_class = min(total_samples // 2, max_samples_total // 2)\n",
    "            \n",
    "            # Adjust grey matter\n",
    "            if len(grey_df) > target_per_class:\n",
    "                sample_indices = np.random.choice(len(grey_df), target_per_class, replace=False)\n",
    "                grey_df = grey_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                grey_channel_info = [grey_channel_info[i] for i in sample_indices]\n",
    "            elif len(grey_df) < target_per_class:\n",
    "                n_upsample = target_per_class - len(grey_df)\n",
    "                upsample_indices = np.random.choice(len(grey_df), n_upsample, replace=True)\n",
    "                grey_upsampled = grey_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                grey_df = pd.concat([grey_df, grey_upsampled], ignore_index=True)\n",
    "                grey_channel_info.extend([grey_channel_info[i] for i in upsample_indices])\n",
    "            \n",
    "            # Adjust white matter\n",
    "            if len(white_df) > target_per_class:\n",
    "                sample_indices = np.random.choice(len(white_df), target_per_class, replace=False)\n",
    "                white_df = white_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                white_channel_info = [white_channel_info[i] for i in sample_indices]\n",
    "            elif len(white_df) < target_per_class:\n",
    "                n_upsample = target_per_class - len(white_df)\n",
    "                upsample_indices = np.random.choice(len(white_df), n_upsample, replace=True)\n",
    "                white_upsampled = white_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                white_df = pd.concat([white_df, white_upsampled], ignore_index=True)\n",
    "                white_channel_info.extend([white_channel_info[i] for i in upsample_indices])\n",
    "        \n",
    "        print(f\"Balanced data - Grey: {len(grey_df)}, White: {len(white_df)}\")\n",
    "        return grey_df, white_df, grey_channel_info, white_channel_info\n",
    "    \n",
    "    # ===============================\n",
    "    # MAIN FEATURE EXTRACTION\n",
    "    # ===============================\n",
    "    \n",
    "    print(\"\\nExtracting features with channel separation...\")\n",
    "    grey_samples, grey_channel_info = create_channel_separated_samples(\n",
    "        grey_matter_data, \"Grey Matter\"\n",
    "    )\n",
    "    white_samples, white_channel_info = create_channel_separated_samples(\n",
    "        white_matter_data, \"White Matter\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Grey matter samples created: {len(grey_samples)} from {len(set(grey_channel_info))} channels\")\n",
    "    print(f\"White matter samples created: {len(white_samples)} from {len(set(white_channel_info))} channels\")\n",
    "    \n",
    "    if len(grey_samples) == 0 or len(white_samples) == 0:\n",
    "        return {'error': 'No samples created'}\n",
    "    \n",
    "    # Add contact depth features\n",
    "    print(\"Adding contact depth features...\")\n",
    "    grey_samples = add_contact_depth_features(\n",
    "        grey_samples, grey_channel_info, grey_matter_data.shape[1]\n",
    "    )\n",
    "    white_samples = add_contact_depth_features(\n",
    "        white_samples, white_channel_info, white_matter_data.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    grey_df = pd.DataFrame(grey_samples)\n",
    "    white_df = pd.DataFrame(white_samples)\n",
    "    \n",
    "    # Clean data\n",
    "    def clean_dataframe(df, name):\n",
    "        original_shape = df.shape\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if df[numeric_cols].isna().any().any():\n",
    "            print(f\"Found NaN values in {name}, filling with column medians\")\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if np.any(np.isinf(df[col])):\n",
    "                print(f\"Found infinite values in {name} column {col}\")\n",
    "                finite_mask = np.isfinite(df[col])\n",
    "                if np.any(finite_mask):\n",
    "                    df.loc[~finite_mask, col] = df.loc[finite_mask, col].median()\n",
    "                else:\n",
    "                    df[col] = 0\n",
    "        \n",
    "        print(f\"Cleaned {name}: {original_shape} -> {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    grey_df = clean_dataframe(grey_df, \"grey matter features\")\n",
    "    white_df = clean_dataframe(white_df, \"white matter features\")\n",
    "    \n",
    "    # Balance dataset\n",
    "    print(\"Balancing dataset...\")\n",
    "    grey_df, white_df, grey_channel_info, white_channel_info = balance_dataset(\n",
    "        grey_df, white_df, grey_channel_info, white_channel_info,\n",
    "        balance_method=balance_method, max_samples_total=2000\n",
    "    )\n",
    "    \n",
    "    # Ensure common features\n",
    "    feature_cols = [col for col in grey_df.columns]\n",
    "    common_features = list(set(feature_cols).intersection(set(white_df.columns)))\n",
    "    \n",
    "    if len(common_features) == 0:\n",
    "        return {'error': 'No common features'}\n",
    "    \n",
    "    print(f\"Using {len(common_features)} common features\")\n",
    "    \n",
    "    # Add labels and channel info\n",
    "    grey_df['channel_id'] = grey_channel_info\n",
    "    white_df['channel_id'] = [ch + grey_channels for ch in white_channel_info]\n",
    "    grey_df['Matter'] = 'Grey'\n",
    "    white_df['Matter'] = 'White'\n",
    "    \n",
    "    # Combine data\n",
    "    combined_df = pd.concat([grey_df, white_df], ignore_index=True)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = combined_df[common_features]\n",
    "    y = combined_df['Matter'].map({'Grey': 1, 'White': 0})\n",
    "    channel_ids = combined_df['channel_id'].values\n",
    "    \n",
    "    print(f\"Final dataset: {len(X)} samples × {len(common_features)} features\")\n",
    "    print(f\"Sample-to-feature ratio: {len(X) / len(common_features):.1f}:1\")\n",
    "    \n",
    "    # Check for perfect separability\n",
    "    def check_perfect_separability(X, y, feature_names):\n",
    "        perfect_features = []\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            feature_vals = X.iloc[:, i]\n",
    "            grey_vals = feature_vals[y == 1]\n",
    "            white_vals = feature_vals[y == 0]\n",
    "            \n",
    "            grey_min, grey_max = grey_vals.min(), grey_vals.max()\n",
    "            white_min, white_max = white_vals.min(), white_vals.max()\n",
    "            \n",
    "            if grey_max < white_min or white_max < grey_min:\n",
    "                perfect_features.append(feature)\n",
    "                print(f\"⚠️  PERFECT SEPARATION found in feature '{feature}':\")\n",
    "                print(f\"   Grey range: [{grey_min:.3f}, {grey_max:.3f}]\")\n",
    "                print(f\"   White range: [{white_min:.3f}, {white_max:.3f}]\")\n",
    "        \n",
    "        return perfect_features\n",
    "    \n",
    "    perfect_features = check_perfect_separability(X, y, common_features)\n",
    "    if perfect_features:\n",
    "        print(f\"\\n🚨 WARNING: Found {len(perfect_features)} features with perfect separation!\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train/test split\n",
    "    if validation_type == 'strict':\n",
    "        print(\"\\n🔒 STRICT VALIDATION: Channel-wise train/test split\")\n",
    "        \n",
    "        grey_channels_used = combined_df[combined_df['Matter'] == 'Grey']['channel_id'].unique()\n",
    "        white_channels_used = combined_df[combined_df['Matter'] == 'White']['channel_id'].unique()\n",
    "        \n",
    "        print(f\"Grey matter: {len(grey_channels_used)} unique channels\")\n",
    "        print(f\"White matter: {len(white_channels_used)} unique channels\")\n",
    "        \n",
    "        grey_train_channels, grey_test_channels = train_test_split(\n",
    "            grey_channels_used, test_size=0.3, random_state=42\n",
    "        )\n",
    "        white_train_channels, white_test_channels = train_test_split(\n",
    "            white_channels_used, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_mask = (\n",
    "            (combined_df['Matter'] == 'Grey') & (combined_df['channel_id'].isin(grey_train_channels)) |\n",
    "            (combined_df['Matter'] == 'White') & (combined_df['channel_id'].isin(white_train_channels))\n",
    "        )\n",
    "        test_mask = ~train_mask\n",
    "        \n",
    "        X_train, X_test = X_scaled[train_mask], X_scaled[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        test_channel_ids = channel_ids[test_mask]\n",
    "        \n",
    "        print(f\"Channel-wise split: Train={len(X_train)}, Test={len(X_test)}\")\n",
    "        \n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "            X_scaled, y, np.arange(len(X)), test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        test_channel_ids = channel_ids[test_idx]\n",
    "    \n",
    "    # ===============================\n",
    "    # CLASSIFICATION\n",
    "    # ===============================\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000, random_state=42, class_weight=class_weight\n",
    "        ),\n",
    "        'SVM (RBF kernel)': SVC(\n",
    "            probability=True, random_state=42, C=1.0, class_weight=class_weight\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, random_state=42, max_depth=10, class_weight=class_weight\n",
    "        ),\n",
    "        'MLP Neural Network': MLPClassifier(\n",
    "            max_iter=1000, random_state=42, hidden_layer_sizes=(50,), alpha=0.01\n",
    "        ),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'LDA': LDA(),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    if validation_type == 'strict':\n",
    "        print(\"\\n🔒 Using channel-wise cross-validation...\")\n",
    "        cv = GroupKFold(n_splits=min(5, len(set(channel_ids))))\n",
    "        groups = channel_ids\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        groups = None\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    print(f\"\\nTraining classifiers with {class_weight} class weighting...\")\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation\n",
    "            if validation_type == 'strict' and len(set(channel_ids)) >= 3:\n",
    "                cv_scores_f1 = cross_val_score(clf, X_scaled, y, cv=cv, groups=groups, scoring='f1')\n",
    "                cv_scores_bal = cross_val_score(clf, X_scaled, y, cv=cv, groups=groups, scoring='balanced_accuracy')\n",
    "            else:\n",
    "                cv_scores_f1 = cross_val_score(clf, X_scaled, y, cv=StratifiedKFold(n_splits=3), scoring='f1')\n",
    "                cv_scores_bal = cross_val_score(clf, X_scaled, y, cv=StratifiedKFold(n_splits=3), scoring='balanced_accuracy')\n",
    "            \n",
    "            # Train and test\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_f1 = f1_score(y_test, y_pred)\n",
    "            test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "            test_precision = precision_score(y_test, y_pred)\n",
    "            test_recall = recall_score(y_test, y_pred)\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # ROC curve\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "            else:\n",
    "                y_proba = np.zeros_like(y_pred)\n",
    "                roc_auc = 0.5\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'cv_f1': cv_scores_f1.mean(),\n",
    "                'cv_f1_std': cv_scores_f1.std(),\n",
    "                'cv_balanced_acc': cv_scores_bal.mean(),\n",
    "                'cv_balanced_acc_std': cv_scores_bal.std(),\n",
    "                'test_f1': test_f1,\n",
    "                'test_balanced_acc': test_balanced_acc,\n",
    "                'test_precision': test_precision,\n",
    "                'test_recall': test_recall,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'roc_auc': roc_auc,\n",
    "                'y_pred': y_pred,\n",
    "                'y_proba': y_proba,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "            }\n",
    "            \n",
    "            print(f\"  CV F1: {cv_scores_f1.mean():.3f} ± {cv_scores_f1.std():.3f}\")\n",
    "            print(f\"  CV Balanced Acc: {cv_scores_bal.mean():.3f} ± {cv_scores_bal.std():.3f}\")\n",
    "            print(f\"  Test F1: {test_f1:.3f}\")\n",
    "            print(f\"  Test Precision: {test_precision:.3f}\")\n",
    "            print(f\"  Test Recall: {test_recall:.3f}\")\n",
    "            print(f\"  ROC AUC: {roc_auc:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    # Finalize ROC plot\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {validation_type.upper()} Validation')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, f'roc_curves_{validation_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ===============================\n",
    "    # CHANNEL-LEVEL ANALYSIS\n",
    "    # ===============================\n",
    "    \n",
    "    def aggregate_samples_to_channels(y_true, y_pred, y_proba, channel_ids, \n",
    "                                     aggregation_method='majority_vote'):\n",
    "        \"\"\"Aggregate sample-level predictions to channel-level\"\"\"\n",
    "        unique_channels = np.unique(channel_ids)\n",
    "        \n",
    "        channel_true_labels = []\n",
    "        channel_pred_labels = []\n",
    "        channel_probabilities = []\n",
    "        channel_confidence = []\n",
    "        channel_sample_counts = []\n",
    "        \n",
    "        for ch_id in unique_channels:\n",
    "            ch_mask = channel_ids == ch_id\n",
    "            ch_true = y_true[ch_mask]\n",
    "            ch_pred = y_pred[ch_mask]\n",
    "            ch_proba = y_proba[ch_mask]\n",
    "            \n",
    "            # Get true label (should be consistent)\n",
    "            true_label = np.bincount(ch_true).argmax()\n",
    "            \n",
    "            # Aggregate predictions based on method\n",
    "            if aggregation_method == 'majority_vote':\n",
    "                pred_label = np.bincount(ch_pred).argmax()\n",
    "                avg_proba = np.mean(ch_proba)\n",
    "                confidence = np.sum(ch_pred == pred_label) / len(ch_pred)\n",
    "                \n",
    "            elif aggregation_method == 'average_probability':\n",
    "                avg_proba = np.mean(ch_proba)\n",
    "                pred_label = 1 if avg_proba > 0.5 else 0\n",
    "                confidence = abs(avg_proba - 0.5) * 2\n",
    "                \n",
    "            elif aggregation_method == 'weighted_vote':\n",
    "                weights = np.abs(ch_proba - 0.5) * 2\n",
    "                weighted_votes_0 = np.sum(weights[ch_pred == 0])\n",
    "                weighted_votes_1 = np.sum(weights[ch_pred == 1])\n",
    "                \n",
    "                if weighted_votes_1 > weighted_votes_0:\n",
    "                    pred_label = 1\n",
    "                    confidence = weighted_votes_1 / (weighted_votes_0 + weighted_votes_1)\n",
    "                else:\n",
    "                    pred_label = 0\n",
    "                    confidence = weighted_votes_0 / (weighted_votes_0 + weighted_votes_1)\n",
    "                \n",
    "                avg_proba = np.mean(ch_proba)\n",
    "                \n",
    "            elif aggregation_method == 'confidence_threshold':\n",
    "                confidence_threshold = 0.7\n",
    "                high_conf_mask = (ch_proba > confidence_threshold) | (ch_proba < (1 - confidence_threshold))\n",
    "                \n",
    "                if np.any(high_conf_mask):\n",
    "                    high_conf_pred = ch_pred[high_conf_mask]\n",
    "                    high_conf_proba = ch_proba[high_conf_mask]\n",
    "                    pred_label = np.bincount(high_conf_pred).argmax()\n",
    "                    avg_proba = np.mean(high_conf_proba)\n",
    "                    confidence = np.sum(high_conf_mask) / len(ch_pred)\n",
    "                else:\n",
    "                    avg_proba = np.mean(ch_proba)\n",
    "                    pred_label = 1 if avg_proba > 0.5 else 0\n",
    "                    confidence = 0.5\n",
    "            \n",
    "            channel_true_labels.append(true_label)\n",
    "            channel_pred_labels.append(pred_label)\n",
    "            channel_probabilities.append(avg_proba)\n",
    "            channel_confidence.append(confidence)\n",
    "            channel_sample_counts.append(len(ch_pred))\n",
    "        \n",
    "        return {\n",
    "            'channel_ids': unique_channels,\n",
    "            'true_labels': np.array(channel_true_labels),\n",
    "            'pred_labels': np.array(channel_pred_labels),\n",
    "            'probabilities': np.array(channel_probabilities),\n",
    "            'confidence': np.array(channel_confidence),\n",
    "            'sample_counts': np.array(channel_sample_counts),\n",
    "            'aggregation_method': aggregation_method\n",
    "        }\n",
    "    \n",
    "    def evaluate_channel_level_performance(channel_results, plot_folder=None, validation_type='strict'):\n",
    "        \"\"\"Evaluate channel-level performance\"\"\"\n",
    "        y_true_ch = channel_results['true_labels']\n",
    "        y_pred_ch = channel_results['pred_labels']\n",
    "        y_proba_ch = channel_results['probabilities']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true_ch, y_pred_ch),\n",
    "            'f1_score': f1_score(y_true_ch, y_pred_ch),\n",
    "            'precision': precision_score(y_true_ch, y_pred_ch),\n",
    "            'recall': recall_score(y_true_ch, y_pred_ch),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_true_ch, y_pred_ch),\n",
    "            'roc_auc': roc_auc_score(y_true_ch, y_proba_ch),\n",
    "            'confusion_matrix': confusion_matrix(y_true_ch, y_pred_ch),\n",
    "            'n_channels': len(y_true_ch),\n",
    "            'n_grey_channels': np.sum(y_true_ch == 1),\n",
    "            'n_white_channels': np.sum(y_true_ch == 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CHANNEL-LEVEL PERFORMANCE ({channel_results['aggregation_method']})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total channels: {metrics['n_channels']}\")\n",
    "        print(f\"Grey matter channels: {metrics['n_grey_channels']}\")\n",
    "        print(f\"White matter channels: {metrics['n_white_channels']}\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.3f}\")\n",
    "        print(f\"ROC AUC: {metrics['roc_auc']:.3f}\")\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(f\"  [[TN={metrics['confusion_matrix'][0,0]}, FP={metrics['confusion_matrix'][0,1]}],\")\n",
    "        print(f\"   [FN={metrics['confusion_matrix'][1,0]}, TP={metrics['confusion_matrix'][1,1]}]]\")\n",
    "        \n",
    "        # Visualizations\n",
    "        if plot_folder:\n",
    "            # Confusion matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=['White Matter', 'Grey Matter'],\n",
    "                       yticklabels=['White Matter', 'Grey Matter'])\n",
    "            plt.title(f'Channel-Level Confusion Matrix\\n({channel_results[\"aggregation_method\"]}, {validation_type})')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_folder, f'channel_confusion_matrix_{validation_type}_{channel_results[\"aggregation_method\"]}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Probability distribution\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            grey_proba = y_proba_ch[y_true_ch == 1]\n",
    "            white_proba = y_proba_ch[y_true_ch == 0]\n",
    "            \n",
    "            plt.hist(white_proba, bins=20, alpha=0.7, label='White Matter Channels', color='red')\n",
    "            plt.hist(grey_proba, bins=20, alpha=0.7, label='Grey Matter Channels', color='blue')\n",
    "            plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "            plt.xlabel('Predicted Probability (Grey Matter)')\n",
    "            plt.ylabel('Number of Channels')\n",
    "            plt.title('Channel-Level Probability Distribution')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Confidence analysis\n",
    "            plt.subplot(1, 2, 2)\n",
    "            confidence = channel_results['confidence']\n",
    "            correct_pred = (y_true_ch == y_pred_ch)\n",
    "            \n",
    "            plt.scatter(confidence[correct_pred], y_proba_ch[correct_pred], \n",
    "                       alpha=0.6, c='green', label='Correct Predictions', s=50)\n",
    "            plt.scatter(confidence[~correct_pred], y_proba_ch[~correct_pred], \n",
    "                       alpha=0.6, c='red', label='Incorrect Predictions', s=50)\n",
    "            plt.xlabel('Confidence Score')\n",
    "            plt.ylabel('Predicted Probability')\n",
    "            plt.title('Confidence vs Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_folder, f'channel_analysis_{validation_type}_{channel_results[\"aggregation_method\"]}.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compare_aggregation_methods(y_true, y_pred, y_proba, channel_ids, plot_folder=None, validation_type='strict'):\n",
    "        \"\"\"Compare different aggregation methods\"\"\"\n",
    "        methods = ['majority_vote', 'average_probability', 'weighted_vote', 'confidence_threshold']\n",
    "        comparison_results = {}\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COMPARING CHANNEL-LEVEL AGGREGATION METHODS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"\\n--- {method.upper()} ---\")\n",
    "            \n",
    "            channel_results = aggregate_samples_to_channels(\n",
    "                y_true, y_pred, y_proba, channel_ids, aggregation_method=method\n",
    "            )\n",
    "            \n",
    "            metrics = evaluate_channel_level_performance(\n",
    "                channel_results, plot_folder, validation_type\n",
    "            )\n",
    "            \n",
    "            comparison_results[method] = {\n",
    "                'channel_results': channel_results,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_df = pd.DataFrame({\n",
    "            method: {\n",
    "                'F1 Score': results['metrics']['f1_score'],\n",
    "                'Accuracy': results['metrics']['accuracy'],\n",
    "                'Balanced Acc': results['metrics']['balanced_accuracy'],\n",
    "                'Precision': results['metrics']['precision'],\n",
    "                'Recall': results['metrics']['recall'],\n",
    "                'ROC AUC': results['metrics']['roc_auc']\n",
    "            }\n",
    "            for method, results in comparison_results.items()\n",
    "        }).round(3)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"AGGREGATION METHODS COMPARISON\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(comparison_df.to_string())\n",
    "        \n",
    "        # Save comparison results\n",
    "        if plot_folder:\n",
    "            comparison_df.to_csv(os.path.join(plot_folder, f'channel_aggregation_comparison_{validation_type}.csv'))\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            metrics_to_plot = ['F1 Score', 'Balanced Acc', 'Precision', 'Recall', 'ROC AUC']\n",
    "            x = np.arange(len(methods))\n",
    "            width = 0.15\n",
    "            \n",
    "            for i, metric in enumerate(metrics_to_plot):\n",
    "                values = [comparison_df.loc[metric, method] for method in methods]\n",
    "                plt.bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "            \n",
    "            plt.xlabel('Aggregation Methods')\n",
    "            plt.ylabel('Performance Score')\n",
    "            plt.title(f'Channel-Level Aggregation Methods Comparison ({validation_type})')\n",
    "            plt.xticks(x + width*2, [m.replace('_', ' ').title() for m in methods], rotation=15)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_folder, f'aggregation_methods_comparison_{validation_type}.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # Recommend best method\n",
    "        best_method = comparison_df.loc['F1 Score'].idxmax()\n",
    "        best_f1 = comparison_df.loc['F1 Score', best_method]\n",
    "        \n",
    "        print(f\"\\n🏆 RECOMMENDED METHOD: {best_method}\")\n",
    "        print(f\"   Best F1 Score: {best_f1:.3f}\")\n",
    "        \n",
    "        return comparison_results, best_method\n",
    "    \n",
    "    # ===============================\n",
    "    # EXECUTE CHANNEL-LEVEL ANALYSIS\n",
    "    # ===============================\n",
    "    \n",
    "    # Find best classifier\n",
    "    valid_results = {name: result for name, result in results.items() if 'error' not in result}\n",
    "    if not valid_results:\n",
    "        return {'error': 'No valid classifier results'}\n",
    "    \n",
    "    best_classifier_name = max(valid_results.items(), key=lambda x: x[1]['test_f1'])[0]\n",
    "    best_result = valid_results[best_classifier_name]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHANNEL-LEVEL ANALYSIS USING BEST CLASSIFIER: {best_classifier_name}\")\n",
    "    print(f\"Best Sample-Level F1: {best_result['test_f1']:.3f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get predictions from best classifier\n",
    "    y_pred_best = best_result['y_pred']\n",
    "    y_proba_best = best_result['y_proba']\n",
    "    \n",
    "    # Compare aggregation methods\n",
    "    channel_comparison, best_aggregation_method = compare_aggregation_methods(\n",
    "        y_test, y_pred_best, y_proba_best, test_channel_ids, \n",
    "        plot_folder=plot_folder, validation_type=validation_type\n",
    "    )\n",
    "    \n",
    "    # Final channel-level results with best method\n",
    "    final_channel_results = aggregate_samples_to_channels(\n",
    "        y_test, y_pred_best, y_proba_best, test_channel_ids, \n",
    "        aggregation_method=best_aggregation_method\n",
    "    )\n",
    "    \n",
    "    final_channel_metrics = evaluate_channel_level_performance(\n",
    "        final_channel_results, plot_folder, validation_type\n",
    "    )\n",
    "    \n",
    "    # ===============================\n",
    "    # CREATE PERFORMANCE SUMMARY\n",
    "    # ===============================\n",
    "    \n",
    "    # Sample-level summary\n",
    "    sample_performance = []\n",
    "    for name, result in valid_results.items():\n",
    "        sample_performance.append({\n",
    "            'Classifier': name,\n",
    "            'CV F1': f\"{result['cv_f1']:.3f} ± {result['cv_f1_std']:.3f}\",\n",
    "            'Test F1': result['test_f1'],\n",
    "            'Test Precision': result['test_precision'],\n",
    "            'Test Recall': result['test_recall'],\n",
    "            'Test Balanced Acc': result['test_balanced_acc'],\n",
    "            'ROC AUC': result['roc_auc']\n",
    "        })\n",
    "    \n",
    "    sample_performance_df = pd.DataFrame(sample_performance)\n",
    "    sample_performance_df.to_csv(os.path.join(plot_folder, f'sample_performance_{validation_type}.csv'), index=False)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL SUMMARY - {validation_type.upper()} VALIDATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Perfect separation features: {len(perfect_features)}\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Sample-to-feature ratio: {len(X) / len(common_features):.1f}:1\")\n",
    "    print(f\"Class distribution: Grey={np.sum(y==1)}, White={np.sum(y==0)}\")\n",
    "    \n",
    "    print(f\"\\nSAMPLE-LEVEL PERFORMANCE:\")\n",
    "    print(f\"Best classifier: {best_classifier_name}\")\n",
    "    print(f\"Best F1 score: {best_result['test_f1']:.3f}\")\n",
    "    print(f\"Best ROC AUC: {best_result['roc_auc']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nCHANNEL-LEVEL PERFORMANCE:\")\n",
    "    print(f\"Best aggregation method: {best_aggregation_method}\")\n",
    "    print(f\"Channel-level F1 score: {final_channel_metrics['f1_score']:.3f}\")\n",
    "    print(f\"Channel-level ROC AUC: {final_channel_metrics['roc_auc']:.3f}\")\n",
    "    print(f\"Total channels analyzed: {final_channel_metrics['n_channels']}\")\n",
    "    \n",
    "    # Performance improvement\n",
    "    improvement = final_channel_metrics['f1_score'] - best_result['test_f1']\n",
    "    print(f\"F1 improvement (channel vs sample): {improvement:+.3f}\")\n",
    "    \n",
    "    if best_result['test_f1'] > 0.95:\n",
    "        print(\"⚠️  WARNING: Sample-level accuracy > 95% may indicate data leakage!\")\n",
    "    elif best_result['test_f1'] > 0.90:\n",
    "        print(\"⚠️  CAUTION: Sample-level accuracy > 90% - verify results\")\n",
    "    else:\n",
    "        print(\"✅ Sample-level accuracy seems reasonable\")\n",
    "    \n",
    "    if final_channel_metrics['f1_score'] > 0.8:\n",
    "        print(\"🎉 Excellent channel-level performance!\")\n",
    "    elif final_channel_metrics['f1_score'] > 0.6:\n",
    "        print(\"✅ Good channel-level performance\")\n",
    "    else:\n",
    "        print(\"⚠️  Channel-level performance could be improved\")\n",
    "    \n",
    "    # ===============================\n",
    "    # RETURN COMPREHENSIVE RESULTS\n",
    "    # ===============================\n",
    "    \n",
    "    return {\n",
    "        'validation_type': validation_type,\n",
    "        'balance_method': balance_method,\n",
    "        'class_weight': class_weight,\n",
    "        \n",
    "        # Data info\n",
    "        'n_samples': len(X),\n",
    "        'n_features': len(common_features),\n",
    "        'sample_feature_ratio': len(X) / len(common_features),\n",
    "        'perfect_features': perfect_features,\n",
    "        'feature_names': common_features,\n",
    "        \n",
    "        # Sample-level results\n",
    "        'sample_level': {\n",
    "            'classifier_results': valid_results,\n",
    "            'performance_summary': sample_performance_df,\n",
    "            'best_classifier': best_classifier_name,\n",
    "            'best_metrics': {\n",
    "                'f1_score': best_result['test_f1'],\n",
    "                'balanced_accuracy': best_result['test_balanced_acc'],\n",
    "                'roc_auc': best_result['roc_auc'],\n",
    "                'precision': best_result['test_precision'],\n",
    "                'recall': best_result['test_recall']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Channel-level results\n",
    "        'channel_level': {\n",
    "            'comparison_results': channel_comparison,\n",
    "            'best_aggregation_method': best_aggregation_method,\n",
    "            'final_results': final_channel_results,\n",
    "            'final_metrics': final_channel_metrics,\n",
    "            'improvement_over_sample': improvement\n",
    "        },\n",
    "        \n",
    "        # Files saved\n",
    "        'output_folder': plot_folder,\n",
    "        'files_generated': [\n",
    "            f'roc_curves_{validation_type}.png',\n",
    "            f'sample_performance_{validation_type}.csv',\n",
    "            f'channel_aggregation_comparison_{validation_type}.csv',\n",
    "            f'aggregation_methods_comparison_{validation_type}.png',\n",
    "            f'channel_confusion_matrix_{validation_type}_{best_aggregation_method}.png',\n",
    "            f'channel_analysis_{validation_type}_{best_aggregation_method}.png'\n",
    "        ]\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdec6570b94f9f21",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_3 = improved_extract_and_classify_features_complete(\n",
    "    seizure_data_grey_new, seizure_data_white_new, \n",
    "    plot_folder=f'result/{seizureID}/wg_classification_strict_ictal_data',\n",
    "    fs=p66_data.samplingRate,\n",
    "    use_windowing=True,  # Use windowing for better sample size\n",
    "    n_windows_per_channel=200,\n",
    "    window_overlap=0.5,\n",
    "    min_window_length=500,\n",
    "    validation_type='strict',  # Strict channel-wise validation\n",
    "    max_samples_per_channel=5000)  # Limit samples per channel to prevent overfitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805a5e5dc1394c2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "958f802ab3d64eb5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
