{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# At the start of your notebook\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "# After heavy computations\n",
    "clear_output(wait=True)\n",
    "gc.collect()\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "RESULT_FOLDER = \"result\"\n",
    "MODEL_FOLDER = \"model\"\n",
    "model_names = ['Wavenet']  # 'CNN1D', 'Wavenet', 'S4', 'Resnet'\n",
    "# Do batch analysis to find the best hyperparameters\n",
    "seizures = [1, 2, 3, 5, 7]\n",
    "thresholds = [0.8]\n",
    "smooth_windows = [80]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the data from one patient:\n",
    "\n",
    "p66_data = pickle.load(open('data/P66/seizure_All_combined.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e49261ab5184e787",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_grey_matter_channels(matter: pd.DataFrame):\n",
    "    \"\"\"Extract grey matter channels from Matter file\"\"\"\n",
    "    # Get grey matter channels\n",
    "    selected_matter = matter[matter['MatterType'].isin(['G', 'A'])]\n",
    "    grey_matter_channels = selected_matter['ChannelNumber'].values\n",
    "    \n",
    "    return grey_matter_channels\n",
    "\n",
    "p66_data.matter = pd.read_csv('data/P66/matter.csv')\n",
    "all_channels = np.arange(0, p66_data.channelNumber)\n",
    "grey_channel = extract_grey_matter_channels(p66_data.matter) - 1\n",
    "white_channel = np.setdiff1d(all_channels, grey_channel)\n",
    "\n",
    "seizure_data_grey = p66_data.postictal_transformed[:,grey_channel,:,:]\n",
    "seizure_data_white = p66_data.postictal_transformed[:,white_channel,:,:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa00cf25b39ca515",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def visualize_features(seizure_data_grey, seizure_data_white, feature_names, time_aggregation='mean', plot_folder='result/wgfeatures'):\n",
    "    \"\"\"\n",
    "    Visualize features from grey and white matter data to find best separating features,\n",
    "    with outlier removal and proper time-axis aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seizure_data_grey : np.ndarray\n",
    "        Grey matter data with shape [samples, channels, time, features]\n",
    "    seizure_data_white : np.ndarray\n",
    "        White matter data with shape [samples, channels, time, features]\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    time_aggregation : str, optional\n",
    "        Method for aggregating across time axis ('mean' or 'median'), default is 'median'\n",
    "    plot_folder : str, optional\n",
    "        Folder path to save all plots, default is 'results/wgfeatures'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the best features and their metrics\n",
    "    \"\"\"\n",
    "    # Create the plot folder if it doesn't exist\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    \n",
    "    # Get the number of features\n",
    "    n_features = seizure_data_grey.shape[3]\n",
    "    \n",
    "    # Ensure feature_names has right length\n",
    "    if len(feature_names) != n_features:\n",
    "        print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match number of features ({n_features})\")\n",
    "        feature_names = [f\"Feature {i+1}\" for i in range(n_features)]\n",
    "    \n",
    "    # 1. Aggregate data across time dimension\n",
    "    print(f\"Aggregating data across time using {time_aggregation}...\")\n",
    "    \n",
    "    if time_aggregation == 'mean':\n",
    "        # Calculate mean across time dimension\n",
    "        grey_agg = np.mean(seizure_data_grey, axis=2)  # [samples, channels, features]\n",
    "        white_agg = np.mean(seizure_data_white, axis=2)\n",
    "    elif time_aggregation == 'median':\n",
    "        # Calculate median across time dimension\n",
    "        grey_agg = np.median(seizure_data_grey, axis=2)\n",
    "        white_agg = np.median(seizure_data_white, axis=2)\n",
    "    else:\n",
    "        raise ValueError(\"time_aggregation must be either 'mean' or 'median'\")\n",
    "    \n",
    "    # Reshape to 2D: [samples*channels, features]\n",
    "    grey_reshaped = grey_agg.reshape(-1, n_features)\n",
    "    white_reshaped = white_agg.reshape(-1, n_features)\n",
    "    \n",
    "    print(f\"Grey matter data shape after time aggregation: {grey_reshaped.shape}\")\n",
    "    print(f\"White matter data shape after time aggregation: {white_reshaped.shape}\")\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_grey = pd.DataFrame(grey_reshaped, columns=feature_names)\n",
    "    df_white = pd.DataFrame(white_reshaped, columns=feature_names)\n",
    "    \n",
    "    # 2. Remove outliers from each feature (using IQR method)\n",
    "    print(\"Removing outliers...\")\n",
    "    \n",
    "    def remove_outliers(df):\n",
    "        df_clean = df.copy()\n",
    "        for feature in feature_names:\n",
    "            Q1 = df_clean[feature].quantile(0.25)\n",
    "            Q3 = df_clean[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Count outliers before removal\n",
    "            outliers = df_clean[(df_clean[feature] < lower_bound) | (df_clean[feature] > upper_bound)]\n",
    "            outlier_count = len(outliers)\n",
    "            \n",
    "            # Remove outliers\n",
    "            df_clean = df_clean[(df_clean[feature] >= lower_bound) & (df_clean[feature] <= upper_bound)]\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"  Removed {outlier_count} outliers from {feature} ({outlier_count/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    # Apply outlier removal\n",
    "    df_grey_clean = remove_outliers(df_grey)\n",
    "    df_white_clean = remove_outliers(df_white)\n",
    "    \n",
    "    print(f\"After outlier removal: Grey matter: {len(df_grey_clean)} samples, White matter: {len(df_white_clean)} samples\")\n",
    "    \n",
    "    # Add matter labels for combined dataframe\n",
    "    df_grey_clean['Matter'] = 'Grey'\n",
    "    df_white_clean['Matter'] = 'White'\n",
    "    df_clean = pd.concat([df_grey_clean, df_white_clean])\n",
    "    \n",
    "    # 3. Create boxplots for each feature\n",
    "    nrows = (n_features + 2) // 3  # Adjust rows for better layout\n",
    "    fig_width = min(20, n_features * 6)\n",
    "    plt.figure(figsize=(fig_width, nrows * 5))\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        plt.subplot(nrows, 3, i+1)\n",
    "        sns.boxplot(x='Matter', y=feature, data=df_clean)\n",
    "        plt.title(feature)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(plot_folder, 'feature_boxplots.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Feature distributions\n",
    "    plt.figure(figsize=(fig_width, nrows * 5))\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        plt.subplot(nrows, 3, i+1)\n",
    "        sns.histplot(df_grey_clean[feature], color='blue', label='Grey', alpha=0.5, kde=True)\n",
    "        sns.histplot(df_white_clean[feature], color='red', label='White', alpha=0.5, kde=True)\n",
    "        plt.title(f'{feature} Distribution')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(plot_folder, 'feature_distributions.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Calculate ROC curves and AUC for each feature\n",
    "    def calculate_roc(feature_grey, feature_white):\n",
    "        X = np.concatenate([feature_grey, feature_white])\n",
    "        # Grey = 1, White = 0\n",
    "        y = np.concatenate([np.ones(len(feature_grey)), np.zeros(len(feature_white))])\n",
    "        fpr, tpr, _ = roc_curve(y, X)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        return fpr, tpr, roc_auc\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    auc_values = []\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        grey_feature = df_grey_clean[feature].values\n",
    "        white_feature = df_white_clean[feature].values\n",
    "        fpr, tpr, roc_auc = calculate_roc(grey_feature, white_feature)\n",
    "        plt.plot(fpr, tpr, label=f'{feature} (AUC = {roc_auc:.3f})')\n",
    "        auc_values.append(roc_auc)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Individual Features')\n",
    "    plt.legend(loc=\"lower right\", fontsize=8)\n",
    "    plt.savefig(os.path.join(plot_folder, 'roc_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify the best feature based on AUC\n",
    "    best_feature_idx = np.argmax(auc_values)\n",
    "    best_feature_name = feature_names[best_feature_idx]\n",
    "    print(f\"Best single feature: {best_feature_name} with AUC = {auc_values[best_feature_idx]:.3f}\")\n",
    "    \n",
    "    # 6. Create scatter plots for feature pairs\n",
    "    # Create pair-wise AUC matrix\n",
    "    pair_auc_matrix = np.zeros((n_features, n_features))\n",
    "    best_pair_auc = 0\n",
    "    best_pair = None\n",
    "    \n",
    "    # Create a subfolder for scatter plots\n",
    "    scatter_folder = os.path.join(plot_folder, 'scatter_plots')\n",
    "    os.makedirs(scatter_folder, exist_ok=True)\n",
    "    \n",
    "    # Only do top feature combinations if there are many features\n",
    "    feature_indices = range(n_features)\n",
    "    if n_features > 10:  # If more than 10 features, limit pairs to top 5 individual features\n",
    "        top_indices = np.argsort(auc_values)[-5:]\n",
    "        print(\"Too many features, limiting pairs to combinations of top 5 individual features\")\n",
    "        feature_pairs = list(combinations(top_indices, 2))\n",
    "    else:\n",
    "        feature_pairs = list(combinations(feature_indices, 2))\n",
    "    \n",
    "    for i, j in feature_pairs:\n",
    "        feature_i = feature_names[i]\n",
    "        feature_j = feature_names[j]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(df_grey_clean[feature_i], df_grey_clean[feature_j], \n",
    "                  alpha=0.5, label='Grey Matter', color='blue')\n",
    "        plt.scatter(df_white_clean[feature_i], df_white_clean[feature_j], \n",
    "                  alpha=0.5, label='White Matter', color='red')\n",
    "        plt.xlabel(feature_i)\n",
    "        plt.ylabel(feature_j)\n",
    "        plt.title(f'{feature_i} vs {feature_j}')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(scatter_folder, f'feature_scatter_{i+1}_{j+1}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Calculate discrimination power using LDA\n",
    "        X_grey = df_grey_clean[[feature_i, feature_j]].values\n",
    "        X_white = df_white_clean[[feature_i, feature_j]].values\n",
    "        X = np.vstack([X_grey, X_white])\n",
    "        y = np.concatenate([np.ones(len(X_grey)), np.zeros(len(X_white))])\n",
    "        \n",
    "        lda = LDA(n_components=1)\n",
    "        X_lda = lda.fit_transform(X, y)\n",
    "        \n",
    "        # Calculate ROC for the LDA projection\n",
    "        fpr, tpr, _ = roc_curve(y, X_lda.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        pair_auc_matrix[i, j] = roc_auc\n",
    "        pair_auc_matrix[j, i] = roc_auc  # Mirror the matrix\n",
    "        \n",
    "        if roc_auc > best_pair_auc:\n",
    "            best_pair_auc = roc_auc\n",
    "            best_pair = (feature_i, feature_j)\n",
    "    \n",
    "    # Fill diagonal with single feature AUCs\n",
    "    for i in range(n_features):\n",
    "        pair_auc_matrix[i, i] = auc_values[i]\n",
    "    \n",
    "    print(f\"Best feature pair: {best_pair[0]} and {best_pair[1]} with AUC = {best_pair_auc:.3f}\")\n",
    "    \n",
    "    # 7. Feature pair heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # Create shorter feature names for the heatmap if needed\n",
    "    short_names = [name[:10] + '...' if len(name) > 10 else name for name in feature_names]\n",
    "    \n",
    "    sns.heatmap(pair_auc_matrix, annot=True, cmap='YlGnBu', fmt='.3f',\n",
    "                xticklabels=short_names, yticklabels=short_names)\n",
    "    plt.title('AUC Values for Feature Pairs (diagonal = single feature)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, 'feature_pair_auc_heatmap.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. LDA for all features\n",
    "    X_grey_all = df_grey_clean[feature_names].values\n",
    "    X_white_all = df_white_clean[feature_names].values\n",
    "    X_all = np.vstack([X_grey_all, X_white_all])\n",
    "    y_all = np.concatenate([np.ones(len(X_grey_all)), np.zeros(len(X_white_all))])\n",
    "    \n",
    "    lda_all = LDA(n_components=1)\n",
    "    X_lda_all = lda_all.fit_transform(X_all, y_all)\n",
    "    \n",
    "    # Plot LDA projection\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(X_lda_all[y_all==1], color='blue', label='Grey Matter', alpha=0.5, kde=True)\n",
    "    sns.histplot(X_lda_all[y_all==0], color='red', label='White Matter', alpha=0.5, kde=True)\n",
    "    plt.title('LDA Projection Using All Features')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(plot_folder, 'lda_all_features.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate ROC for all-feature LDA projection\n",
    "    fpr_all, tpr_all, _ = roc_curve(y_all, X_lda_all.ravel())\n",
    "    roc_auc_all = auc(fpr_all, tpr_all)\n",
    "    print(f\"AUC using all features with LDA: {roc_auc_all:.3f}\")\n",
    "    \n",
    "    # 9. Feature importance from LDA\n",
    "    feature_importance = np.abs(lda_all.coef_[0])\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Feature', y='Importance', data=importance_df)\n",
    "    plt.title('Feature Importance from LDA')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, 'feature_importance.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Top features by LDA importance:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    df_results = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'AUC': auc_values,\n",
    "        'LDA_Importance': feature_importance\n",
    "    })\n",
    "    df_results = df_results.sort_values('AUC', ascending=False)\n",
    "    df_results.to_csv(os.path.join(plot_folder, 'feature_results.csv'), index=False)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'best_single_feature': best_feature_name,\n",
    "        'best_single_feature_auc': auc_values[best_feature_idx],\n",
    "        'best_feature_pair': best_pair,\n",
    "        'best_feature_pair_auc': best_pair_auc,\n",
    "        'all_features_auc': roc_auc_all,\n",
    "        'feature_importance': importance_df,\n",
    "        'auc_values': dict(zip(feature_names, auc_values)),\n",
    "        'pair_auc_matrix': pd.DataFrame(pair_auc_matrix, index=feature_names, columns=feature_names),\n",
    "        'grey_data_clean': df_grey_clean,\n",
    "        'white_data_clean': df_white_clean\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1905752cec53b576",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = visualize_features(seizure_data_grey, seizure_data_white, p66_data.feature_names,plot_folder='result/wgfeatures/postictal')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96d21ad46e3367ac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasetConstruct import EDFData\n",
    "p66_raw = pickle.load(open('data/P65/seizure_SZ1.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da74fdd12182140",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "raw_grey = p66_raw.interictal[:, grey_channel]\n",
    "raw_white = p66_raw.interictal[:, white_channel]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb127528015710b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_raw_timeseries(grey_matter_data, white_matter_data, channel_names=None, \n",
    "                          plot_folder='result/raw_wg_comparison', fs=250):\n",
    "    \"\"\"\n",
    "    Analyze and visualize differences between grey and white matter using full time series data,\n",
    "    with special focus on frequency band differentiation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grey_matter_data : np.ndarray\n",
    "        Grey matter data with shape [time, channels]\n",
    "    white_matter_data : np.ndarray\n",
    "        White matter data with shape [time, channels]\n",
    "    channel_names : list, optional\n",
    "        List of channel names, default is None (will use indices)\n",
    "    plot_folder : str, optional\n",
    "        Folder path to save all plots\n",
    "    fs : int, optional\n",
    "        Sampling frequency in Hz, default is 250\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    # Create the plot folder if it doesn't exist\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    \n",
    "    # Get dimensions\n",
    "    grey_time, grey_channels = grey_matter_data.shape\n",
    "    white_time, white_channels = white_matter_data.shape\n",
    "    \n",
    "    print(f\"Grey matter data shape: [{grey_time}, {grey_channels}]\")\n",
    "    print(f\"White matter data shape: [{white_time}, {white_channels}]\")\n",
    "    \n",
    "    # Create channel names if not provided\n",
    "    if channel_names is None:\n",
    "        channel_names = [f\"Channel {i+1}\" for i in range(max(grey_channels, white_channels))]\n",
    "    \n",
    "    # 1. Basic time series visualization\n",
    "    # Sample a shorter segment for visualization (e.g., 10 seconds)\n",
    "    sample_duration = min(10 * fs, min(grey_time, white_time))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    # Plot a few random channels from each group\n",
    "    channels_to_plot = min(5, min(grey_channels, white_channels))\n",
    "    grey_indices = np.random.choice(grey_channels, channels_to_plot, replace=False)\n",
    "    white_indices = np.random.choice(white_channels, channels_to_plot, replace=False)\n",
    "    \n",
    "    for i in range(channels_to_plot):\n",
    "        plt.subplot(channels_to_plot, 2, i*2+1)\n",
    "        plt.plot(grey_matter_data[:sample_duration, grey_indices[i]])\n",
    "        plt.title(f\"Grey Matter: {channel_names[grey_indices[i]]}\")\n",
    "        \n",
    "        plt.subplot(channels_to_plot, 2, i*2+2)\n",
    "        plt.plot(white_matter_data[:sample_duration, white_indices[i]])\n",
    "        plt.title(f\"White Matter: {channel_names[white_indices[i]]}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, 'raw_time_series_samples.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Calculate summary statistics for each channel\n",
    "    grey_stats = {\n",
    "        'mean': np.mean(grey_matter_data, axis=0),\n",
    "        'std': np.std(grey_matter_data, axis=0),\n",
    "        'median': np.median(grey_matter_data, axis=0),\n",
    "        'iqr': np.percentile(grey_matter_data, 75, axis=0) - np.percentile(grey_matter_data, 25, axis=0),\n",
    "        'skew': stats.skew(grey_matter_data, axis=0),\n",
    "        'kurtosis': stats.kurtosis(grey_matter_data, axis=0),\n",
    "        'range': np.max(grey_matter_data, axis=0) - np.min(grey_matter_data, axis=0),\n",
    "        'rms': np.sqrt(np.mean(grey_matter_data**2, axis=0))\n",
    "    }\n",
    "    \n",
    "    white_stats = {\n",
    "        'mean': np.mean(white_matter_data, axis=0),\n",
    "        'std': np.std(white_matter_data, axis=0),\n",
    "        'median': np.median(white_matter_data, axis=0),\n",
    "        'iqr': np.percentile(white_matter_data, 75, axis=0) - np.percentile(white_matter_data, 25, axis=0),\n",
    "        'skew': stats.skew(white_matter_data, axis=0),\n",
    "        'kurtosis': stats.kurtosis(white_matter_data, axis=0),\n",
    "        'range': np.max(white_matter_data, axis=0) - np.min(white_matter_data, axis=0),\n",
    "        'rms': np.sqrt(np.mean(white_matter_data**2, axis=0))\n",
    "    }\n",
    "    \n",
    "    # 3. Define frequency bands\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 30),\n",
    "        'low_gamma': (30, 60),\n",
    "        'high_gamma': (60, 100),\n",
    "        'ripple': (100, 200)\n",
    "    }\n",
    "    \n",
    "    # 4. Extract band power for each channel\n",
    "    def calculate_band_powers(data, fs, bands):\n",
    "        \"\"\"Calculate power in each frequency band for each channel\"\"\"\n",
    "        n_channels = data.shape[1]\n",
    "        band_powers = {band: np.zeros(n_channels) for band in bands}\n",
    "        \n",
    "        for ch in range(n_channels):\n",
    "            # Calculate PSD\n",
    "            f, pxx = welch(data[:, ch], fs=fs, nperseg=min(1024, len(data)), scaling='density')\n",
    "            \n",
    "            # Calculate power in each band\n",
    "            for band, (low_freq, high_freq) in bands.items():\n",
    "                band_idx = np.logical_and(f >= low_freq, f <= high_freq)\n",
    "                if np.any(band_idx):\n",
    "                    band_powers[band][ch] = np.mean(pxx[band_idx])\n",
    "        \n",
    "        return band_powers\n",
    "    \n",
    "    # Calculate band powers for grey and white matter\n",
    "    grey_band_powers = calculate_band_powers(grey_matter_data, fs, freq_bands)\n",
    "    white_band_powers = calculate_band_powers(white_matter_data, fs, freq_bands)\n",
    "    \n",
    "    # 5. Calculate band power ratios (can be more discriminative than absolute powers)\n",
    "    band_pairs = [\n",
    "        ('theta', 'delta'),  # theta/delta ratio\n",
    "        ('alpha', 'theta'),  # alpha/theta ratio\n",
    "        ('beta', 'alpha'),   # beta/alpha ratio\n",
    "        ('low_gamma', 'beta'),  # low gamma/beta ratio\n",
    "        ('high_gamma', 'low_gamma'),  # high gamma/low gamma ratio\n",
    "        ('ripple', 'high_gamma')  # ripple/high gamma ratio\n",
    "    ]\n",
    "    \n",
    "    grey_band_ratios = {}\n",
    "    white_band_ratios = {}\n",
    "    \n",
    "    for num_band, denom_band in band_pairs:\n",
    "        ratio_name = f\"{num_band}/{denom_band}\"\n",
    "        grey_band_ratios[ratio_name] = grey_band_powers[num_band] / (grey_band_powers[denom_band] + 1e-10)  # avoid div by zero\n",
    "        white_band_ratios[ratio_name] = white_band_powers[num_band] / (white_band_powers[denom_band] + 1e-10)  # avoid div by zero\n",
    "    \n",
    "    # 6. Combine features for ROC analysis\n",
    "    all_features = {}\n",
    "    \n",
    "    # Add statistical features\n",
    "    for stat in grey_stats:\n",
    "        all_features[stat] = {\n",
    "            'grey': grey_stats[stat],\n",
    "            'white': white_stats[stat]\n",
    "        }\n",
    "    \n",
    "    # Add band power features\n",
    "    for band in freq_bands:\n",
    "        all_features[f\"power_{band}\"] = {\n",
    "            'grey': grey_band_powers[band],\n",
    "            'white': white_band_powers[band]\n",
    "        }\n",
    "    \n",
    "    # Add band power ratio features\n",
    "    for ratio_name in grey_band_ratios:\n",
    "        all_features[f\"ratio_{ratio_name}\"] = {\n",
    "            'grey': grey_band_ratios[ratio_name],\n",
    "            'white': white_band_ratios[ratio_name]\n",
    "        }\n",
    "    \n",
    "    # 7. Calculate ROC for each feature\n",
    "    roc_results = {}\n",
    "    \n",
    "    for feature_name, feature_data in all_features.items():\n",
    "        grey_feature = feature_data['grey']\n",
    "        white_feature = feature_data['white']\n",
    "        \n",
    "        # Skip features with NaN or inf values\n",
    "        if np.any(np.isnan(grey_feature)) or np.any(np.isnan(white_feature)) or \\\n",
    "           np.any(np.isinf(grey_feature)) or np.any(np.isinf(white_feature)):\n",
    "            continue\n",
    "            \n",
    "        X = np.concatenate([grey_feature, white_feature])\n",
    "        y = np.concatenate([np.ones_like(grey_feature), np.zeros_like(white_feature)])\n",
    "        \n",
    "        # Calculate ROC\n",
    "        fpr, tpr, _ = roc_curve(y, X)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        roc_results[feature_name] = {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'auc': roc_auc\n",
    "        }\n",
    "    \n",
    "    # 8. Plot ROC curves for frequency band features\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # First, plot ROC curves for band powers\n",
    "    for band in freq_bands:\n",
    "        feature_name = f\"power_{band}\"\n",
    "        if feature_name in roc_results:\n",
    "            plt.plot(\n",
    "                roc_results[feature_name]['fpr'], \n",
    "                roc_results[feature_name]['tpr'], \n",
    "                label=f\"{feature_name} (AUC = {roc_results[feature_name]['auc']:.3f})\"\n",
    "            )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random classifier line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Frequency Band Powers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'frequency_bands_roc.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 9. Plot ROC curves for band power ratios\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for ratio_name in grey_band_ratios:\n",
    "        feature_name = f\"ratio_{ratio_name}\"\n",
    "        if feature_name in roc_results:\n",
    "            plt.plot(\n",
    "                roc_results[feature_name]['fpr'], \n",
    "                roc_results[feature_name]['tpr'], \n",
    "                label=f\"{feature_name} (AUC = {roc_results[feature_name]['auc']:.3f})\"\n",
    "            )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random classifier line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Frequency Band Ratios')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'frequency_ratios_roc.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 10. Plot ROC curves for best features overall\n",
    "    # Sort features by AUC\n",
    "    sorted_features = sorted(roc_results.items(), key=lambda x: x[1]['auc'], reverse=True)\n",
    "    \n",
    "    # Plot top 10 features (or fewer if there are less than 10)\n",
    "    top_n = min(10, len(sorted_features))\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i in range(top_n):\n",
    "        feature_name, feature_data = sorted_features[i]\n",
    "        plt.plot(\n",
    "            feature_data['fpr'], \n",
    "            feature_data['tpr'], \n",
    "            label=f\"{feature_name} (AUC = {feature_data['auc']:.3f})\"\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random classifier line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Top Features')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'top_features_roc.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 11. Create feature comparison table\n",
    "    feature_comparison = []\n",
    "    for feature_name, feature_data in sorted_features:\n",
    "        # Calculate mean and std for grey and white matter\n",
    "        if feature_name in all_features:\n",
    "            grey_mean = np.mean(all_features[feature_name]['grey'])\n",
    "            grey_std = np.std(all_features[feature_name]['grey'])\n",
    "            white_mean = np.mean(all_features[feature_name]['white'])\n",
    "            white_std = np.std(all_features[feature_name]['white'])\n",
    "            \n",
    "            feature_comparison.append({\n",
    "                'Feature': feature_name,\n",
    "                'AUC': feature_data['auc'],\n",
    "                'Grey Mean': grey_mean,\n",
    "                'Grey STD': grey_std,\n",
    "                'White Mean': white_mean,\n",
    "                'White STD': white_std,\n",
    "                'Mean Diff': grey_mean - white_mean,\n",
    "                'Relative Diff (%)': 100 * (grey_mean - white_mean) / (abs(white_mean) + 1e-10)\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(feature_comparison)\n",
    "    comparison_df.to_csv(os.path.join(plot_folder, 'feature_comparison.csv'))\n",
    "    \n",
    "    # 12. Visualize the most discriminative features\n",
    "    top_features = comparison_df.head(min(5, len(comparison_df)))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (_, row) in enumerate(top_features.iterrows()):\n",
    "        feature_name = row['Feature']\n",
    "        \n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sns.histplot(all_features[feature_name]['grey'], color='blue', label='Grey Matter', kde=True, alpha=0.5)\n",
    "        sns.histplot(all_features[feature_name]['white'], color='red', label='White Matter', kde=True, alpha=0.5)\n",
    "        plt.title(f\"{feature_name} (AUC: {row['AUC']:.3f})\")\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, 'top_features_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 13. Average power spectra comparison\n",
    "    # Calculate average PSD across all channels\n",
    "    def calculate_avg_psd(data, fs):\n",
    "        \"\"\"Calculate average PSD across all channels\"\"\"\n",
    "        n_channels = data.shape[1]\n",
    "        all_pxx = []\n",
    "        \n",
    "        for ch in range(n_channels):\n",
    "            f, pxx = welch(data[:, ch], fs=fs, nperseg=min(2048, len(data)))\n",
    "            all_pxx.append(pxx)\n",
    "        \n",
    "        avg_pxx = np.mean(np.array(all_pxx), axis=0)\n",
    "        return f, avg_pxx\n",
    "    \n",
    "    f_grey, pxx_grey = calculate_avg_psd(grey_matter_data, fs)\n",
    "    f_white, pxx_white = calculate_avg_psd(white_matter_data, fs)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.semilogy(f_grey, pxx_grey, label='Grey Matter', color='blue')\n",
    "    plt.semilogy(f_white, pxx_white, label='White Matter', color='red')\n",
    "    \n",
    "    # Shade the frequency bands\n",
    "    colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightpink', 'lavender', 'peachpuff', 'mistyrose']\n",
    "    i = 0\n",
    "    for band, (low, high) in freq_bands.items():\n",
    "        if low <= max(f_grey) and high >= min(f_grey):\n",
    "            # Adjust boundaries to be within plot range\n",
    "            low_adj = max(low, min(f_grey))\n",
    "            high_adj = min(high, max(f_grey))\n",
    "            \n",
    "            # Find nearest indices\n",
    "            low_idx = np.argmin(np.abs(f_grey - low_adj))\n",
    "            high_idx = np.argmin(np.abs(f_grey - high_adj))\n",
    "            \n",
    "            # Calculate max y value at these frequencies for proper shading\n",
    "            y_max = max(np.max(pxx_grey[low_idx:high_idx+1]), np.max(pxx_white[low_idx:high_idx+1]))\n",
    "            \n",
    "            # Add shaded region\n",
    "            plt.fill_between(\n",
    "                f_grey[low_idx:high_idx+1], \n",
    "                0, y_max, \n",
    "                color=colors[i % len(colors)], \n",
    "                alpha=0.3, \n",
    "                label=band\n",
    "            )\n",
    "            i += 1\n",
    "    \n",
    "    plt.title('Average Power Spectral Density')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power Spectral Density (log scale)')\n",
    "    plt.xlim([0,150])\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'average_psd_with_bands.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Return results summary\n",
    "    return {\n",
    "        'feature_comparison': comparison_df,\n",
    "        'roc_results': roc_results,\n",
    "        'grey_band_powers': grey_band_powers,\n",
    "        'white_band_powers': white_band_powers,\n",
    "        'grey_band_ratios': grey_band_ratios,\n",
    "        'white_band_ratios': white_band_ratios\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4c0a2f9fd7f24eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "channel_names = p66_data.matter['ElectrodeName'].values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda9017ae9535797",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results = analyze_raw_timeseries(raw_grey, raw_white, channel_names, plot_folder='result/65sz1/raw_wg_comparison',fs=p66_raw.samplingRate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f68ce10ffad960f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_and_classify_features(grey_matter_data, white_matter_data, \n",
    "                                  plot_folder='result/wg_classification', fs=250):\n",
    "    \"\"\"\n",
    "    Extract features from raw time series, perform PCA, and classify grey vs white matter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grey_matter_data : np.ndarray\n",
    "        Grey matter data with shape [time, channels]\n",
    "    white_matter_data : np.ndarray\n",
    "        White matter data with shape [time, channels]\n",
    "    plot_folder : str, optional\n",
    "        Folder path to save all plots, default is 'results/wg_classification'\n",
    "    fs : int, optional\n",
    "        Sampling frequency in Hz, default is 250\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing classification results and PCA components\n",
    "    \"\"\"\n",
    "    # Create the plot folder if it doesn't exist\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    \n",
    "    # Get dimensions\n",
    "    grey_time, grey_channels = grey_matter_data.shape\n",
    "    white_time, white_channels = white_matter_data.shape\n",
    "    \n",
    "    print(f\"Grey matter data shape: [{grey_time}, {grey_channels}]\")\n",
    "    print(f\"White matter data shape: [{white_time}, {white_channels}]\")\n",
    "    \n",
    "    # 1. Extract features from raw data\n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    # Define frequency bands\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 13),\n",
    "        'beta': (13, 30),\n",
    "        'low_gamma': (30, 60),\n",
    "        'high_gamma': (60, 150)\n",
    "    }\n",
    "    \n",
    "    # Function to extract features from a channel\n",
    "    def extract_channel_features(signal):\n",
    "        features = {}\n",
    "        \n",
    "        # Time domain statistical features\n",
    "        features['mean'] = np.mean(signal)\n",
    "        features['std'] = np.std(signal)\n",
    "        features['median'] = np.median(signal)\n",
    "        features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "        features['skew'] = stats.skew(signal)\n",
    "        features['kurtosis'] = stats.kurtosis(signal)\n",
    "        features['range'] = np.max(signal) - np.min(signal)\n",
    "        features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "        features['zero_crossings'] = np.sum(np.diff(np.signbit(signal).astype(int)) != 0)\n",
    "        \n",
    "        # Frequency domain features\n",
    "        f, pxx = welch(signal, fs=fs, nperseg=min(1024, len(signal)))\n",
    "        \n",
    "        # Band powers\n",
    "        for band_name, (low_freq, high_freq) in freq_bands.items():\n",
    "            band_idx = np.logical_and(f >= low_freq, f <= high_freq)\n",
    "            if np.any(band_idx):\n",
    "                features[f'power_{band_name}'] = np.mean(pxx[band_idx])\n",
    "            else:\n",
    "                features[f'power_{band_name}'] = 0\n",
    "        \n",
    "        # Band power ratios\n",
    "        band_pairs = [\n",
    "            ('theta', 'delta'),   # theta/delta ratio\n",
    "            ('alpha', 'theta'),   # alpha/theta ratio\n",
    "            ('beta', 'alpha'),    # beta/alpha ratio\n",
    "            ('low_gamma', 'beta'),  # low gamma/beta ratio\n",
    "            ('high_gamma', 'low_gamma')  # high gamma/low gamma ratio\n",
    "        ]\n",
    "        \n",
    "        for num_band, denom_band in band_pairs:\n",
    "            features[f'ratio_{num_band}_{denom_band}'] = (\n",
    "                features[f'power_{num_band}'] / (features[f'power_{denom_band}'] + 1e-10)  # avoid div by zero\n",
    "            )\n",
    "        \n",
    "        # Total power\n",
    "        features['total_power'] = np.sum(pxx)\n",
    "        \n",
    "        # Spectral edge frequency (95%)\n",
    "        total_power = np.sum(pxx)\n",
    "        power_sum = 0\n",
    "        for i, power in enumerate(pxx):\n",
    "            power_sum += power\n",
    "            if power_sum >= 0.95 * total_power:\n",
    "                features['spectral_edge_freq'] = f[i]\n",
    "                break\n",
    "        \n",
    "        # Spectral entropy\n",
    "        pxx_norm = pxx / np.sum(pxx)\n",
    "        features['spectral_entropy'] = -np.sum(pxx_norm * np.log2(pxx_norm + 1e-10))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # Extract features for each channel\n",
    "    grey_features_list = []\n",
    "    for ch in range(grey_channels):\n",
    "        features = extract_channel_features(grey_matter_data[:, ch])\n",
    "        grey_features_list.append(features)\n",
    "    \n",
    "    white_features_list = []\n",
    "    for ch in range(white_channels):\n",
    "        features = extract_channel_features(white_matter_data[:, ch])\n",
    "        white_features_list.append(features)\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrames\n",
    "    grey_df = pd.DataFrame(grey_features_list)\n",
    "    white_df = pd.DataFrame(white_features_list)\n",
    "    \n",
    "    print(f\"Extracted {len(grey_df.columns)} features from each channel\")\n",
    "    \n",
    "    # 2. Clean data - check for NaNs or infinite values\n",
    "    # Handle NaNs in grey_df\n",
    "    if grey_df.isna().any().any():\n",
    "        print(f\"Found NaN values in grey matter features, filling with column medians\")\n",
    "        grey_df = grey_df.fillna(grey_df.median())\n",
    "    \n",
    "    # Handle NaNs in white_df\n",
    "    if white_df.isna().any().any():\n",
    "        print(f\"Found NaN values in white matter features, filling with column medians\")\n",
    "        white_df = white_df.fillna(white_df.median())\n",
    "    \n",
    "    # Handle infinites in grey_df\n",
    "    inf_mask_grey = np.isinf(grey_df.values)\n",
    "    if np.any(inf_mask_grey):\n",
    "        print(f\"Found infinite values in grey matter features, replacing with column medians\")\n",
    "        for col_idx in range(grey_df.shape[1]):\n",
    "            col_inf_mask = inf_mask_grey[:, col_idx]\n",
    "            if np.any(col_inf_mask):\n",
    "                grey_df.iloc[col_inf_mask, col_idx] = grey_df.iloc[~col_inf_mask, col_idx].median()\n",
    "    \n",
    "    # Handle infinites in white_df\n",
    "    inf_mask_white = np.isinf(white_df.values)\n",
    "    if np.any(inf_mask_white):\n",
    "        print(f\"Found infinite values in white matter features, replacing with column medians\")\n",
    "        for col_idx in range(white_df.shape[1]):\n",
    "            col_inf_mask = inf_mask_white[:, col_idx]\n",
    "            if np.any(col_inf_mask):\n",
    "                white_df.iloc[col_inf_mask, col_idx] = white_df.iloc[~col_inf_mask, col_idx].median()\n",
    "    \n",
    "    # 3. Make sure both dataframes have the same columns\n",
    "    common_features = list(set(grey_df.columns).intersection(set(white_df.columns)))\n",
    "    grey_df = grey_df[common_features]\n",
    "    white_df = white_df[common_features]\n",
    "    \n",
    "    # 4. Add labels and combine data\n",
    "    grey_df['Matter'] = 'Grey'\n",
    "    white_df['Matter'] = 'White'\n",
    "    combined_df = pd.concat([grey_df, white_df])\n",
    "    \n",
    "    # 5. Prepare data for PCA and classification\n",
    "    X = combined_df.drop('Matter', axis=1)\n",
    "    y = combined_df['Matter'].map({'Grey': 1, 'White': 0})\n",
    "    \n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # 6. Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 7. Apply PCA\n",
    "    # Try different numbers of components\n",
    "    variance_threshold = 0.95  # Capture 95% of variance\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Individual')\n",
    "    plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative')\n",
    "    plt.axhline(y=variance_threshold, color='r', linestyle='--', label=f'{variance_threshold*100}% Variance')\n",
    "    \n",
    "    # Find number of components for threshold\n",
    "    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
    "    plt.axvline(x=n_components, color='r', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'pca_explained_variance.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Number of components needed for {variance_threshold*100}% variance: {n_components}\")\n",
    "    \n",
    "    # 8. Apply PCA with the determined number of components\n",
    "    pca = PCA(n_components=min(n_components, len(feature_names)))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # 9. Visualize the PCA results (first 2 or 3 components)\n",
    "    # 2D scatter plot (first 2 components)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', alpha=0.7, s=50)\n",
    "    plt.colorbar(scatter, label='Grey Matter (1) vs White Matter (0)')\n",
    "    plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.1%} variance)')\n",
    "    plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.1%} variance)')\n",
    "    plt.title('PCA of Grey vs White Matter Features (2D)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'pca_2d_scatter.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # If we have at least 3 components, create a 3D scatter plot\n",
    "    if X_pca.shape[1] >= 3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "            X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "            c=y, cmap='coolwarm', alpha=0.7, s=50\n",
    "        )\n",
    "        \n",
    "        ax.set_xlabel(f'PC1 ({explained_variance[0]:.1%})')\n",
    "        ax.set_ylabel(f'PC2 ({explained_variance[1]:.1%})')\n",
    "        ax.set_zlabel(f'PC3 ({explained_variance[2]:.1%})')\n",
    "        \n",
    "        plt.colorbar(scatter, label='Grey Matter (1) vs White Matter (0)')\n",
    "        plt.title('PCA of Grey vs White Matter Features (3D)')\n",
    "        plt.savefig(os.path.join(plot_folder, 'pca_3d_scatter.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # 10. Feature importance in PCA\n",
    "    # Get the top 20 features that contribute most to the first 2 PCs\n",
    "    pc1_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'PC1_Importance': np.abs(pca.components_[0])\n",
    "    }).sort_values('PC1_Importance', ascending=False)\n",
    "    \n",
    "    pc2_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'PC2_Importance': np.abs(pca.components_[1])\n",
    "    }).sort_values('PC2_Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 10 features for PC1 and PC2\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(x='PC1_Importance', y='Feature', data=pc1_importance.head(10))\n",
    "    plt.title('Top 10 Features for Principal Component 1')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.barplot(x='PC2_Importance', y='Feature', data=pc2_importance.head(10))\n",
    "    plt.title('Top 10 Features for Principal Component 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, 'pca_feature_importance.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 11. Classification Analysis\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Define classifiers to evaluate\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM (RBF kernel)': SVC(probability=True, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'MLP Neural Network': MLPClassifier(max_iter=1000, random_state=42),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'LDA': LDA()\n",
    "    }\n",
    "    \n",
    "    # Evaluate classifiers\n",
    "    results = {}\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i, (name, clf) in enumerate(classifiers.items()):\n",
    "        # Train and evaluate with cross-validation\n",
    "        cv_scores = cross_val_score(clf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        # Train on the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # If classifier supports predict_proba, calculate ROC\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'cv_accuracy': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_accuracy': accuracy,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "            'importance': None\n",
    "        }\n",
    "        \n",
    "        # Store feature importance if available\n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            results[name]['importance'] = clf.feature_importances_\n",
    "        elif hasattr(clf, \"coef_\"):\n",
    "            results[name]['importance'] = np.abs(clf.coef_[0]) if clf.coef_.ndim > 1 else np.abs(clf.coef_)\n",
    "    \n",
    "    # Finalize ROC plot\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random classifier line\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Different Classifiers')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, 'classifier_roc_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 12. Summarize classifier performance\n",
    "    performance_summary = []\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        performance_summary.append({\n",
    "            'Classifier': name,\n",
    "            'CV Accuracy': f\"{result['cv_accuracy']:.3f}  {result['cv_std']:.3f}\",\n",
    "            'Test Accuracy': f\"{result['test_accuracy']:.3f}\",\n",
    "            'Precision (Grey)': f\"{result['classification_report'].get('1', {}).get('precision', 0):.3f}\",\n",
    "            'Recall (Grey)': f\"{result['classification_report'].get('1', {}).get('recall', 0):.3f}\",\n",
    "            'F1 Score (Grey)': f\"{result['classification_report'].get('1', {}).get('f1-score', 0):.3f}\"\n",
    "        })\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_summary)\n",
    "    performance_df.to_csv(os.path.join(plot_folder, 'classifier_performance.csv'), index=False)\n",
    "    \n",
    "    # 13. Visualize classifier performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    performance_plot_df = pd.DataFrame([\n",
    "        {\n",
    "            'Classifier': row['Classifier'],\n",
    "            'CV Accuracy': float(row['CV Accuracy'].split(' ')[0])\n",
    "        } \n",
    "        for row in performance_summary\n",
    "    ])\n",
    "    \n",
    "    ax = sns.barplot(x='Classifier', y='CV Accuracy', data=performance_plot_df)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.3f}\", \n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha='center', va='bottom'\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel('Cross-Validation Accuracy')\n",
    "    plt.title('Classifier Performance Comparison')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder, 'classifier_performance.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 14. Plot feature importance for the best classifier\n",
    "    best_classifier = max(results.items(), key=lambda x: x[1]['cv_accuracy'])\n",
    "    best_name, best_result = best_classifier\n",
    "    \n",
    "    if best_result['importance'] is not None:\n",
    "        # Create a DataFrame with feature importance\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': best_result['importance']\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top 15 features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "        plt.title(f'Top 15 Features Importance for {best_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_folder, 'best_classifier_feature_importance.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Save all feature importance\n",
    "        importance_df.to_csv(os.path.join(plot_folder, 'feature_importance.csv'), index=False)\n",
    "    \n",
    "    # 15. Visualize feature distributions\n",
    "    # Select top 6 features based on importance\n",
    "    if best_result['importance'] is not None:\n",
    "        top_features = importance_df.head(6)['Feature'].values\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, feature in enumerate(top_features):\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            sns.histplot(grey_df[feature], color='blue', label='Grey', kde=True, alpha=0.5)\n",
    "            sns.histplot(white_df[feature], color='red', label='White', kde=True, alpha=0.5)\n",
    "            plt.title(feature)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_folder, 'top_features_distribution.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'grey_features': grey_df,\n",
    "        'white_features': white_df,\n",
    "        'pca': pca,\n",
    "        'pca_data': X_pca,\n",
    "        'classifier_results': results,\n",
    "        'performance_summary': performance_df,\n",
    "        'best_classifier': best_name,\n",
    "        'feature_names': feature_names\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f2c4bcaa183a865",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "classification_results = extract_and_classify_features(raw_grey, raw_white, plot_folder='result/65sz1/wg_classification', fs=p66_raw.samplingRate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f551bc81f965768",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8a43f4c10a0a6912"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
