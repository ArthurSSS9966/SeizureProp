{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# At the start of your notebook\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "\n",
    "# After heavy computations\n",
    "clear_output(wait=True)\n",
    "gc.collect()\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "RESULT_FOLDER = \"result\"\n",
    "MODEL_FOLDER = \"model\"\n",
    "model_names = ['Wavenet']  # 'CNN1D', 'Wavenet', 'S4', 'Resnet'\n",
    "# Do batch analysis to find the best hyperparameters\n",
    "seizures = [1, 2, 3, 5, 7]\n",
    "thresholds = [0.8]\n",
    "smooth_windows = [80]\n",
    "patientID = 'P65'  # P65\n",
    "seizureID = f'{patientID}SZ'"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the data from one patient:\n",
    "\n",
    "p66_data = pickle.load(open(f'data/{patientID}/seizure_All_combined.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e49261ab5184e787",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_grey_matter_channels(matter: pd.DataFrame):\n",
    "    \"\"\"Extract grey matter channels from Matter file\"\"\"\n",
    "    # Get grey matter channels\n",
    "    selected_matter = matter[matter['MatterType'].isin(['G', 'A'])]\n",
    "    grey_matter_channels = selected_matter['ChannelNumber'].values\n",
    "    \n",
    "    return grey_matter_channels\n",
    "\n",
    "p66_data.matter = pd.read_csv(f'data/{patientID}/matter.csv')\n",
    "all_channels = np.arange(0, p66_data.channelNumber)\n",
    "grey_channel = extract_grey_matter_channels(p66_data.matter) - 1\n",
    "white_channel = np.setdiff1d(all_channels, grey_channel)\n",
    "\n",
    "seizure_data_grey = p66_data.ictal[:,:,grey_channel]\n",
    "seizure_data_white = p66_data.ictal[:,:,white_channel]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa00cf25b39ca515",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasetConstruct import EDFData\n",
    "p66_raw = pickle.load(open(f'data/{patientID}/seizure_SZ1.pkl', \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da74fdd12182140",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "raw_grey = p66_raw.ictal[:, grey_channel]\n",
    "raw_white = p66_raw.ictal[:, white_channel]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb127528015710b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "channel_names = p66_data.matter['ElectrodeName'].values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda9017ae9535797",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine the segment data for seizure_data_grey and seizure_data_white\n",
    "seizure_data_grey_new = np.concatenate(seizure_data_grey, axis=0)\n",
    "seizure_data_white_new = np.concatenate(seizure_data_white, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47aebafcb78e1824",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, GroupKFold, \n",
    "                                   cross_val_score)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
    "                           balanced_accuracy_score, roc_auc_score, confusion_matrix,\n",
    "                           classification_report, roc_curve, auc)\n",
    "\n",
    "\n",
    "def improved_extract_and_classify_features_complete(grey_matter_data, white_matter_data, \n",
    "                                         plot_folder='result/complete_strict_validation', \n",
    "                                         fs=250, \n",
    "                                         use_windowing=True, \n",
    "                                         n_windows_per_channel=None,\n",
    "                                         window_overlap=None,\n",
    "                                         min_window_length=None,\n",
    "                                         validation_type='strict',\n",
    "                                         max_samples_per_channel=20,\n",
    "                                         balance_method='hybrid',\n",
    "                                         class_weight='balanced'):\n",
    "    \"\"\"\n",
    "    ÂÆåÊï¥ÁöÑgrey/white matterÂàÜÁ±ªÁ≥ªÁªüÔºåÂåÖÂê´strict validationÂíåchannel-levelÂàÜÊûê\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    grey_matter_data : np.ndarray\n",
    "        Grey matter data with shape [time, channels]\n",
    "    white_matter_data : np.ndarray\n",
    "        White matter data with shape [time, channels]\n",
    "    plot_folder : str\n",
    "        Folder path to save all plots\n",
    "    fs : int\n",
    "        Sampling frequency in Hz\n",
    "    use_windowing : bool\n",
    "        Whether to use time windowing to increase sample size\n",
    "    n_windows_per_channel : int, optional\n",
    "        Maximum number of windows per channel\n",
    "    window_overlap : float, optional\n",
    "        Overlap between windows (0-1)\n",
    "    min_window_length : int, optional\n",
    "        Minimum window length in samples\n",
    "    validation_type : str\n",
    "        'strict' = channel-wise split, 'normal' = random split\n",
    "    max_samples_per_channel : int\n",
    "        Maximum number of samples per channel\n",
    "    balance_method : str\n",
    "        'downsample', 'upsample', 'hybrid'\n",
    "    class_weight : str or dict\n",
    "        Class weighting for classifiers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Complete results including sample-level and channel-level analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    \n",
    "    # Get dimensions\n",
    "    grey_time, grey_channels = grey_matter_data.shape\n",
    "    white_time, white_channels = white_matter_data.shape\n",
    "    \n",
    "    print(f\"COMPLETE GREY/WHITE MATTER CLASSIFICATION SYSTEM\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"VALIDATION MODE: {validation_type}\")\n",
    "    print(f\"Grey matter data shape: [{grey_time}, {grey_channels}]\")\n",
    "    print(f\"White matter data shape: [{white_time}, {white_channels}]\")\n",
    "    print(f\"Windowing approach: {use_windowing}\")\n",
    "    print(f\"Balance method: {balance_method}\")\n",
    "    print(f\"Class weight: {class_weight}\")\n",
    "    print(f\"Max samples per channel: {max_samples_per_channel}\")\n",
    "    \n",
    "    # Calculate windowing parameters\n",
    "    if use_windowing:\n",
    "        if min_window_length is None:\n",
    "            window_duration = 0.05  # 50ms\n",
    "            min_window_length = int(window_duration * fs)\n",
    "        \n",
    "        if window_overlap is None:\n",
    "            window_step = 0.025     # 25ms step\n",
    "            step_samples = int(window_step * fs)\n",
    "        else:\n",
    "            step_samples = int(min_window_length * (1 - window_overlap))\n",
    "        \n",
    "        if n_windows_per_channel is None:\n",
    "            max_windows_grey = (grey_time - min_window_length) // step_samples + 1\n",
    "            max_windows_white = (white_time - min_window_length) // step_samples + 1\n",
    "            n_windows_per_channel = min(max_windows_grey, max_windows_white)\n",
    "            \n",
    "        print(f\"Window parameters:\")\n",
    "        print(f\"  Window size: {min_window_length} samples ({min_window_length/fs:.3f}s)\")\n",
    "        print(f\"  Step size: {step_samples} samples ({step_samples/fs:.3f}s)\")\n",
    "        print(f\"  Expected windows per channel: {n_windows_per_channel}\")\n",
    "    \n",
    "    # Define frequency bands\n",
    "    freq_bands = {\n",
    "        'delta': (0.5, 4),\n",
    "        'theta': (4, 8),        \n",
    "        'alpha': (8, 13),       \n",
    "        'beta': (13, 30),       \n",
    "        'gamma': (30, 100),     \n",
    "        'high_gamma': (100, min(200, fs//2))\n",
    "    }\n",
    "    \n",
    "    def extract_half_wave_features(signal, amplitude_threshold=0.1):\n",
    "        \"\"\"Extract half-wave features\"\"\"\n",
    "        zero_crossings = np.where(np.diff(np.signbit(signal)))[0]\n",
    "        \n",
    "        if len(zero_crossings) <= 1:\n",
    "            return {\n",
    "                'hw_count': 0,\n",
    "                'hw_mean_amp': 0,\n",
    "                'hw_mean_duration': 0\n",
    "            }\n",
    "        \n",
    "        half_wave_amps = []\n",
    "        half_wave_durations = []\n",
    "        \n",
    "        for i in range(len(zero_crossings) - 1):\n",
    "            start_idx = zero_crossings[i]\n",
    "            end_idx = zero_crossings[i + 1]\n",
    "            duration = end_idx - start_idx\n",
    "            segment = signal[start_idx:end_idx]\n",
    "            \n",
    "            if len(segment) > 0:\n",
    "                amplitude = np.max(np.abs(segment))\n",
    "                if amplitude >= amplitude_threshold:\n",
    "                    half_wave_amps.append(amplitude)\n",
    "                    half_wave_durations.append(duration)\n",
    "        \n",
    "        return {\n",
    "            'hw_count': len(half_wave_amps),\n",
    "            'hw_mean_amp': np.mean(half_wave_amps) if half_wave_amps else 0,\n",
    "            'hw_mean_duration': np.mean(half_wave_durations) / fs if half_wave_durations else 0\n",
    "        }\n",
    "    \n",
    "    def extract_comprehensive_features_from_window(signal):\n",
    "        \"\"\"Extract comprehensive features from a signal window\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        if len(signal) < 10 or np.all(signal == 0):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Time domain statistical features\n",
    "            features['mean'] = np.mean(signal)\n",
    "            features['std'] = np.std(signal)\n",
    "            features['median'] = np.median(signal)\n",
    "            features['iqr'] = np.percentile(signal, 75) - np.percentile(signal, 25)\n",
    "            \n",
    "            try:\n",
    "                features['skew'] = stats.skew(signal)\n",
    "                features['kurtosis'] = stats.kurtosis(signal)\n",
    "            except:\n",
    "                features['skew'] = 0\n",
    "                features['kurtosis'] = 0\n",
    "                \n",
    "            features['range'] = np.max(signal) - np.min(signal)\n",
    "            features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "            features['zero_crossings'] = np.sum(np.diff(np.signbit(signal).astype(int)) != 0)\n",
    "            \n",
    "            # Half-wave features\n",
    "            hw_features = extract_half_wave_features(signal)\n",
    "            features.update(hw_features)\n",
    "            \n",
    "            # Line length and area features\n",
    "            features['line_length'] = np.sum(np.abs(np.diff(signal)))\n",
    "            features['area'] = np.sum(np.abs(signal))\n",
    "            \n",
    "            # Frequency domain features\n",
    "            try:\n",
    "                windowed_signal = signal * np.hamming(len(signal))\n",
    "                from scipy.fftpack import fft\n",
    "                fft_vals = fft(windowed_signal)\n",
    "                fft_abs = np.abs(fft_vals[:len(signal) // 2])\n",
    "                fft_abs = fft_abs / len(signal)\n",
    "                freq_bins = np.fft.fftfreq(len(signal), 1 / fs)[:len(signal) // 2]\n",
    "                \n",
    "                # Band powers\n",
    "                for band_name, (low_freq, high_freq) in freq_bands.items():\n",
    "                    band_mask = (freq_bins >= low_freq) & (freq_bins <= high_freq)\n",
    "                    if np.any(band_mask):\n",
    "                        band_power = np.sum(fft_abs[band_mask] ** 2)\n",
    "                        features[f'power_{band_name}'] = band_power\n",
    "                    else:\n",
    "                        features[f'power_{band_name}'] = 0\n",
    "                \n",
    "                # Total power\n",
    "                total_power = sum([features[f'power_{band}'] for band in freq_bands.keys()])\n",
    "                features['total_power'] = total_power\n",
    "                \n",
    "                # Spectral edge frequency (95%)\n",
    "                if len(fft_abs) > 0 and total_power > 0:\n",
    "                    cumulative_power = np.cumsum(fft_abs ** 2)\n",
    "                    edge_95_idx = np.argmax(cumulative_power >= 0.95 * np.sum(fft_abs ** 2))\n",
    "                    features['spectral_edge_freq'] = freq_bins[edge_95_idx] if edge_95_idx > 0 else freq_bins[-1]\n",
    "                else:\n",
    "                    features['spectral_edge_freq'] = 0\n",
    "                \n",
    "                # Spectral entropy\n",
    "                if total_power > 0:\n",
    "                    power_spectrum = fft_abs ** 2\n",
    "                    pxx_norm = power_spectrum / np.sum(power_spectrum)\n",
    "                    features['spectral_entropy'] = -np.sum(pxx_norm * np.log2(pxx_norm + 1e-10))\n",
    "                else:\n",
    "                    features['spectral_entropy'] = 0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Spectral analysis failed: {str(e)}\")\n",
    "                for band_name in freq_bands.keys():\n",
    "                    features[f'power_{band_name}'] = 0\n",
    "                features['total_power'] = 0\n",
    "                features['spectral_edge_freq'] = 0\n",
    "                features['spectral_entropy'] = 0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Feature extraction failed: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def add_contact_depth_features(samples, channel_info, n_total_channels):\n",
    "        \"\"\"Add contact depth features\"\"\"\n",
    "        enhanced_samples = []\n",
    "        \n",
    "        for sample, ch_id in zip(samples, channel_info):\n",
    "            enhanced_sample = sample.copy()\n",
    "            \n",
    "            if n_total_channels > 1:\n",
    "                relative_depth = ch_id / (n_total_channels - 1)\n",
    "            else:\n",
    "                relative_depth = 0.5\n",
    "            \n",
    "            enhanced_sample['contact_depth'] = relative_depth\n",
    "            enhanced_sample['contact_depth_squared'] = relative_depth ** 2\n",
    "            enhanced_sample['is_surface_contact'] = 1 if relative_depth < 0.2 else 0\n",
    "            enhanced_sample['is_middle_contact'] = 1 if 0.3 <= relative_depth <= 0.7 else 0\n",
    "            enhanced_sample['is_deep_contact'] = 1 if relative_depth > 0.8 else 0\n",
    "            \n",
    "            enhanced_samples.append(enhanced_sample)\n",
    "        \n",
    "        return enhanced_samples\n",
    "    \n",
    "    def create_channel_separated_samples(data, tissue_type):\n",
    "        \"\"\"Create samples with channel tracking\"\"\"\n",
    "        samples = []\n",
    "        channel_info = []\n",
    "        n_time, n_channels = data.shape\n",
    "        \n",
    "        if use_windowing:\n",
    "            window_samples = min_window_length\n",
    "            \n",
    "            print(f\"{tissue_type} - Window: {window_samples} samples, Step: {step_samples} samples\")\n",
    "            \n",
    "            for ch in range(n_channels):\n",
    "                channel_data = data[:, ch]\n",
    "                channel_samples = []\n",
    "                \n",
    "                for start_idx in range(0, len(channel_data) - window_samples + 1, step_samples):\n",
    "                    end_idx = start_idx + window_samples\n",
    "                    window_data = channel_data[start_idx:end_idx]\n",
    "                    \n",
    "                    features = extract_comprehensive_features_from_window(window_data)\n",
    "                    if features is not None:\n",
    "                        channel_samples.append(features)\n",
    "                    \n",
    "                    if len(channel_samples) >= n_windows_per_channel:\n",
    "                        break\n",
    "                \n",
    "                if max_samples_per_channel and len(channel_samples) > max_samples_per_channel:\n",
    "                    indices = np.random.choice(len(channel_samples), max_samples_per_channel, replace=False)\n",
    "                    channel_samples = [channel_samples[i] for i in sorted(indices)]\n",
    "                \n",
    "                for sample in channel_samples:\n",
    "                    samples.append(sample)\n",
    "                    channel_info.append(ch)\n",
    "                    \n",
    "        else:\n",
    "            for ch in range(n_channels):\n",
    "                channel_data = data[:, ch]\n",
    "                features = extract_comprehensive_features_from_window(channel_data)\n",
    "                if features is not None:\n",
    "                    samples.append(features)\n",
    "                    channel_info.append(ch)\n",
    "        \n",
    "        return samples, channel_info\n",
    "    \n",
    "    def balance_dataset(grey_df, white_df, grey_channel_info, white_channel_info, \n",
    "                       balance_method='hybrid', max_samples_total=2000):\n",
    "        \"\"\"Balance dataset to handle class imbalance\"\"\"\n",
    "        print(f\"Original data - Grey: {len(grey_df)}, White: {len(white_df)}\")\n",
    "        \n",
    "        if balance_method == 'downsample':\n",
    "            min_samples = min(len(grey_df), len(white_df))\n",
    "            target_samples = min(min_samples, max_samples_total // 2)\n",
    "            \n",
    "            if len(grey_df) > target_samples:\n",
    "                sample_indices = np.random.choice(len(grey_df), target_samples, replace=False)\n",
    "                grey_df = grey_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                grey_channel_info = [grey_channel_info[i] for i in sample_indices]\n",
    "            \n",
    "            if len(white_df) > target_samples:\n",
    "                sample_indices = np.random.choice(len(white_df), target_samples, replace=False)\n",
    "                white_df = white_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                white_channel_info = [white_channel_info[i] for i in sample_indices]\n",
    "                \n",
    "        elif balance_method == 'upsample':\n",
    "            max_samples = max(len(grey_df), len(white_df))\n",
    "            target_samples = min(max_samples, max_samples_total // 2)\n",
    "            \n",
    "            if len(grey_df) < target_samples:\n",
    "                n_upsample = target_samples - len(grey_df)\n",
    "                upsample_indices = np.random.choice(len(grey_df), n_upsample, replace=True)\n",
    "                grey_upsampled = grey_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                grey_df = pd.concat([grey_df, grey_upsampled], ignore_index=True)\n",
    "                grey_channel_info.extend([grey_channel_info[i] for i in upsample_indices])\n",
    "            \n",
    "            if len(white_df) < target_samples:\n",
    "                n_upsample = target_samples - len(white_df)\n",
    "                upsample_indices = np.random.choice(len(white_df), n_upsample, replace=True)\n",
    "                white_upsampled = white_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                white_df = pd.concat([white_df, white_upsampled], ignore_index=True)\n",
    "                white_channel_info.extend([white_channel_info[i] for i in upsample_indices])\n",
    "                \n",
    "        elif balance_method == 'hybrid':\n",
    "            total_samples = len(grey_df) + len(white_df)\n",
    "            target_per_class = min(total_samples // 2, max_samples_total // 2)\n",
    "            \n",
    "            # Adjust grey matter\n",
    "            if len(grey_df) > target_per_class:\n",
    "                sample_indices = np.random.choice(len(grey_df), target_per_class, replace=False)\n",
    "                grey_df = grey_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                grey_channel_info = [grey_channel_info[i] for i in sample_indices]\n",
    "            elif len(grey_df) < target_per_class:\n",
    "                n_upsample = target_per_class - len(grey_df)\n",
    "                upsample_indices = np.random.choice(len(grey_df), n_upsample, replace=True)\n",
    "                grey_upsampled = grey_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                grey_df = pd.concat([grey_df, grey_upsampled], ignore_index=True)\n",
    "                grey_channel_info.extend([grey_channel_info[i] for i in upsample_indices])\n",
    "            \n",
    "            # Adjust white matter\n",
    "            if len(white_df) > target_per_class:\n",
    "                sample_indices = np.random.choice(len(white_df), target_per_class, replace=False)\n",
    "                white_df = white_df.iloc[sample_indices].reset_index(drop=True)\n",
    "                white_channel_info = [white_channel_info[i] for i in sample_indices]\n",
    "            elif len(white_df) < target_per_class:\n",
    "                n_upsample = target_per_class - len(white_df)\n",
    "                upsample_indices = np.random.choice(len(white_df), n_upsample, replace=True)\n",
    "                white_upsampled = white_df.iloc[upsample_indices].reset_index(drop=True)\n",
    "                white_df = pd.concat([white_df, white_upsampled], ignore_index=True)\n",
    "                white_channel_info.extend([white_channel_info[i] for i in upsample_indices])\n",
    "        \n",
    "        print(f\"Balanced data - Grey: {len(grey_df)}, White: {len(white_df)}\")\n",
    "        return grey_df, white_df, grey_channel_info, white_channel_info\n",
    "    \n",
    "    # ===============================\n",
    "    # MAIN FEATURE EXTRACTION\n",
    "    # ===============================\n",
    "    \n",
    "    print(\"\\nExtracting features with channel separation...\")\n",
    "    grey_samples, grey_channel_info = create_channel_separated_samples(\n",
    "        grey_matter_data, \"Grey Matter\"\n",
    "    )\n",
    "    white_samples, white_channel_info = create_channel_separated_samples(\n",
    "        white_matter_data, \"White Matter\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Grey matter samples created: {len(grey_samples)} from {len(set(grey_channel_info))} channels\")\n",
    "    print(f\"White matter samples created: {len(white_samples)} from {len(set(white_channel_info))} channels\")\n",
    "    \n",
    "    if len(grey_samples) == 0 or len(white_samples) == 0:\n",
    "        return {'error': 'No samples created'}\n",
    "    \n",
    "    # Add contact depth features\n",
    "    print(\"Adding contact depth features...\")\n",
    "    grey_samples = add_contact_depth_features(\n",
    "        grey_samples, grey_channel_info, grey_matter_data.shape[1]\n",
    "    )\n",
    "    white_samples = add_contact_depth_features(\n",
    "        white_samples, white_channel_info, white_matter_data.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    grey_df = pd.DataFrame(grey_samples)\n",
    "    white_df = pd.DataFrame(white_samples)\n",
    "    \n",
    "    # Clean data\n",
    "    def clean_dataframe(df, name):\n",
    "        original_shape = df.shape\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if df[numeric_cols].isna().any().any():\n",
    "            print(f\"Found NaN values in {name}, filling with column medians\")\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if np.any(np.isinf(df[col])):\n",
    "                print(f\"Found infinite values in {name} column {col}\")\n",
    "                finite_mask = np.isfinite(df[col])\n",
    "                if np.any(finite_mask):\n",
    "                    df.loc[~finite_mask, col] = df.loc[finite_mask, col].median()\n",
    "                else:\n",
    "                    df[col] = 0\n",
    "        \n",
    "        print(f\"Cleaned {name}: {original_shape} -> {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    grey_df = clean_dataframe(grey_df, \"grey matter features\")\n",
    "    white_df = clean_dataframe(white_df, \"white matter features\")\n",
    "    \n",
    "    # Balance dataset\n",
    "    print(\"Balancing dataset...\")\n",
    "    grey_df, white_df, grey_channel_info, white_channel_info = balance_dataset(\n",
    "        grey_df, white_df, grey_channel_info, white_channel_info,\n",
    "        balance_method=balance_method, max_samples_total=2000\n",
    "    )\n",
    "    \n",
    "    # Ensure common features\n",
    "    feature_cols = [col for col in grey_df.columns]\n",
    "    common_features = list(set(feature_cols).intersection(set(white_df.columns)))\n",
    "    \n",
    "    if len(common_features) == 0:\n",
    "        return {'error': 'No common features'}\n",
    "    \n",
    "    print(f\"Using {len(common_features)} common features\")\n",
    "    \n",
    "    # Add labels and channel info\n",
    "    grey_df['channel_id'] = grey_channel_info\n",
    "    white_df['channel_id'] = [ch + grey_channels for ch in white_channel_info]\n",
    "    grey_df['Matter'] = 'Grey'\n",
    "    white_df['Matter'] = 'White'\n",
    "    \n",
    "    # Combine data\n",
    "    combined_df = pd.concat([grey_df, white_df], ignore_index=True)\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = combined_df[common_features]\n",
    "    y = combined_df['Matter'].map({'Grey': 1, 'White': 0})\n",
    "    channel_ids = combined_df['channel_id'].values\n",
    "    \n",
    "    print(f\"Final dataset: {len(X)} samples √ó {len(common_features)} features\")\n",
    "    print(f\"Sample-to-feature ratio: {len(X) / len(common_features):.1f}:1\")\n",
    "    \n",
    "    # Check for perfect separability\n",
    "    def check_perfect_separability(X, y, feature_names):\n",
    "        perfect_features = []\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            feature_vals = X.iloc[:, i]\n",
    "            grey_vals = feature_vals[y == 1]\n",
    "            white_vals = feature_vals[y == 0]\n",
    "            \n",
    "            grey_min, grey_max = grey_vals.min(), grey_vals.max()\n",
    "            white_min, white_max = white_vals.min(), white_vals.max()\n",
    "            \n",
    "            if grey_max < white_min or white_max < grey_min:\n",
    "                perfect_features.append(feature)\n",
    "                print(f\"‚ö†Ô∏è  PERFECT SEPARATION found in feature '{feature}':\")\n",
    "                print(f\"   Grey range: [{grey_min:.3f}, {grey_max:.3f}]\")\n",
    "                print(f\"   White range: [{white_min:.3f}, {white_max:.3f}]\")\n",
    "        \n",
    "        return perfect_features\n",
    "    \n",
    "    perfect_features = check_perfect_separability(X, y, common_features)\n",
    "    if perfect_features:\n",
    "        print(f\"\\nüö® WARNING: Found {len(perfect_features)} features with perfect separation!\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train/test split\n",
    "    if validation_type == 'strict':\n",
    "        print(\"\\nüîí STRICT VALIDATION: Channel-wise train/test split\")\n",
    "        \n",
    "        grey_channels_used = combined_df[combined_df['Matter'] == 'Grey']['channel_id'].unique()\n",
    "        white_channels_used = combined_df[combined_df['Matter'] == 'White']['channel_id'].unique()\n",
    "        \n",
    "        print(f\"Grey matter: {len(grey_channels_used)} unique channels\")\n",
    "        print(f\"White matter: {len(white_channels_used)} unique channels\")\n",
    "        \n",
    "        grey_train_channels, grey_test_channels = train_test_split(\n",
    "            grey_channels_used, test_size=0.3, random_state=42\n",
    "        )\n",
    "        white_train_channels, white_test_channels = train_test_split(\n",
    "            white_channels_used, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_mask = (\n",
    "            (combined_df['Matter'] == 'Grey') & (combined_df['channel_id'].isin(grey_train_channels)) |\n",
    "            (combined_df['Matter'] == 'White') & (combined_df['channel_id'].isin(white_train_channels))\n",
    "        )\n",
    "        test_mask = ~train_mask\n",
    "        \n",
    "        X_train, X_test = X_scaled[train_mask], X_scaled[test_mask]\n",
    "        y_train, y_test = y[train_mask], y[test_mask]\n",
    "        test_channel_ids = channel_ids[test_mask]\n",
    "        \n",
    "        print(f\"Channel-wise split: Train={len(X_train)}, Test={len(X_test)}\")\n",
    "        \n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "            X_scaled, y, np.arange(len(X)), test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        test_channel_ids = channel_ids[test_idx]\n",
    "    \n",
    "    # ===============================\n",
    "    # CLASSIFICATION\n",
    "    # ===============================\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000, random_state=42, class_weight=class_weight\n",
    "        ),\n",
    "        'SVM (RBF kernel)': SVC(\n",
    "            probability=True, random_state=42, C=1.0, class_weight=class_weight\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, random_state=42, max_depth=10, class_weight=class_weight\n",
    "        ),\n",
    "        'MLP Neural Network': MLPClassifier(\n",
    "            max_iter=1000, random_state=42, hidden_layer_sizes=(50,), alpha=0.01\n",
    "        ),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "        'LDA': LDA(),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    if validation_type == 'strict':\n",
    "        print(\"\\nüîí Using channel-wise cross-validation...\")\n",
    "        cv = GroupKFold(n_splits=min(5, len(set(channel_ids))))\n",
    "        groups = channel_ids\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        groups = None\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    print(f\"\\nTraining classifiers with {class_weight} class weighting...\")\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation\n",
    "            if validation_type == 'strict' and len(set(channel_ids)) >= 3:\n",
    "                cv_scores_f1 = cross_val_score(clf, X_scaled, y, cv=cv, groups=groups, scoring='f1')\n",
    "                cv_scores_bal = cross_val_score(clf, X_scaled, y, cv=cv, groups=groups, scoring='balanced_accuracy')\n",
    "            else:\n",
    "                cv_scores_f1 = cross_val_score(clf, X_scaled, y, cv=StratifiedKFold(n_splits=3), scoring='f1')\n",
    "                cv_scores_bal = cross_val_score(clf, X_scaled, y, cv=StratifiedKFold(n_splits=3), scoring='balanced_accuracy')\n",
    "            \n",
    "            # Train and test\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_f1 = f1_score(y_test, y_pred)\n",
    "            test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "            test_precision = precision_score(y_test, y_pred)\n",
    "            test_recall = recall_score(y_test, y_pred)\n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # ROC curve\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "            else:\n",
    "                y_proba = np.zeros_like(y_pred)\n",
    "                roc_auc = 0.5\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'cv_f1': cv_scores_f1.mean(),\n",
    "                'cv_f1_std': cv_scores_f1.std(),\n",
    "                'cv_balanced_acc': cv_scores_bal.mean(),\n",
    "                'cv_balanced_acc_std': cv_scores_bal.std(),\n",
    "                'test_f1': test_f1,\n",
    "                'test_balanced_acc': test_balanced_acc,\n",
    "                'test_precision': test_precision,\n",
    "                'test_recall': test_recall,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'roc_auc': roc_auc,\n",
    "                'y_pred': y_pred,\n",
    "                'y_proba': y_proba,\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "            }\n",
    "            \n",
    "            print(f\"  CV F1: {cv_scores_f1.mean():.3f} ¬± {cv_scores_f1.std():.3f}\")\n",
    "            print(f\"  CV Balanced Acc: {cv_scores_bal.mean():.3f} ¬± {cv_scores_bal.std():.3f}\")\n",
    "            print(f\"  Test F1: {test_f1:.3f}\")\n",
    "            print(f\"  Test Precision: {test_precision:.3f}\")\n",
    "            print(f\"  Test Recall: {test_recall:.3f}\")\n",
    "            print(f\"  ROC AUC: {roc_auc:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    # Finalize ROC plot\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {validation_type.upper()} Validation')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(plot_folder, f'roc_curves_{validation_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ===============================\n",
    "    # CHANNEL-LEVEL ANALYSIS\n",
    "    # ===============================\n",
    "    \n",
    "    def aggregate_samples_to_channels(y_true, y_pred, y_proba, channel_ids, \n",
    "                                     aggregation_method='majority_vote'):\n",
    "        \"\"\"Aggregate sample-level predictions to channel-level\"\"\"\n",
    "        unique_channels = np.unique(channel_ids)\n",
    "        \n",
    "        channel_true_labels = []\n",
    "        channel_pred_labels = []\n",
    "        channel_probabilities = []\n",
    "        channel_confidence = []\n",
    "        channel_sample_counts = []\n",
    "        \n",
    "        for ch_id in unique_channels:\n",
    "            ch_mask = channel_ids == ch_id\n",
    "            ch_true = y_true[ch_mask]\n",
    "            ch_pred = y_pred[ch_mask]\n",
    "            ch_proba = y_proba[ch_mask]\n",
    "            \n",
    "            # Get true label (should be consistent)\n",
    "            true_label = np.bincount(ch_true).argmax()\n",
    "            \n",
    "            # Aggregate predictions based on method\n",
    "            if aggregation_method == 'majority_vote':\n",
    "                pred_label = np.bincount(ch_pred).argmax()\n",
    "                avg_proba = np.mean(ch_proba)\n",
    "                confidence = np.sum(ch_pred == pred_label) / len(ch_pred)\n",
    "                \n",
    "            elif aggregation_method == 'average_probability':\n",
    "                avg_proba = np.mean(ch_proba)\n",
    "                pred_label = 1 if avg_proba > 0.5 else 0\n",
    "                confidence = abs(avg_proba - 0.5) * 2\n",
    "                \n",
    "            elif aggregation_method == 'weighted_vote':\n",
    "                weights = np.abs(ch_proba - 0.5) * 2\n",
    "                weighted_votes_0 = np.sum(weights[ch_pred == 0])\n",
    "                weighted_votes_1 = np.sum(weights[ch_pred == 1])\n",
    "                \n",
    "                if weighted_votes_1 > weighted_votes_0:\n",
    "                    pred_label = 1\n",
    "                    confidence = weighted_votes_1 / (weighted_votes_0 + weighted_votes_1)\n",
    "                else:\n",
    "                    pred_label = 0\n",
    "                    confidence = weighted_votes_0 / (weighted_votes_0 + weighted_votes_1)\n",
    "                \n",
    "                avg_proba = np.mean(ch_proba)\n",
    "                \n",
    "            elif aggregation_method == 'confidence_threshold':\n",
    "                confidence_threshold = 0.7\n",
    "                high_conf_mask = (ch_proba > confidence_threshold) | (ch_proba < (1 - confidence_threshold))\n",
    "                \n",
    "                if np.any(high_conf_mask):\n",
    "                    high_conf_pred = ch_pred[high_conf_mask]\n",
    "                    high_conf_proba = ch_proba[high_conf_mask]\n",
    "                    pred_label = np.bincount(high_conf_pred).argmax()\n",
    "                    avg_proba = np.mean(high_conf_proba)\n",
    "                    confidence = np.sum(high_conf_mask) / len(ch_pred)\n",
    "                else:\n",
    "                    avg_proba = np.mean(ch_proba)\n",
    "                    pred_label = 1 if avg_proba > 0.5 else 0\n",
    "                    confidence = 0.5\n",
    "            \n",
    "            channel_true_labels.append(true_label)\n",
    "            channel_pred_labels.append(pred_label)\n",
    "            channel_probabilities.append(avg_proba)\n",
    "            channel_confidence.append(confidence)\n",
    "            channel_sample_counts.append(len(ch_pred))\n",
    "        \n",
    "        return {\n",
    "            'channel_ids': unique_channels,\n",
    "            'true_labels': np.array(channel_true_labels),\n",
    "            'pred_labels': np.array(channel_pred_labels),\n",
    "            'probabilities': np.array(channel_probabilities),\n",
    "            'confidence': np.array(channel_confidence),\n",
    "            'sample_counts': np.array(channel_sample_counts),\n",
    "            'aggregation_method': aggregation_method\n",
    "        }\n",
    "    \n",
    "    def evaluate_channel_level_performance(channel_results, plot_folder=None, validation_type='strict'):\n",
    "        \"\"\"Evaluate channel-level performance\"\"\"\n",
    "        y_true_ch = channel_results['true_labels']\n",
    "        y_pred_ch = channel_results['pred_labels']\n",
    "        y_proba_ch = channel_results['probabilities']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true_ch, y_pred_ch),\n",
    "            'f1_score': f1_score(y_true_ch, y_pred_ch),\n",
    "            'precision': precision_score(y_true_ch, y_pred_ch),\n",
    "            'recall': recall_score(y_true_ch, y_pred_ch),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_true_ch, y_pred_ch),\n",
    "            'roc_auc': roc_auc_score(y_true_ch, y_proba_ch),\n",
    "            'confusion_matrix': confusion_matrix(y_true_ch, y_pred_ch),\n",
    "            'n_channels': len(y_true_ch),\n",
    "            'n_grey_channels': np.sum(y_true_ch == 1),\n",
    "            'n_white_channels': np.sum(y_true_ch == 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CHANNEL-LEVEL PERFORMANCE ({channel_results['aggregation_method']})\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total channels: {metrics['n_channels']}\")\n",
    "        print(f\"Grey matter channels: {metrics['n_grey_channels']}\")\n",
    "        print(f\"White matter channels: {metrics['n_white_channels']}\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.3f}\")\n",
    "        print(f\"ROC AUC: {metrics['roc_auc']:.3f}\")\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(f\"  [[TN={metrics['confusion_matrix'][0,0]}, FP={metrics['confusion_matrix'][0,1]}],\")\n",
    "        print(f\"   [FN={metrics['confusion_matrix'][1,0]}, TP={metrics['confusion_matrix'][1,1]}]]\")\n",
    "        \n",
    "        # Visualizations\n",
    "        if plot_folder:\n",
    "            # Confusion matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=['White Matter', 'Grey Matter'],\n",
    "                       yticklabels=['White Matter', 'Grey Matter'])\n",
    "            plt.title(f'Channel-Level Confusion Matrix\\n({channel_results[\"aggregation_method\"]}, {validation_type})')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_folder, f'channel_confusion_matrix_{validation_type}_{channel_results[\"aggregation_method\"]}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Probability distribution\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            grey_proba = y_proba_ch[y_true_ch == 1]\n",
    "            white_proba = y_proba_ch[y_true_ch == 0]\n",
    "            \n",
    "            plt.hist(white_proba, bins=20, alpha=0.7, label='White Matter Channels', color='red')\n",
    "            plt.hist(grey_proba, bins=20, alpha=0.7, label='Grey Matter Channels', color='blue')\n",
    "            plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "            plt.xlabel('Predicted Probability (Grey Matter)')\n",
    "            plt.ylabel('Number of Channels')\n",
    "            plt.title('Channel-Level Probability Distribution')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Confidence analysis\n",
    "            plt.subplot(1, 2, 2)\n",
    "            confidence = channel_results['confidence']\n",
    "            correct_pred = (y_true_ch == y_pred_ch)\n",
    "            \n",
    "            plt.scatter(confidence[correct_pred], y_proba_ch[correct_pred], \n",
    "                       alpha=0.6, c='green', label='Correct Predictions', s=50)\n",
    "            plt.scatter(confidence[~correct_pred], y_proba_ch[~correct_pred], \n",
    "                       alpha=0.6, c='red', label='Incorrect Predictions', s=50)\n",
    "            plt.xlabel('Confidence Score')\n",
    "            plt.ylabel('Predicted Probability')\n",
    "            plt.title('Confidence vs Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_folder, f'channel_analysis_{validation_type}_{channel_results[\"aggregation_method\"]}.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compare_aggregation_methods(y_true, y_pred, y_proba, channel_ids, plot_folder=None, validation_type='strict'):\n",
    "        \"\"\"Compare different aggregation methods\"\"\"\n",
    "        methods = ['majority_vote', 'average_probability', 'weighted_vote', 'confidence_threshold']\n",
    "        comparison_results = {}\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COMPARING CHANNEL-LEVEL AGGREGATION METHODS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"\\n--- {method.upper()} ---\")\n",
    "            \n",
    "            channel_results = aggregate_samples_to_channels(\n",
    "                y_true, y_pred, y_proba, channel_ids, aggregation_method=method\n",
    "            )\n",
    "            \n",
    "            metrics = evaluate_channel_level_performance(\n",
    "                channel_results, plot_folder, validation_type\n",
    "            )\n",
    "            \n",
    "            comparison_results[method] = {\n",
    "                'channel_results': channel_results,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_df = pd.DataFrame({\n",
    "            method: {\n",
    "                'F1 Score': results['metrics']['f1_score'],\n",
    "                'Accuracy': results['metrics']['accuracy'],\n",
    "                'Balanced Acc': results['metrics']['balanced_accuracy'],\n",
    "                'Precision': results['metrics']['precision'],\n",
    "                'Recall': results['metrics']['recall'],\n",
    "                'ROC AUC': results['metrics']['roc_auc']\n",
    "            }\n",
    "            for method, results in comparison_results.items()\n",
    "        }).round(3)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"AGGREGATION METHODS COMPARISON\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(comparison_df.to_string())\n",
    "        \n",
    "        # Save comparison results\n",
    "        if plot_folder:\n",
    "            comparison_df.to_csv(os.path.join(plot_folder, f'channel_aggregation_comparison_{validation_type}.csv'))\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            metrics_to_plot = ['F1 Score', 'Balanced Acc', 'Precision', 'Recall', 'ROC AUC']\n",
    "            x = np.arange(len(methods))\n",
    "            width = 0.15\n",
    "            \n",
    "            for i, metric in enumerate(metrics_to_plot):\n",
    "                values = [comparison_df.loc[metric, method] for method in methods]\n",
    "                plt.bar(x + i*width, values, width, label=metric, alpha=0.8)\n",
    "            \n",
    "            plt.xlabel('Aggregation Methods')\n",
    "            plt.ylabel('Performance Score')\n",
    "            plt.title(f'Channel-Level Aggregation Methods Comparison ({validation_type})')\n",
    "            plt.xticks(x + width*2, [m.replace('_', ' ').title() for m in methods], rotation=15)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_folder, f'aggregation_methods_comparison_{validation_type}.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # Recommend best method\n",
    "        best_method = comparison_df.loc['F1 Score'].idxmax()\n",
    "        best_f1 = comparison_df.loc['F1 Score', best_method]\n",
    "        \n",
    "        print(f\"\\nüèÜ RECOMMENDED METHOD: {best_method}\")\n",
    "        print(f\"   Best F1 Score: {best_f1:.3f}\")\n",
    "        \n",
    "        return comparison_results, best_method\n",
    "    \n",
    "    # ===============================\n",
    "    # EXECUTE CHANNEL-LEVEL ANALYSIS\n",
    "    # ===============================\n",
    "    \n",
    "    # Find best classifier\n",
    "    valid_results = {name: result for name, result in results.items() if 'error' not in result}\n",
    "    if not valid_results:\n",
    "        return {'error': 'No valid classifier results'}\n",
    "    \n",
    "    best_classifier_name = max(valid_results.items(), key=lambda x: x[1]['test_f1'])[0]\n",
    "    best_result = valid_results[best_classifier_name]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHANNEL-LEVEL ANALYSIS USING BEST CLASSIFIER: {best_classifier_name}\")\n",
    "    print(f\"Best Sample-Level F1: {best_result['test_f1']:.3f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get predictions from best classifier\n",
    "    y_pred_best = best_result['y_pred']\n",
    "    y_proba_best = best_result['y_proba']\n",
    "    \n",
    "    # Compare aggregation methods\n",
    "    channel_comparison, best_aggregation_method = compare_aggregation_methods(\n",
    "        y_test, y_pred_best, y_proba_best, test_channel_ids, \n",
    "        plot_folder=plot_folder, validation_type=validation_type\n",
    "    )\n",
    "    \n",
    "    # Final channel-level results with best method\n",
    "    final_channel_results = aggregate_samples_to_channels(\n",
    "        y_test, y_pred_best, y_proba_best, test_channel_ids, \n",
    "        aggregation_method=best_aggregation_method\n",
    "    )\n",
    "    \n",
    "    final_channel_metrics = evaluate_channel_level_performance(\n",
    "        final_channel_results, plot_folder, validation_type\n",
    "    )\n",
    "    \n",
    "    # ===============================\n",
    "    # CREATE PERFORMANCE SUMMARY\n",
    "    # ===============================\n",
    "    \n",
    "    # Sample-level summary\n",
    "    sample_performance = []\n",
    "    for name, result in valid_results.items():\n",
    "        sample_performance.append({\n",
    "            'Classifier': name,\n",
    "            'CV F1': f\"{result['cv_f1']:.3f} ¬± {result['cv_f1_std']:.3f}\",\n",
    "            'Test F1': result['test_f1'],\n",
    "            'Test Precision': result['test_precision'],\n",
    "            'Test Recall': result['test_recall'],\n",
    "            'Test Balanced Acc': result['test_balanced_acc'],\n",
    "            'ROC AUC': result['roc_auc']\n",
    "        })\n",
    "    \n",
    "    sample_performance_df = pd.DataFrame(sample_performance)\n",
    "    sample_performance_df.to_csv(os.path.join(plot_folder, f'sample_performance_{validation_type}.csv'), index=False)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL SUMMARY - {validation_type.upper()} VALIDATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Perfect separation features: {len(perfect_features)}\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Sample-to-feature ratio: {len(X) / len(common_features):.1f}:1\")\n",
    "    print(f\"Class distribution: Grey={np.sum(y==1)}, White={np.sum(y==0)}\")\n",
    "    \n",
    "    print(f\"\\nSAMPLE-LEVEL PERFORMANCE:\")\n",
    "    print(f\"Best classifier: {best_classifier_name}\")\n",
    "    print(f\"Best F1 score: {best_result['test_f1']:.3f}\")\n",
    "    print(f\"Best ROC AUC: {best_result['roc_auc']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nCHANNEL-LEVEL PERFORMANCE:\")\n",
    "    print(f\"Best aggregation method: {best_aggregation_method}\")\n",
    "    print(f\"Channel-level F1 score: {final_channel_metrics['f1_score']:.3f}\")\n",
    "    print(f\"Channel-level ROC AUC: {final_channel_metrics['roc_auc']:.3f}\")\n",
    "    print(f\"Total channels analyzed: {final_channel_metrics['n_channels']}\")\n",
    "    \n",
    "    # Performance improvement\n",
    "    improvement = final_channel_metrics['f1_score'] - best_result['test_f1']\n",
    "    print(f\"F1 improvement (channel vs sample): {improvement:+.3f}\")\n",
    "    \n",
    "    if best_result['test_f1'] > 0.95:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Sample-level accuracy > 95% may indicate data leakage!\")\n",
    "    elif best_result['test_f1'] > 0.90:\n",
    "        print(\"‚ö†Ô∏è  CAUTION: Sample-level accuracy > 90% - verify results\")\n",
    "    else:\n",
    "        print(\"‚úÖ Sample-level accuracy seems reasonable\")\n",
    "    \n",
    "    if final_channel_metrics['f1_score'] > 0.8:\n",
    "        print(\"üéâ Excellent channel-level performance!\")\n",
    "    elif final_channel_metrics['f1_score'] > 0.6:\n",
    "        print(\"‚úÖ Good channel-level performance\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Channel-level performance could be improved\")\n",
    "    \n",
    "    # ===============================\n",
    "    # RETURN COMPREHENSIVE RESULTS\n",
    "    # ===============================\n",
    "    \n",
    "    return {\n",
    "        'validation_type': validation_type,\n",
    "        'balance_method': balance_method,\n",
    "        'class_weight': class_weight,\n",
    "        \n",
    "        # Data info\n",
    "        'n_samples': len(X),\n",
    "        'n_features': len(common_features),\n",
    "        'sample_feature_ratio': len(X) / len(common_features),\n",
    "        'perfect_features': perfect_features,\n",
    "        'feature_names': common_features,\n",
    "        \n",
    "        # Sample-level results\n",
    "        'sample_level': {\n",
    "            'classifier_results': valid_results,\n",
    "            'performance_summary': sample_performance_df,\n",
    "            'best_classifier': best_classifier_name,\n",
    "            'best_metrics': {\n",
    "                'f1_score': best_result['test_f1'],\n",
    "                'balanced_accuracy': best_result['test_balanced_acc'],\n",
    "                'roc_auc': best_result['roc_auc'],\n",
    "                'precision': best_result['test_precision'],\n",
    "                'recall': best_result['test_recall']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Channel-level results\n",
    "        'channel_level': {\n",
    "            'comparison_results': channel_comparison,\n",
    "            'best_aggregation_method': best_aggregation_method,\n",
    "            'final_results': final_channel_results,\n",
    "            'final_metrics': final_channel_metrics,\n",
    "            'improvement_over_sample': improvement\n",
    "        },\n",
    "        \n",
    "        # Files saved\n",
    "        'output_folder': plot_folder,\n",
    "        'files_generated': [\n",
    "            f'roc_curves_{validation_type}.png',\n",
    "            f'sample_performance_{validation_type}.csv',\n",
    "            f'channel_aggregation_comparison_{validation_type}.csv',\n",
    "            f'aggregation_methods_comparison_{validation_type}.png',\n",
    "            f'channel_confusion_matrix_{validation_type}_{best_aggregation_method}.png',\n",
    "            f'channel_analysis_{validation_type}_{best_aggregation_method}.png'\n",
    "        ]\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdec6570b94f9f21",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_3 = improved_extract_and_classify_features_complete(\n",
    "    seizure_data_grey_new, seizure_data_white_new, \n",
    "    plot_folder=f'result/{seizureID}/wg_classification_strict_ictal_data',\n",
    "    fs=p66_data.samplingRate,\n",
    "    use_windowing=True,  # Use windowing for better sample size\n",
    "    window_overlap=0.5,\n",
    "    min_window_length=1000,\n",
    "    validation_type='strict')  # Strict channel-wise validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805a5e5dc1394c2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1da97553c324e6f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
